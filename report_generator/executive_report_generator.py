#!/usr/bin/env python3
"""ìµœì¢… ë³´ê³ ì„œ ìƒì„±ê¸° (Executive Report)

ì„¤ì •: OpenAI + RRF MultiQuery (Top 8) + DeepSeek-V3.1
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

from dotenv import load_dotenv
load_dotenv()

import os
import json
from datetime import datetime
from typing import List, Dict, Any, Optional
from langchain.chat_models import init_chat_model

from config.settings import AZURE_AI_CREDENTIAL, AZURE_AI_ENDPOINT
from utils.langfuse_utils import get_langfuse_client
from utils.date_utils import parse_date_range, extract_date_filter_from_question
from retrievers.ensemble_retriever import get_ensemble_retriever
from retrievers.multiquery_retriever import get_multiquery_retriever


class ExecutiveReportGenerator:
    """ìµœì¢… ë³´ê³ ì„œ ìƒì„±ê¸° (Executive Report)

    ì„¤ì •:
    - Retriever: OpenAI + RRF MultiQuery (Top 8)
    - LLM: DeepSeek-V3.1
    """

    def __init__(self):
        self.retriever_config = {
            'name': 'openai_rrf_multiquery',
            'display_name': 'OpenAI + RRF MultiQuery (Top 8)',
            'embedding': 'openai-large',
            'type': 'rrf_multiquery',
            'top_k': 8,
            'description': 'Faithfulness + MultiQuery'
        }

        self.llm_config = {
            'name': 'deepseek_v31',
            'display_name': 'DeepSeek-V3.1',
            'model_id': 'azure_ai:DeepSeek-V3.1',
            'description': 'DeepSeek ìµœì‹  ë²„ì „'
        }

        self.langfuse = get_langfuse_client()
        self.qdrant_lock = "/home/work/rag/Project/rag-report-generator/data/qdrant_data/openai-large/.lock"

    def load_prompt(self, prompt_file: str) -> str:
        """í”„ë¡¬í”„íŠ¸ íŒŒì¼ ë¡œë“œ"""
        prompt_path = Path(__file__).parent.parent / "prompts" / "templates" / "service" / "executive_report" / prompt_file
        with open(prompt_path, 'r', encoding='utf-8') as f:
            return f.read()

    def retrieve_documents(self, question: str, date_filter: Optional[tuple] = None) -> List[Any]:
        """ë¬¸ì„œ ê²€ìƒ‰ - RRF MultiQuery"""
        import time
        import gc

        # Qdrant ë½ íŒŒì¼ ì •ë¦¬
        if os.path.exists(self.qdrant_lock):
            try:
                os.remove(self.qdrant_lock)
            except:
                pass

        # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        gc.collect()
        time.sleep(0.5)

        # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
        os.environ["MODEL_PRESET"] = self.retriever_config['embedding']
        os.environ["USE_EMBEDDING_CACHE"] = "true"

        # ê¸°ë³¸ RRF Ensemble ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±
        base_retriever = get_ensemble_retriever(
            k=self.retriever_config['top_k'],
            bm25_weight=0.5,
            dense_weight=0.5,
            date_filter=date_filter
        )

        # MultiQueryë¡œ ë˜í•‘
        retriever = get_multiquery_retriever(
            base_retriever=base_retriever,
            num_queries=3,
            temperature=0.7
        )

        print(f"ğŸ” ê²€ìƒ‰ ì¤‘... ({self.retriever_config['display_name']})")

        # ë¬¸ì„œ ê²€ìƒ‰
        docs = retriever.invoke(question)

        print(f"ğŸ“„ ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(docs)}")
        for i, doc in enumerate(docs, 1):
            print(f"  {i}. {doc.metadata.get('page_title', 'Unknown')}")

        return docs

    def generate_answer(self, question: str, docs: List[Any]) -> str:
        """DeepSeek-V3.1ë¡œ ë‹µë³€ ìƒì„±"""
        # Context êµ¬ì„±
        context_parts = []
        for i, doc in enumerate(docs, 1):
            title = doc.metadata.get('page_title', 'Unknown')
            content = doc.page_content
            context_parts.append(f"[ë¬¸ì„œ {i}] {title}\n{content}\n")

        context_text = "\n".join(context_parts)

        # í”„ë¡¬í”„íŠ¸ ë¡œë“œ
        system_prompt = self.load_prompt("system_prompt.txt")
        answer_generation_template = self.load_prompt("answer_generation_prompt.txt")

        # í…œí”Œë¦¿ì— ë³€ìˆ˜ ëŒ€ì…
        user_prompt = answer_generation_template.replace("{context}", context_text).replace("{question}", question)

        # Azure AI ì„¤ì •
        os.environ['AZURE_AI_CREDENTIAL'] = AZURE_AI_CREDENTIAL
        os.environ['AZURE_AI_ENDPOINT'] = AZURE_AI_ENDPOINT

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]

        print(f"ğŸ’¬ ë‹µë³€ ìƒì„± ì¤‘... ({self.llm_config['display_name']})")

        # LLM ìƒì„±
        model = init_chat_model(
            self.llm_config['model_id'],
            temperature=0,
            max_completion_tokens=1000
        )

        # Langfuseë¡œ ë‹µë³€ ìƒì„± ê¸°ë¡
        with self.langfuse.start_as_current_observation(
            as_type='generation',
            name=f"generation_{self.llm_config['name']}",
            model=self.llm_config['model_id'],
            input={"question": question, "context": context_text[:500] + "..." if len(context_text) > 500 else context_text},
            metadata={"llm": self.llm_config['name'], "num_docs": len(docs)}
        ) as generation:
            response = model.invoke(messages)
            answer = response.content

            # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œ
            usage_dict = None
            if hasattr(response, 'usage_metadata') and response.usage_metadata:
                usage_dict = {
                    "input": response.usage_metadata.get('input_tokens', 0),
                    "output": response.usage_metadata.get('output_tokens', 0),
                    "total": response.usage_metadata.get('total_tokens', 0)
                }

            generation.update(
                output={"answer": answer},
                usage=usage_dict if usage_dict else None
            )

            # Trace ì—…ë°ì´íŠ¸
            self.langfuse.update_current_trace(
                tags=["executive_report", self.retriever_config['name'], self.llm_config['name']],
                output={"answer": answer}
            )

        print(f"âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ\n")

        return answer

    def generate_report(self, questions: List[str], global_date_filter: Optional[tuple] = None) -> Dict[str, Any]:
        """ìµœì¢… ë³´ê³ ì„œ ìƒì„±

        Args:
            questions: ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
            global_date_filter: ì „ì—­ ë‚ ì§œ í•„í„° (ëª…ì‹œì ìœ¼ë¡œ ì§€ì •ëœ ê²½ìš°, ì§ˆë¬¸ë³„ ì¶”ì¶œë³´ë‹¤ ìš°ì„ )

        Returns:
            ë³´ê³ ì„œ ë°ì´í„°
        """
        print("\n" + "=" * 100)
        print("ğŸ“Š ìµœì¢… ë³´ê³ ì„œ ìƒì„± ì‹œì‘ (Executive Report)")
        print("=" * 100)
        print(f"ğŸ”§ ì„¤ì •: {self.retriever_config['display_name']} + {self.llm_config['display_name']}")
        if global_date_filter:
            print(f"ğŸ“… ì „ì—­ ë‚ ì§œ í•„í„°: {global_date_filter[0][:10]} ~ {global_date_filter[1][:10]}")
        print(f"ğŸ“ ì§ˆë¬¸ ìˆ˜: {len(questions)}")
        print()

        results = []

        for i, question in enumerate(questions, 1):
            print(f"\n{'=' * 100}")
            print(f"ì§ˆë¬¸ {i}/{len(questions)}")
            print(f"{'=' * 100}")
            print(f"â“ {question}\n")

            try:
                # ë‚ ì§œ í•„í„° ê²°ì •: ì „ì—­ í•„í„°ê°€ ìˆìœ¼ë©´ ê·¸ê²ƒ ì‚¬ìš©, ì—†ìœ¼ë©´ ì§ˆë¬¸ì—ì„œ ì¶”ì¶œ
                if global_date_filter:
                    date_filter = global_date_filter
                else:
                    date_filter = extract_date_filter_from_question(question)
                    if date_filter:
                        print(f"ğŸ“… ê°ì§€ëœ ë‚ ì§œ í•„í„°: {date_filter[0][:10]} ~ {date_filter[1][:10]}\n")

                # ë¬¸ì„œ ê²€ìƒ‰
                docs = self.retrieve_documents(question, date_filter)

                # ë‹µë³€ ìƒì„± (Langfuse ìë™ ì¶”ì )
                answer = self.generate_answer(question, docs)

                print(f"ğŸ“ ë‹µë³€:\n{answer}\n")

                results.append({
                    "question_id": i,
                    "question": question,
                    "date_filter": f"{date_filter[0][:10]} ~ {date_filter[1][:10]}" if date_filter else None,
                    "num_docs": len(docs),
                    "doc_titles": [doc.metadata.get('page_title', 'Unknown') for doc in docs],
                    "answer": answer,
                    "success": True
                })

            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}\n")
                import traceback
                traceback.print_exc()

                results.append({
                    "question_id": i,
                    "question": question,
                    "error": str(e),
                    "success": False
                })

        # Langfuse flush
        self.langfuse.flush()

        report_data = {
            "report_type": "executive",
            "generated_at": datetime.now().isoformat(),
            "retriever": self.retriever_config,
            "llm": self.llm_config,
            "global_date_filter": f"{global_date_filter[0][:10]} ~ {global_date_filter[1][:10]}" if global_date_filter else None,
            "num_questions": len(questions),
            "results": results
        }

        print("\n" + "=" * 100)
        print("âœ… ìµœì¢… ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ!")
        print("=" * 100)

        return report_data

    def save_json(self, report_data: Dict[str, Any], output_path: str):
        """JSON íŒŒì¼ë¡œ ì €ì¥"""
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ JSON ì €ì¥: {output_file}")


def main():
    import argparse

    parser = argparse.ArgumentParser(description="ìµœì¢… ë³´ê³ ì„œ ìƒì„±ê¸° (OpenAI + DeepSeek-V3.1)")
    parser.add_argument("--questions", type=str, nargs='+', help="ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸")
    parser.add_argument("--date-range", type=str, help="ë‚ ì§œ ë²”ìœ„ (ì˜ˆ: 'ì´ë²ˆ ì£¼', '12ì›” 2ì£¼ì°¨')")
    parser.add_argument("--start-date", type=str, help="ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)")
    parser.add_argument("--end-date", type=str, help="ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)")
    parser.add_argument("--output", type=str, default=None, help="ì¶œë ¥ íŒŒì¼ ê²½ë¡œ")

    args = parser.parse_args()

    # ë‚ ì§œ í•„í„° íŒŒì‹±
    date_filter = parse_date_range(
        date_input=args.date_range,
        start_date=args.start_date,
        end_date=args.end_date
    )

    # ì§ˆë¬¸ ì„¤ì •
    if args.questions:
        questions = args.questions
    else:
        # ê¸°ë³¸ ì§ˆë¬¸
        questions = [
            "10ì›” ìµœì¢… ë³´ê³ ì„œ ë§Œë“¤ì–´ì¤˜",
            "ì§€ê¸ˆê¹Œì§€ í•œ ê²ƒ ì¤‘ì— ì¤‘ìš” ìš”ì¸ë“¤ë¡œ ìµœì¢… ë³´ê³ ì„œ ë§Œë“¤ì–´ì¤˜",
            "ì¶”ì²œì‹œìŠ¤í…œ ìµœì¢… ë³´ê³ ì„œ ë§Œë“¤ì–´ì¤˜"
        ]

    # ë³´ê³ ì„œ ìƒì„±
    generator = ExecutiveReportGenerator()
    report_data = generator.generate_report(questions, date_filter)

    # JSON ì €ì¥
    if args.output:
        generator.save_json(report_data, args.output)
    else:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        default_output = f"data/reports/executive_report_{timestamp}.json"
        generator.save_json(report_data, default_output)


if __name__ == "__main__":
    main()
