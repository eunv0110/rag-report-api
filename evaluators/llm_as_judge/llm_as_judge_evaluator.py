#!/usr/bin/env python3
"""LLM as Judge í‰ê°€ê¸° - ìƒì„±ëœ ë³´ê³ ì„œì˜ í’ˆì§ˆì„ LLMìœ¼ë¡œ í‰ê°€

ì—¬ëŸ¬ LLMì´ ìƒì„±í•œ ì£¼ê°„/ì›”ê°„ ë³´ê³ ì„œë¥¼ í‰ê°€í•˜ì—¬ ì–´ë–¤ ë³´ê³ ì„œê°€ ë” ì í•©í•œì§€ íŒë‹¨í•©ë‹ˆë‹¤.
Azure AI ë° OpenRouterë¥¼ í†µí•œ ë‹¤ì–‘í•œ Judge ëª¨ë¸ ì§€ì›.
"""

import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

import json
import re
from typing import Dict, Any, List, Optional
from datetime import datetime
from langchain.chat_models import init_chat_model
from openai import OpenAI
import pandas as pd


class LLMAsJudgeEvaluator:
    """LLM as Judge í‰ê°€ í´ë˜ìŠ¤

    Azure AI ë˜ëŠ” OpenRouterë¥¼ í†µí•´ ë‹¤ì–‘í•œ Judge ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬
    ìƒì„±ëœ ë³´ê³ ì„œì˜ í’ˆì§ˆì„ í‰ê°€í•©ë‹ˆë‹¤.
    """

    # í‰ê°€ ê¸°ì¤€ ì •ì˜
    EVALUATION_CRITERIA = {
        "weekly_report": {
            "completeness": {
                "name": "ì™„ì „ì„± (Completeness)",
                "description": "ì£¼ìš” ì§€í‘œ, í™œë™, ì´ìŠˆ, ë‹¤ìŒ ì£¼ ê³„íšì´ ëª¨ë‘ í¬í•¨ë˜ì—ˆëŠ”ê°€?",
                "weight": 0.25
            },
            "relevance": {
                "name": "ê´€ë ¨ì„± (Relevance)",
                "description": "ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ë§Œ í¬í•¨í•˜ê³  ë¶ˆí•„ìš”í•œ ì •ë³´ëŠ” ì œì™¸ë˜ì—ˆëŠ”ê°€?",
                "weight": 0.20
            },
            "accuracy": {
                "name": "ì •í™•ì„± (Accuracy)",
                "description": "ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ë‚´ìš©ì„ ì •í™•í•˜ê²Œ ë°˜ì˜í•˜ê³  ìˆëŠ”ê°€? í™˜ê°ì€ ì—†ëŠ”ê°€?",
                "weight": 0.25
            },
            "structure": {
                "name": "êµ¬ì¡°í™” (Structure)",
                "description": "ì£¼ê°„ ë³´ê³ ì„œ í˜•ì‹ì— ì í•©í•˜ê²Œ êµ¬ì¡°í™”ë˜ì—ˆëŠ”ê°€?",
                "weight": 0.15
            },
            "readability": {
                "name": "ê°€ë…ì„± (Readability)",
                "description": "ì½ê¸° ì‰½ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ê°€? ì ì ˆí•œ í¬ë§·íŒ…ì´ ë˜ì–´ ìˆëŠ”ê°€?",
                "weight": 0.15
            }
        },
        "executive_report": {
            "conciseness": {
                "name": "ê°„ê²°ì„± (Conciseness)",
                "description": "í•µì‹¬ë§Œ ê°„ê²°í•˜ê²Œ ìš”ì•½ë˜ì—ˆëŠ”ê°€? ë¶ˆí•„ìš”í•œ ì„¸ë¶€ì‚¬í•­ì€ ì—†ëŠ”ê°€?",
                "weight": 0.25
            },
            "strategic_value": {
                "name": "ì „ëµì  ê°€ì¹˜ (Strategic Value)",
                "description": "ê²½ì˜ì§„ì´ ì˜ì‚¬ê²°ì •ì— í™œìš©í•  ìˆ˜ ìˆëŠ” ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•˜ëŠ”ê°€?",
                "weight": 0.25
            },
            "accuracy": {
                "name": "ì •í™•ì„± (Accuracy)",
                "description": "ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ë‚´ìš©ì„ ì •í™•í•˜ê²Œ ë°˜ì˜í•˜ê³  ìˆëŠ”ê°€? í™˜ê°ì€ ì—†ëŠ”ê°€?",
                "weight": 0.25
            },
            "clarity": {
                "name": "ëª…í™•ì„± (Clarity)",
                "description": "ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ì–¸ì–´ë¡œ ì‘ì„±ë˜ì—ˆëŠ”ê°€?",
                "weight": 0.15
            },
            "priority": {
                "name": "ìš°ì„ ìˆœìœ„ (Priority)",
                "description": "ì¤‘ìš”í•œ ì •ë³´ê°€ ë¨¼ì € ì œì‹œë˜ê³  ìš°ì„ ìˆœìœ„ê°€ ëª…í™•í•œê°€?",
                "weight": 0.10
            }
        }
    }

    def __init__(
        self,
        judge_model: str = "gpt-4o",
        provider: str = "azure_ai",
        temperature: float = 0
    ):
        """
        Args:
            judge_model: í‰ê°€ì— ì‚¬ìš©í•  LLM ëª¨ë¸
                - Azure AI: gpt-4o, gpt-4.5, o1 ë“±
                - OpenRouter: anthropic/claude-opus-4.5 ë“±
            provider: ì‚¬ìš©í•  ì œê³µì ("azure_ai" ë˜ëŠ” "openrouter")
            temperature: ìƒì„± ì˜¨ë„ (ê¸°ë³¸: 0 - ì¼ê´€ëœ í‰ê°€ë¥¼ ìœ„í•¨)
        """
        self.judge_model = judge_model
        self.provider = provider
        self.temperature = temperature

        # Judge LLM ì´ˆê¸°í™”
        if provider == "azure_ai":
            from config.settings import AZURE_AI_CREDENTIAL, AZURE_AI_ENDPOINT

            self.llm = init_chat_model(
                model=f"azure_ai:{judge_model}",
                api_key=AZURE_AI_CREDENTIAL,
                azure_endpoint=AZURE_AI_ENDPOINT,
                temperature=temperature
            )
            self.client = None
        elif provider == "openrouter":
            from config.settings import OPENROUTER_API_KEY, OPENROUTER_BASE_URL

            self.llm = None
            self.client = OpenAI(
                api_key=OPENROUTER_API_KEY,
                base_url=OPENROUTER_BASE_URL
            )
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” provider: {provider}. 'azure_ai' ë˜ëŠ” 'openrouter'ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")

    def _load_prompt_template(self, template_name: str) -> str:
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¡œë“œ

        Args:
            template_name: í…œí”Œë¦¿ íŒŒì¼ ì´ë¦„

        Returns:
            í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¬¸ìì—´
        """
        template_path = Path(__file__).parent.parent.parent / "prompts" / "templates" / "evaluation" / "llm_judge" / template_name
        with open(template_path, 'r', encoding='utf-8') as f:
            return f.read()

    def _create_evaluation_prompt(
        self,
        question: str,
        answer: str,
        report_type: str,
        criteria_name: str,
        criteria_desc: str
    ) -> str:
        """í‰ê°€ í”„ë¡¬í”„íŠ¸ ìƒì„±

        Args:
            question: ì›ë³¸ ì§ˆë¬¸
            answer: í‰ê°€í•  ë‹µë³€
            report_type: ë³´ê³ ì„œ íƒ€ì… (weekly_report, executive_report)
            criteria_name: í‰ê°€ ê¸°ì¤€ ì´ë¦„
            criteria_desc: í‰ê°€ ê¸°ì¤€ ì„¤ëª…

        Returns:
            í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´
        """
        report_type_ko = "ì£¼ê°„ ë³´ê³ ì„œ" if report_type == "weekly_report" else "ê²½ì˜ì§„ ë³´ê³ ì„œ"

        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¡œë“œ
        template = self._load_prompt_template("criterion_evaluation_prompt.txt")

        # í…œí”Œë¦¿ì— ê°’ ì±„ìš°ê¸°
        prompt = template.replace("{report_type_ko}", report_type_ko)\
                        .replace("{criteria_name}", criteria_name)\
                        .replace("{criteria_desc}", criteria_desc)\
                        .replace("{question}", question)\
                        .replace("{answer}", answer)

        return prompt

    def _evaluate_single_criterion(
        self,
        question: str,
        answer: str,
        report_type: str,
        criterion_key: str,
        criterion_info: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ë‹¨ì¼ í‰ê°€ ê¸°ì¤€ìœ¼ë¡œ ë‹µë³€ í‰ê°€

        Args:
            question: ì§ˆë¬¸
            answer: ë‹µë³€
            report_type: ë³´ê³ ì„œ íƒ€ì…
            criterion_key: í‰ê°€ ê¸°ì¤€ í‚¤
            criterion_info: í‰ê°€ ê¸°ì¤€ ì •ë³´

        Returns:
            í‰ê°€ ê²°ê³¼
        """
        prompt = self._create_evaluation_prompt(
            question=question,
            answer=answer,
            report_type=report_type,
            criteria_name=criterion_info["name"],
            criteria_desc=criterion_info["description"]
        )

        messages = [
            {"role": "system", "content": "ë‹¹ì‹ ì€ ë³´ê³ ì„œ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê³µì •í•˜ê³  ê°ê´€ì ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”."},
            {"role": "user", "content": prompt}
        ]

        try:
            # Providerì— ë”°ë¼ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ í˜¸ì¶œ
            if self.provider == "azure_ai":
                response = self.llm.invoke(messages)
                result = json.loads(response.content)
            else:  # openrouter
                response = self.client.chat.completions.create(
                    model=self.judge_model,
                    messages=messages,
                    max_tokens=4000,
                    temperature=self.temperature
                )
                content = response.choices[0].message.content

                # JSON ì¶”ì¶œ (```json ... ``` í˜•ì‹ ì§€ì›)
                json_match = re.search(r'```json\s*(\{.*?\})\s*```', content, re.DOTALL)
                if json_match:
                    result = json.loads(json_match.group(1))
                else:
                    result = json.loads(content)

            return {
                "criterion": criterion_key,
                "criterion_name": criterion_info["name"],
                "weight": criterion_info["weight"],
                "score": result["score"],
                "weighted_score": result["score"] * criterion_info["weight"],
                "reasoning": result["reasoning"],
                "strengths": result.get("strengths", []),
                "weaknesses": result.get("weaknesses", [])
            }
        except Exception as e:
            print(f"âš ï¸  í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({criterion_key}): {e}")
            return {
                "criterion": criterion_key,
                "criterion_name": criterion_info["name"],
                "weight": criterion_info["weight"],
                "score": 0,
                "weighted_score": 0,
                "reasoning": f"í‰ê°€ ì‹¤íŒ¨: {str(e)}",
                "strengths": [],
                "weaknesses": []
            }

    def evaluate_answer(
        self,
        question: str,
        answer: str,
        report_type: str = "weekly_report"
    ) -> Dict[str, Any]:
        """ë‹µë³€ì„ ëª¨ë“  ê¸°ì¤€ìœ¼ë¡œ í‰ê°€

        Args:
            question: ì§ˆë¬¸
            answer: ë‹µë³€
            report_type: ë³´ê³ ì„œ íƒ€ì… (weekly_report, executive_report)

        Returns:
            ì¢…í•© í‰ê°€ ê²°ê³¼
        """
        if report_type not in self.EVALUATION_CRITERIA:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë³´ê³ ì„œ íƒ€ì…: {report_type}")

        criteria = self.EVALUATION_CRITERIA[report_type]
        criterion_results = []

        print(f"\n{'='*80}")
        print(f"ğŸ“Š {report_type} í‰ê°€ ì‹œì‘")
        print(f"{'='*80}")

        # ê° í‰ê°€ ê¸°ì¤€ë³„ë¡œ í‰ê°€
        for criterion_key, criterion_info in criteria.items():
            print(f"\ní‰ê°€ ê¸°ì¤€: {criterion_info['name']}")

            result = self._evaluate_single_criterion(
                question=question,
                answer=answer,
                report_type=report_type,
                criterion_key=criterion_key,
                criterion_info=criterion_info
            )

            criterion_results.append(result)
            print(f"  ì ìˆ˜: {result['score']}/10 (ê°€ì¤‘ì¹˜: {result['weight']}, ê°€ì¤‘ ì ìˆ˜: {result['weighted_score']:.2f})")

        # ì¢…í•© ì ìˆ˜ ê³„ì‚°
        total_weighted_score = sum(r["weighted_score"] for r in criterion_results)
        total_max_score = sum(10 * r["weight"] for r in criterion_results)
        final_score = (total_weighted_score / total_max_score) * 10

        print(f"\n{'='*80}")
        print(f"âœ… ìµœì¢… ì ìˆ˜: {final_score:.2f}/10")
        print(f"{'='*80}")

        return {
            "report_type": report_type,
            "final_score": final_score,
            "total_weighted_score": total_weighted_score,
            "criterion_results": criterion_results,
            "timestamp": datetime.now().isoformat()
        }

    def compare_multiple_answers(
        self,
        question: str,
        answers: Dict[str, str],
        report_type: str = "weekly_report"
    ) -> Dict[str, Any]:
        """ì—¬ëŸ¬ ë‹µë³€ì„ ë¹„êµ í‰ê°€

        Args:
            question: ì§ˆë¬¸
            answers: {llm_name: answer} í˜•ì‹ì˜ ë”•ì…”ë„ˆë¦¬
            report_type: ë³´ê³ ì„œ íƒ€ì…

        Returns:
            ë¹„êµ í‰ê°€ ê²°ê³¼
        """
        results = {}

        print(f"\n{'='*80}")
        print(f"ğŸ” {len(answers)}ê°œ ë‹µë³€ ë¹„êµ í‰ê°€")
        print(f"{'='*80}")

        # ê° ë‹µë³€ í‰ê°€
        for llm_name, answer in answers.items():
            print(f"\n[{llm_name}] í‰ê°€ ì¤‘...")
            results[llm_name] = self.evaluate_answer(question, answer, report_type)

        # ë­í‚¹ ìƒì„±
        ranking = sorted(
            results.items(),
            key=lambda x: x[1]["final_score"],
            reverse=True
        )

        print(f"\n{'='*80}")
        print("ğŸ† ìµœì¢… ìˆœìœ„")
        print(f"{'='*80}")
        for rank, (llm_name, result) in enumerate(ranking, 1):
            print(f"{rank}ìœ„: {llm_name:<20} - {result['final_score']:.2f}/10")

        return {
            "question": question,
            "report_type": report_type,
            "num_answers": len(answers),
            "results": results,
            "ranking": [(name, result["final_score"]) for name, result in ranking],
            "timestamp": datetime.now().isoformat()
        }

    def evaluate_from_results_dir(
        self,
        results_dir: str,
        report_type: str = "weekly_report",
        output_path: str = None
    ) -> Dict[str, Any]:
        """ê²°ê³¼ ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  LLM ë‹µë³€ì„ ë¡œë“œí•˜ê³  í‰ê°€

        Args:
            results_dir: ê²°ê³¼ ë””ë ‰í† ë¦¬ ê²½ë¡œ (ì˜ˆ: .../llm_comparison/bge_m3_rrf_ensemble_20251228_114425)
            report_type: ë³´ê³ ì„œ íƒ€ì…
            output_path: ê²°ê³¼ ì €ì¥ ê²½ë¡œ (Noneì´ë©´ ìë™ ìƒì„±)

        Returns:
            í‰ê°€ ê²°ê³¼
        """
        results_path = Path(results_dir)

        if not results_path.exists():
            raise FileNotFoundError(f"ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {results_dir}")

        # ê° LLM ë””ë ‰í† ë¦¬ì—ì„œ results.json ë¡œë“œ
        answers = {}
        question = None

        for llm_dir in results_path.iterdir():
            if not llm_dir.is_dir():
                continue

            result_file = llm_dir / "results.json"
            if not result_file.exists():
                print(f"âš ï¸  {llm_dir.name}ì˜ results.jsonì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                continue

            try:
                with open(result_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                if data.get("results") and len(data["results"]) > 0:
                    result = data["results"][0]

                    if question is None:
                        question = result.get("question", "")

                    if result.get("result", {}).get("success"):
                        answers[data["llm_name"]] = result["result"]["answer"]
                        print(f"âœ… {data['llm_name']} ë‹µë³€ ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸  {llm_dir.name} ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")

        if not answers:
            raise ValueError("í‰ê°€í•  ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        if not question:
            raise ValueError("ì§ˆë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # ë¹„êµ í‰ê°€ ì‹¤í–‰
        comparison_result = self.compare_multiple_answers(question, answers, report_type)

        # ê²°ê³¼ ì €ì¥
        if output_path is None:
            output_path = results_path / f"llm_judge_evaluation_{report_type}.json"

        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(comparison_result, f, indent=2, ensure_ascii=False)

        print(f"\nğŸ’¾ í‰ê°€ ê²°ê³¼ ì €ì¥: {output_path}")

        # ìƒì„¸ ë³´ê³ ì„œ ìƒì„± (CSV)
        self._save_detailed_report(comparison_result, output_path.parent / f"llm_judge_report_{report_type}.csv")

        return comparison_result

    def batch_evaluate_from_dir(
        self,
        base_dir: str,
        report_type: str = "weekly_report"
    ) -> Dict[str, Any]:
        """ì—¬ëŸ¬ retriever ê²°ê³¼ë¥¼ ì¼ê´„ í‰ê°€

        Args:
            base_dir: llm_comparison ë””ë ‰í† ë¦¬ ê²½ë¡œ
            report_type: ë³´ê³ ì„œ íƒ€ì…

        Returns:
            ì „ì²´ í‰ê°€ ê²°ê³¼
        """
        base_path = Path(base_dir)

        if not base_path.exists():
            raise FileNotFoundError(f"ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {base_dir}")

        all_results = {}

        # ê° retriever ë””ë ‰í† ë¦¬ ìˆœíšŒ
        for retriever_dir in sorted(base_path.iterdir()):
            if not retriever_dir.is_dir():
                continue

            retriever_name = retriever_dir.name
            print(f"\n{'='*100}")
            print(f"ğŸ“‚ {retriever_name} í‰ê°€ ì¤‘...")
            print(f"{'='*100}")

            try:
                result = self.evaluate_from_results_dir(
                    results_dir=str(retriever_dir),
                    report_type=report_type
                )
                all_results[retriever_name] = result
                print(f"âœ… {retriever_name} í‰ê°€ ì™„ë£Œ")
            except Exception as e:
                print(f"âŒ {retriever_name} í‰ê°€ ì‹¤íŒ¨: {e}")
                all_results[retriever_name] = {"error": str(e)}

        # ì „ì²´ ê²°ê³¼ ì €ì¥
        output_path = base_path / f"all_evaluations_{report_type}.json"
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, indent=2, ensure_ascii=False)

        print(f"\nğŸ’¾ ì „ì²´ í‰ê°€ ê²°ê³¼ ì €ì¥: {output_path}")

        # ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
        self._create_summary_report(all_results, base_path / f"summary_{report_type}.csv")

        return all_results

    def _create_summary_report(self, all_results: Dict[str, Any], output_path: Path):
        """ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±

        Args:
            all_results: ì „ì²´ í‰ê°€ ê²°ê³¼
            output_path: CSV ì¶œë ¥ ê²½ë¡œ
        """
        rows = []

        for retriever_name, result in all_results.items():
            if "error" in result:
                continue

            for llm_name, llm_result in result.get("results", {}).items():
                row = {
                    "retriever": retriever_name,
                    "llm_name": llm_name,
                    "final_score": llm_result["final_score"]
                }

                # ê° í‰ê°€ ê¸°ì¤€ë³„ ì ìˆ˜ ì¶”ê°€
                for criterion_result in llm_result["criterion_results"]:
                    criterion_key = criterion_result["criterion"]
                    row[f"{criterion_key}_score"] = criterion_result["score"]
                    row[f"{criterion_key}_weighted"] = criterion_result["weighted_score"]

                rows.append(row)

        df = pd.DataFrame(rows)

        # ì •ë ¬: retrieverë³„, ìµœì¢… ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ
        df = df.sort_values(["retriever", "final_score"], ascending=[True, False])

        df.to_csv(output_path, index=False, encoding='utf-8-sig')
        print(f"ğŸ“Š ìš”ì•½ ë¦¬í¬íŠ¸ ì €ì¥: {output_path}")

        # ì½˜ì†” ì¶œë ¥
        print(f"\n{'='*100}")
        print("ğŸ† Retrieverë³„ ìµœê³  ì ìˆ˜ LLM")
        print(f"{'='*100}")

        for retriever in df["retriever"].unique():
            retriever_df = df[df["retriever"] == retriever]
            top_llm = retriever_df.iloc[0]
            print(f"\n{retriever}:")
            print(f"  1ìœ„: {top_llm['llm_name']:<20} - {top_llm['final_score']:.2f}/10")

            if len(retriever_df) > 1:
                second_llm = retriever_df.iloc[1]
                print(f"  2ìœ„: {second_llm['llm_name']:<20} - {second_llm['final_score']:.2f}/10")

        # ì „ì²´ LLMë³„ í‰ê·  ì ìˆ˜
        print(f"\n{'='*100}")
        print("ğŸ¯ LLMë³„ í‰ê·  ì ìˆ˜ (ëª¨ë“  retriever)")
        print(f"{'='*100}")

        llm_avg = df.groupby("llm_name")["final_score"].mean().sort_values(ascending=False)
        for llm_name, avg_score in llm_avg.items():
            print(f"  {llm_name:<20} - {avg_score:.2f}/10")

    def _save_detailed_report(self, comparison_result: Dict[str, Any], output_path: Path):
        """ìƒì„¸ í‰ê°€ ë³´ê³ ì„œë¥¼ CSVë¡œ ì €ì¥

        Args:
            comparison_result: ë¹„êµ í‰ê°€ ê²°ê³¼
            output_path: CSV ì¶œë ¥ ê²½ë¡œ
        """
        rows = []

        for llm_name, result in comparison_result["results"].items():
            for criterion_result in result["criterion_results"]:
                rows.append({
                    "llm_name": llm_name,
                    "final_score": result["final_score"],
                    "criterion": criterion_result["criterion"],
                    "criterion_name": criterion_result["criterion_name"],
                    "weight": criterion_result["weight"],
                    "score": criterion_result["score"],
                    "weighted_score": criterion_result["weighted_score"],
                    "reasoning": criterion_result["reasoning"],
                    "strengths": " | ".join(criterion_result["strengths"]),
                    "weaknesses": " | ".join(criterion_result["weaknesses"])
                })

        df = pd.DataFrame(rows)
        df.to_csv(output_path, index=False, encoding='utf-8-sig')
        print(f"ğŸ“Š ìƒì„¸ ë³´ê³ ì„œ ì €ì¥: {output_path}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse

    parser = argparse.ArgumentParser(
        description="LLM as Judge í‰ê°€ê¸° - ìƒì„±ëœ ë³´ê³ ì„œì˜ í’ˆì§ˆì„ LLMìœ¼ë¡œ í‰ê°€"
    )

    # ëª¨ë“œ ì„ íƒ
    subparsers = parser.add_subparsers(dest="mode", help="ì‹¤í–‰ ëª¨ë“œ")

    # ë‹¨ì¼ ë””ë ‰í† ë¦¬ í‰ê°€
    single_parser = subparsers.add_parser("single", help="ë‹¨ì¼ ê²°ê³¼ ë””ë ‰í† ë¦¬ í‰ê°€")
    single_parser.add_argument("--results-dir", required=True, help="ê²°ê³¼ ë””ë ‰í† ë¦¬ ê²½ë¡œ")
    single_parser.add_argument("--report-type", default="weekly_report",
                              choices=["weekly_report", "executive_report"],
                              help="ë³´ê³ ì„œ íƒ€ì…")
    single_parser.add_argument("--output", help="ê²°ê³¼ ì €ì¥ ê²½ë¡œ (ì„ íƒ)")

    # ë°°ì¹˜ í‰ê°€
    batch_parser = subparsers.add_parser("batch", help="ì—¬ëŸ¬ retriever ê²°ê³¼ ì¼ê´„ í‰ê°€")
    batch_parser.add_argument("--base-dir", required=True,
                             help="llm_comparison ë””ë ‰í† ë¦¬ ê²½ë¡œ")
    batch_parser.add_argument("--report-type", default="weekly_report",
                             choices=["weekly_report", "executive_report"],
                             help="ë³´ê³ ì„œ íƒ€ì…")

    # ê³µí†µ ì˜µì…˜
    for p in [single_parser, batch_parser]:
        p.add_argument("--judge-model", default="gpt-4o",
                      help="í‰ê°€ì— ì‚¬ìš©í•  LLM ëª¨ë¸ (ì˜ˆ: gpt-4o, anthropic/claude-opus-4.5)")
        p.add_argument("--provider", default="azure_ai",
                      choices=["azure_ai", "openrouter"],
                      help="LLM ì œê³µì")
        p.add_argument("--temperature", type=float, default=0,
                      help="ìƒì„± ì˜¨ë„ (ê¸°ë³¸: 0)")

    args = parser.parse_args()

    if not args.mode:
        parser.print_help()
        return

    # í‰ê°€ê¸° ìƒì„±
    evaluator = LLMAsJudgeEvaluator(
        judge_model=args.judge_model,
        provider=args.provider,
        temperature=args.temperature
    )

    print(f"\n{'='*100}")
    print(f"ğŸ¤– Judge ëª¨ë¸: {args.judge_model} (provider: {args.provider})")
    print(f"{'='*100}\n")

    # ëª¨ë“œë³„ ì‹¤í–‰
    if args.mode == "single":
        evaluator.evaluate_from_results_dir(
            results_dir=args.results_dir,
            report_type=args.report_type,
            output_path=args.output
        )
    elif args.mode == "batch":
        evaluator.batch_evaluate_from_dir(
            base_dir=args.base_dir,
            report_type=args.report_type
        )

    print("\nâœ… ëª¨ë“  í‰ê°€ ì™„ë£Œ!")


if __name__ == "__main__":
    main()
