#!/usr/bin/env python3
"""ì¡°í•© ê²°ê³¼ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸ - LLM as Judgeë¡œ ì—¬ëŸ¬ LLM ì„±ëŠ¥ ë¹„êµ

all_combinations.json íŒŒì¼ì„ ì½ì–´ì„œ ê° ì§ˆë¬¸ì— ëŒ€í•´ ì—¬ëŸ¬ LLMì˜ ë‹µë³€ì„ í‰ê°€í•˜ê³  ìˆœìœ„ë¥¼ ë§¤ê¹ë‹ˆë‹¤.
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

import json
from typing import Dict, Any, List
from datetime import datetime
from llm_as_judge_evaluator import LLMAsJudgeEvaluator
import pandas as pd


def load_combinations_result(combinations_file: str) -> Dict[str, Any]:
    """all_combinations.json íŒŒì¼ ë¡œë“œ"""
    with open(combinations_file, 'r', encoding='utf-8') as f:
        return json.load(f)


def extract_answers_by_question(combinations_data: Dict[str, Any]) -> List[Dict[str, Any]]:
    """ì§ˆë¬¸ë³„ë¡œ ë‹µë³€ì„ ê·¸ë£¹í™”

    Returns:
        [
            {
                "question_id": 1,
                "question": "ì§ˆë¬¸ ë‚´ìš©",
                "date_filter": "ë‚ ì§œ ë²”ìœ„",
                "answers": {
                    "phi4": "ë‹µë³€ ë‚´ìš©",
                    "gpt41": "ë‹µë³€ ë‚´ìš©",
                    ...
                }
            },
            ...
        ]
    """
    # ì§ˆë¬¸ë³„ë¡œ ë°ì´í„°ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬
    questions_dict = {}

    # ê° ì¡°í•©(LLM)ì˜ ê²°ê³¼ë¥¼ ìˆœíšŒ
    for combination in combinations_data['combinations']:
        llm_name = combination['llm_name']
        llm_display_name = combination['llm_display_name']

        # í•´ë‹¹ LLMì˜ ê° ì§ˆë¬¸ ê²°ê³¼ë¥¼ ìˆœíšŒ
        for result_item in combination['results']:
            question_id = result_item['question_id']
            question = result_item['question']
            date_filter = result_item.get('date_filter')

            # ì§ˆë¬¸ë³„ ë”•ì…”ë„ˆë¦¬ ì´ˆê¸°í™”
            if question_id not in questions_dict:
                questions_dict[question_id] = {
                    "question_id": question_id,
                    "question": question,
                    "date_filter": date_filter,
                    "answers": {},
                    "display_names": {},
                    "num_docs": result_item.get('num_docs', 0),
                    "doc_titles": result_item.get('doc_titles', [])
                }

            # ë‹µë³€ ì¶”ê°€ (ì„±ê³µí•œ ê²½ìš°ë§Œ)
            if result_item['result'].get('success'):
                questions_dict[question_id]['answers'][llm_name] = result_item['result']['answer']
                questions_dict[question_id]['display_names'][llm_name] = llm_display_name

    # ì§ˆë¬¸ ID ìˆœì„œëŒ€ë¡œ ì •ë ¬í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
    return [questions_dict[qid] for qid in sorted(questions_dict.keys())]


def evaluate_all_questions(
    combinations_file: str,
    judge_model: str = "gpt-4o",
    provider: str = "azure_ai",
    report_type: str = "weekly_report",
    output_dir: str = None
) -> Dict[str, Any]:
    """ëª¨ë“  ì§ˆë¬¸ì— ëŒ€í•´ LLM ë‹µë³€ í‰ê°€

    Args:
        combinations_file: all_combinations.json íŒŒì¼ ê²½ë¡œ
        judge_model: í‰ê°€ì— ì‚¬ìš©í•  ëª¨ë¸ (gpt-4o, anthropic/claude-opus-4.5 ë“±)
        provider: LLM ì œê³µì (azure_ai ë˜ëŠ” openrouter)
        report_type: ë³´ê³ ì„œ íƒ€ì… (weekly_report, executive_report)
        output_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ (Noneì´ë©´ ìë™ ìƒì„±)

    Returns:
        ì „ì²´ í‰ê°€ ê²°ê³¼
    """
    # ì¡°í•© ê²°ê³¼ ë¡œë“œ
    print(f"\n{'='*100}")
    print(f"ğŸ“‚ ì¡°í•© ê²°ê³¼ ë¡œë“œ: {combinations_file}")
    print(f"{'='*100}")

    combinations_data = load_combinations_result(combinations_file)

    print(f"âœ… ë¡œë“œ ì™„ë£Œ:")
    print(f"  - Retriever: {combinations_data['retriever_display_name']}")
    print(f"  - LLM ê°œìˆ˜: {combinations_data['num_llms']}")
    print(f"  - ì§ˆë¬¸ ê°œìˆ˜: {combinations_data['num_questions']}")
    print(f"  - ì™„ë£Œëœ ì§ˆë¬¸: {combinations_data['num_completed_questions']}")

    # ì§ˆë¬¸ë³„ë¡œ ë‹µë³€ ì¶”ì¶œ
    questions_with_answers = extract_answers_by_question(combinations_data)

    print(f"\nğŸ“Š í‰ê°€ ê°€ëŠ¥í•œ ì§ˆë¬¸ ìˆ˜: {len(questions_with_answers)}")
    for q in questions_with_answers:
        print(f"  Q{q['question_id']}: {len(q['answers'])}ê°œ LLM ë‹µë³€")

    # í‰ê°€ê¸° ì´ˆê¸°í™”
    print(f"\nğŸ¤– Judge ëª¨ë¸ ì´ˆê¸°í™”: {judge_model} (provider: {provider})")
    evaluator = LLMAsJudgeEvaluator(
        judge_model=judge_model,
        provider=provider
    )

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    if output_dir is None:
        combinations_path = Path(combinations_file).parent
        output_dir = combinations_path / "evaluation_results"

    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # ëª¨ë“  ì§ˆë¬¸ì— ëŒ€í•´ í‰ê°€
    all_evaluations = []

    for q_data in questions_with_answers:
        question_id = q_data['question_id']
        question = q_data['question']
        answers = q_data['answers']
        display_names = q_data['display_names']

        print(f"\n{'='*100}")
        print(f"ğŸ“‹ ì§ˆë¬¸ {question_id}/{len(questions_with_answers)}")
        print(f"{'='*100}")
        print(f"â“ {question}")
        print(f"ğŸ“… ë‚ ì§œ í•„í„°: {q_data.get('date_filter', 'N/A')}")
        print(f"ğŸ“„ ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜: {q_data['num_docs']}")
        print(f"ğŸ’¬ í‰ê°€í•  LLM: {', '.join(answers.keys())}")

        # ë¹„êµ í‰ê°€ ì‹¤í–‰
        try:
            comparison_result = evaluator.compare_multiple_answers(
                question=question,
                answers=answers,
                report_type=report_type
            )

            # ì§ˆë¬¸ ë©”íƒ€ë°ì´í„° ì¶”ê°€
            comparison_result['question_id'] = question_id
            comparison_result['date_filter'] = q_data.get('date_filter')
            comparison_result['num_docs'] = q_data['num_docs']
            comparison_result['doc_titles'] = q_data['doc_titles']
            comparison_result['display_names'] = display_names

            all_evaluations.append(comparison_result)

            # ì§ˆë¬¸ë³„ ê²°ê³¼ ì €ì¥
            question_output_file = output_dir / f"question_{question_id}_evaluation.json"
            with open(question_output_file, 'w', encoding='utf-8') as f:
                json.dump(comparison_result, f, indent=2, ensure_ascii=False)

            print(f"\nğŸ’¾ ì§ˆë¬¸ {question_id} í‰ê°€ ê²°ê³¼ ì €ì¥: {question_output_file}")

        except Exception as e:
            print(f"\nâŒ ì§ˆë¬¸ {question_id} í‰ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()

    # ì „ì²´ í‰ê°€ ê²°ê³¼ ì§‘ê³„
    print(f"\n{'='*100}")
    print(f"ğŸ“Š ì „ì²´ í‰ê°€ ê²°ê³¼ ì§‘ê³„")
    print(f"{'='*100}")

    final_result = {
        "combinations_file": str(combinations_file),
        "retriever_name": combinations_data['retriever_name'],
        "retriever_display_name": combinations_data['retriever_display_name'],
        "judge_model": judge_model,
        "judge_provider": provider,
        "report_type": report_type,
        "num_questions_evaluated": len(all_evaluations),
        "num_llms": combinations_data['num_llms'],
        "evaluations": all_evaluations,
        "timestamp": datetime.now().isoformat()
    }

    # ì „ì²´ ê²°ê³¼ ì €ì¥
    overall_output_file = output_dir / "overall_evaluation.json"
    with open(overall_output_file, 'w', encoding='utf-8') as f:
        json.dump(final_result, f, indent=2, ensure_ascii=False)

    print(f"\nğŸ’¾ ì „ì²´ í‰ê°€ ê²°ê³¼ ì €ì¥: {overall_output_file}")

    # ì¢…í•© ìˆœìœ„í‘œ ìƒì„±
    generate_summary_ranking(final_result, output_dir)

    # Judge ëª¨ë¸ í¸í–¥ ë¶„ì„
    print(f"\n{'='*100}")
    print(f"ğŸ” Judge ëª¨ë¸ í¸í–¥ ë¶„ì„ ì‹œì‘")
    print(f"{'='*100}")

    bias_analysis = evaluator.analyze_judge_bias(
        all_evaluations=all_evaluations,
        judge_model=judge_model,
        output_dir=output_dir
    )

    final_result['bias_analysis'] = bias_analysis

    # í¸í–¥ ë¶„ì„ í¬í•¨í•œ ìµœì¢… ê²°ê³¼ ì¬ì €ì¥
    with open(overall_output_file, 'w', encoding='utf-8') as f:
        json.dump(final_result, f, indent=2, ensure_ascii=False)

    return final_result


def generate_summary_ranking(final_result: Dict[str, Any], output_dir: Path):
    """ì¢…í•© ìˆœìœ„í‘œ ìƒì„± (CSV + ë§ˆí¬ë‹¤ìš´)

    ê° LLMì˜ í‰ê·  ì ìˆ˜, ì§ˆë¬¸ë³„ ìˆœìœ„, ìŠ¹ë¥  ë“±ì„ ê³„ì‚°í•˜ì—¬ ì¢…í•© ìˆœìœ„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    print(f"\n{'='*100}")
    print(f"ğŸ† ì¢…í•© ìˆœìœ„í‘œ ìƒì„±")
    print(f"{'='*100}")

    # LLMë³„ ì ìˆ˜ ìˆ˜ì§‘
    llm_scores = {}
    llm_rankings = {}

    for evaluation in final_result['evaluations']:
        question_id = evaluation['question_id']

        for llm_name, result in evaluation['results'].items():
            if llm_name not in llm_scores:
                llm_scores[llm_name] = []
                llm_rankings[llm_name] = []

            llm_scores[llm_name].append({
                'question_id': question_id,
                'score': result['final_score']
            })

        # ìˆœìœ„ ê³„ì‚°
        ranking = evaluation['ranking']
        for rank, (llm_name, score) in enumerate(ranking, 1):
            llm_rankings[llm_name].append(rank)

    # ì¢…í•© í†µê³„ ê³„ì‚°
    summary_rows = []

    for llm_name in llm_scores.keys():
        scores = [s['score'] for s in llm_scores[llm_name]]
        rankings = llm_rankings[llm_name]

        summary_rows.append({
            'LLM': llm_name,
            'í‰ê·  ì ìˆ˜': sum(scores) / len(scores) if scores else 0,
            'ìµœê³  ì ìˆ˜': max(scores) if scores else 0,
            'ìµœì € ì ìˆ˜': min(scores) if scores else 0,
            'í‰ê·  ìˆœìœ„': sum(rankings) / len(rankings) if rankings else 0,
            '1ìœ„ íšŸìˆ˜': rankings.count(1),
            '2ìœ„ íšŸìˆ˜': rankings.count(2),
            '3ìœ„ íšŸìˆ˜': rankings.count(3),
            'í‰ê°€ ì§ˆë¬¸ ìˆ˜': len(scores)
        })

    # í‰ê·  ì ìˆ˜ë¡œ ì •ë ¬
    summary_rows.sort(key=lambda x: x['í‰ê·  ì ìˆ˜'], reverse=True)

    # DataFrame ìƒì„±
    df = pd.DataFrame(summary_rows)

    # CSV ì €ì¥
    csv_file = output_dir / "summary_ranking.csv"
    df.to_csv(csv_file, index=False, encoding='utf-8-sig')
    print(f"ğŸ“Š CSV ì €ì¥: {csv_file}")

    # ë§ˆí¬ë‹¤ìš´ í‘œ ìƒì„±
    md_file = output_dir / "summary_ranking.md"
    with open(md_file, 'w', encoding='utf-8') as f:
        f.write("# LLM ì¢…í•© ìˆœìœ„í‘œ\n\n")
        f.write(f"**í‰ê°€ ì¼ì‹œ**: {final_result['timestamp']}\n\n")
        f.write(f"**Retriever**: {final_result['retriever_display_name']}\n\n")
        f.write(f"**Judge Model**: {final_result['judge_model']} ({final_result['judge_provider']})\n\n")
        f.write(f"**í‰ê°€ ì§ˆë¬¸ ìˆ˜**: {final_result['num_questions_evaluated']}\n\n")
        f.write("---\n\n")
        f.write(df.to_markdown(index=False))
        f.write("\n\n---\n\n")

        # ì§ˆë¬¸ë³„ ìƒì„¸ ìˆœìœ„
        f.write("## ì§ˆë¬¸ë³„ ìˆœìœ„\n\n")
        for evaluation in final_result['evaluations']:
            f.write(f"### ì§ˆë¬¸ {evaluation['question_id']}: {evaluation['question']}\n\n")
            f.write(f"**ë‚ ì§œ í•„í„°**: {evaluation.get('date_filter', 'N/A')}\n\n")

            ranking_data = []
            for rank, (llm_name, score) in enumerate(evaluation['ranking'], 1):
                display_name = evaluation['display_names'].get(llm_name, llm_name)
                ranking_data.append({
                    'ìˆœìœ„': rank,
                    'LLM': display_name,
                    'ì ìˆ˜': f"{score:.2f}"
                })

            ranking_df = pd.DataFrame(ranking_data)
            f.write(ranking_df.to_markdown(index=False))
            f.write("\n\n")

    print(f"ğŸ“ ë§ˆí¬ë‹¤ìš´ ì €ì¥: {md_file}")

    # ì½˜ì†” ì¶œë ¥
    print("\n" + "="*100)
    print("ğŸ† ìµœì¢… ì¢…í•© ìˆœìœ„")
    print("="*100)
    print(df.to_string(index=False))
    print("="*100)


def main():
    import argparse

    parser = argparse.ArgumentParser(
        description="ì¡°í•© ê²°ê³¼ í‰ê°€ - LLM as Judgeë¡œ ì—¬ëŸ¬ LLM ì„±ëŠ¥ ë¹„êµ"
    )
    parser.add_argument(
        "--combinations-file",
        type=str,
        required=True,
        help="all_combinations.json íŒŒì¼ ê²½ë¡œ"
    )
    parser.add_argument(
        "--judge-model",
        type=str,
        default="gpt-5.1",
        help="í‰ê°€ì— ì‚¬ìš©í•  LLM ëª¨ë¸ (ê¸°ë³¸: gpt-4o)"
    )
    parser.add_argument(
        "--provider",
        type=str,
        default="azure_ai",
        choices=["azure_ai", "openrouter"],
        help="LLM ì œê³µì (ê¸°ë³¸: azure_ai)"
    )
    parser.add_argument(
        "--report-type",
        type=str,
        default="weekly_report",
        choices=["weekly_report", "executive_report"],
        help="ë³´ê³ ì„œ íƒ€ì… (ê¸°ë³¸: weekly_report)"
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default=None,
        help="ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: combinations_fileê³¼ ê°™ì€ ë””ë ‰í† ë¦¬/evaluation_results)"
    )

    args = parser.parse_args()

    # í‰ê°€ ì‹¤í–‰
    result = evaluate_all_questions(
        combinations_file=args.combinations_file,
        judge_model=args.judge_model,
        provider=args.provider,
        report_type=args.report_type,
        output_dir=args.output_dir
    )

    print("\nâœ… ëª¨ë“  í‰ê°€ ì™„ë£Œ!")


if __name__ == "__main__":
    main()
