#!/usr/bin/env python3
"""í‰ê°€ ì‹¤í–‰ í—¬í¼ ëª¨ë“ˆ

ë¦¬íŠ¸ë¦¬ë²„ì™€ í”„ë¡¬í”„íŠ¸ ì¡°í•©ë³„ë¡œ ì‹¤ì œ í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

import json
import os
from typing import Dict, Any, List, Tuple
from datetime import datetime
from langfuse import Langfuse
from langfuse.decorators import observe, langfuse_context


class EvaluationRunner:
    """í‰ê°€ ì‹¤í–‰ í´ë˜ìŠ¤"""

    def __init__(
        self,
        embedding_preset: str,
        retriever_type: str,
        system_prompt: str,
        answer_generation_prompt: str,
        report_type: str
    ):
        """
        Args:
            embedding_preset: ì„ë² ë”© ëª¨ë¸ í”„ë¦¬ì…‹ (upstage, qwen, openai, bge_m3)
            retriever_type: ë¦¬íŠ¸ë¦¬ë²„ íƒ€ì…
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            answer_generation_prompt: ë‹µë³€ ìƒì„± í”„ë¡¬í”„íŠ¸
            report_type: ë³´ê³ ì„œ íƒ€ì… (weekly_report, executive_report)
        """
        self.embedding_preset = embedding_preset
        self.retriever_type = retriever_type
        self.system_prompt = system_prompt
        self.answer_generation_prompt = answer_generation_prompt
        self.report_type = report_type

        # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
        os.environ['MODEL_PRESET'] = embedding_preset

        # Langfuse ì´ˆê¸°í™”
        self.langfuse = Langfuse(
            public_key=os.getenv("LANGFUSE_PUBLIC_KEY"),
            secret_key=os.getenv("LANGFUSE_SECRET_KEY"),
            host=os.getenv("LANGFUSE_HOST", "https://cloud.langfuse.com")
        )

        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”ëŠ” ì§€ì—° ë¡œë”© (í•„ìš” ì‹œ êµ¬í˜„)
        self.retriever = None
        self.llm = None

    def initialize_retriever(self):
        """ë¦¬íŠ¸ë¦¬ë²„ ì´ˆê¸°í™”

        TODO: ì‹¤ì œ ë¦¬íŠ¸ë¦¬ë²„ íƒ€ì…ì— ë”°ë¼ ì´ˆê¸°í™” ë¡œì§ êµ¬í˜„
        """
        # from retrievers import get_retriever
        # self.retriever = get_retriever(self.retriever_type)
        pass

    def initialize_llm(self):
        """LLM ì´ˆê¸°í™”

        TODO: ì‹¤ì œ LLM ì´ˆê¸°í™” ë¡œì§ êµ¬í˜„
        """
        # from langchain_openai import ChatOpenAI
        # self.llm = ChatOpenAI(...)
        pass

    @observe()
    def retrieve_documents(self, question: str) -> List[Dict[str, Any]]:
        """ë¬¸ì„œ ê²€ìƒ‰

        Args:
            question: ì§ˆë¬¸

        Returns:
            ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        if self.retriever is None:
            self.initialize_retriever()

        # TODO: ì‹¤ì œ ê²€ìƒ‰ ë¡œì§ êµ¬í˜„
        # documents = self.retriever.get_relevant_documents(question)
        # return documents

        # ì„ì‹œ ë°˜í™˜
        return []

    @observe()
    def generate_answer(self, question: str, context: str) -> str:
        """ë‹µë³€ ìƒì„±

        Args:
            question: ì§ˆë¬¸
            context: ê²€ìƒ‰ëœ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸

        Returns:
            ìƒì„±ëœ ë‹µë³€
        """
        if self.llm is None:
            self.initialize_llm()

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = self.answer_generation_prompt.format(
            question=question,
            context=context
        )

        # TODO: ì‹¤ì œ LLM í˜¸ì¶œ ë¡œì§ êµ¬í˜„
        # messages = [
        #     {"role": "system", "content": self.system_prompt},
        #     {"role": "user", "content": prompt}
        # ]
        # response = self.llm.invoke(messages)
        # return response.content

        # ì„ì‹œ ë°˜í™˜
        return ""

    def evaluate_single_qa(
        self,
        question: str,
        ground_truth_answer: str,
        ground_truth_contexts: List[str]
    ) -> Dict[str, float]:
        """ë‹¨ì¼ Q&Aì— ëŒ€í•œ í‰ê°€

        Args:
            question: ì§ˆë¬¸
            ground_truth_answer: ì •ë‹µ
            ground_truth_contexts: ì •ë‹µ ì»¨í…ìŠ¤íŠ¸

        Returns:
            í‰ê°€ ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬ (precision, recall, faithfulness)
        """
        # 1. ë¬¸ì„œ ê²€ìƒ‰
        retrieved_docs = self.retrieve_documents(question)

        # 2. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = "\n\n".join([doc.get("content", "") for doc in retrieved_docs])

        # 3. ë‹µë³€ ìƒì„±
        generated_answer = self.generate_answer(question, context)

        # 4. í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°
        # TODO: ì‹¤ì œ í‰ê°€ ë¡œì§ êµ¬í˜„
        # - Precision: ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ê´€ë ¨ì„±
        # - Recall: ê´€ë ¨ ë¬¸ì„œë¥¼ ì–¼ë§ˆë‚˜ ì°¾ì•˜ëŠ”ê°€
        # - Faithfulness: ìƒì„±ëœ ë‹µë³€ì´ ì»¨í…ìŠ¤íŠ¸ì— ì¶©ì‹¤í•œê°€

        metrics = {
            "precision": 0.0,
            "recall": 0.0,
            "faithfulness": 0.0
        }

        # Langfuseì— ê¸°ë¡
        langfuse_context.update_current_trace(
            name=f"evaluation_{self.report_type}",
            metadata={
                "embedding_preset": self.embedding_preset,
                "retriever_type": self.retriever_type,
                "report_type": self.report_type
            }
        )

        langfuse_context.score_current_trace(
            name="precision",
            value=metrics["precision"]
        )

        langfuse_context.score_current_trace(
            name="recall",
            value=metrics["recall"]
        )

        langfuse_context.score_current_trace(
            name="faithfulness",
            value=metrics["faithfulness"]
        )

        return metrics

    def run_evaluation(self, dataset_path: str) -> Dict[str, Any]:
        """ì „ì²´ ë°ì´í„°ì…‹ì— ëŒ€í•œ í‰ê°€ ì‹¤í–‰

        Args:
            dataset_path: í‰ê°€ ë°ì´í„°ì…‹ ê²½ë¡œ

        Returns:
            í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        # ë°ì´í„°ì…‹ ë¡œë“œ
        with open(dataset_path, 'r', encoding='utf-8') as f:
            dataset = json.load(f)

        print(f"ğŸ“Š ë°ì´í„°ì…‹ ë¡œë“œ: {len(dataset)} ê°œ ìƒ˜í”Œ")

        all_metrics = {
            "precision": [],
            "recall": [],
            "faithfulness": []
        }

        # ê° Q&Aì— ëŒ€í•´ í‰ê°€
        for i, qa_pair in enumerate(dataset, 1):
            print(f"\n[{i}/{len(dataset)}] í‰ê°€ ì¤‘...", end=" ")

            metrics = self.evaluate_single_qa(
                question=qa_pair.get("question", ""),
                ground_truth_answer=qa_pair.get("answer", ""),
                ground_truth_contexts=qa_pair.get("contexts", [])
            )

            for metric_name, value in metrics.items():
                all_metrics[metric_name].append(value)

            print(f"âœ“ (P={metrics['precision']:.2f}, R={metrics['recall']:.2f}, F={metrics['faithfulness']:.2f})")

        # í‰ê·  ê³„ì‚°
        avg_metrics = {
            metric_name: sum(values) / len(values) if values else 0.0
            for metric_name, values in all_metrics.items()
        }

        result = {
            "embedding_preset": self.embedding_preset,
            "retriever_type": self.retriever_type,
            "report_type": self.report_type,
            "total_samples": len(dataset),
            "avg_metrics": avg_metrics,
            "all_metrics": all_metrics,
            "timestamp": datetime.now().isoformat()
        }

        return result


def create_evaluation_runner(
    embedding_preset: str,
    retriever_type: str,
    system_prompt_path: str,
    answer_generation_prompt_path: str,
    report_type: str
) -> EvaluationRunner:
    """í‰ê°€ ëŸ¬ë„ˆ ìƒì„± í—¬í¼ í•¨ìˆ˜

    Args:
        embedding_preset: ì„ë² ë”© ëª¨ë¸ í”„ë¦¬ì…‹
        retriever_type: ë¦¬íŠ¸ë¦¬ë²„ íƒ€ì…
        system_prompt_path: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ íŒŒì¼ ê²½ë¡œ
        answer_generation_prompt_path: ë‹µë³€ ìƒì„± í”„ë¡¬í”„íŠ¸ íŒŒì¼ ê²½ë¡œ
        report_type: ë³´ê³ ì„œ íƒ€ì…

    Returns:
        EvaluationRunner ì¸ìŠ¤í„´ìŠ¤
    """
    base_dir = Path(__file__).parent.parent

    # í”„ë¡¬í”„íŠ¸ ë¡œë“œ
    with open(base_dir / system_prompt_path, 'r', encoding='utf-8') as f:
        system_prompt = f.read().strip()

    with open(base_dir / answer_generation_prompt_path, 'r', encoding='utf-8') as f:
        answer_generation_prompt = f.read().strip()

    return EvaluationRunner(
        embedding_preset=embedding_preset,
        retriever_type=retriever_type,
        system_prompt=system_prompt,
        answer_generation_prompt=answer_generation_prompt,
        report_type=report_type
    )
