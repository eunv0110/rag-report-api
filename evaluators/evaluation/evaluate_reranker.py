#!/usr/bin/env python3
"""BGE-M3 + RRF Ensemble + Qwen3-Reranker-4B ë³´ê³ ì„œ ìë™ ìƒì„± ë° ë¹„êµ (Langfuse ì—°ë™)

BGE-M3 RRF Ensembleë¡œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•œ í›„, Qwen3-Reranker-4Bë¡œ ì¬ìˆœìœ„í™”í•˜ì—¬
ìµœì¢… kê°œ(6, 8, 10)ì˜ ë¬¸ì„œë¥¼ LLMì— ì „ë‹¬í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.

ë³´ê³ ì„œ íƒ€ì…:
- executive: ìµœì¢…ë³´ê³ ì„œ
- weekly: ì£¼ê°„ë³´ê³ ì„œ
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

from dotenv import load_dotenv
load_dotenv()

import json
import os
from datetime import datetime
from typing import List, Dict, Any
from langchain.chat_models import init_chat_model
from concurrent.futures import ThreadPoolExecutor, as_completed
from sentence_transformers import CrossEncoder
from FlagEmbedding import FlagReranker

from config.settings import AZURE_AI_CREDENTIAL, AZURE_AI_ENDPOINT, EVALUATION_CONFIG
from utils.langfuse_utils import get_langfuse_client
from utils.date_utils import parse_date_range, extract_date_filter_from_question
from utils.common_utils import (
    load_evaluation_dataset,
    create_trace_and_generation,
    add_retrieval_quality_score,
    save_embedding_cache
)

# ë¦¬íŠ¸ë¦¬ë²„ ì„í¬íŠ¸
from utils.retriever_factory import create_retriever_from_config


# Reranker ëª¨ë¸ ì´ˆê¸°í™” (ì „ì—­ ë³€ìˆ˜ë¡œ í•œ ë²ˆë§Œ ë¡œë“œ)
RERANKER_MODEL = None
RERANKER_TYPE = None  # í˜„ì¬ ë¡œë“œëœ reranker íƒ€ì…

def format_queries(query: str, instruction: str = None) -> str:
    """Qwen3-Rerankerë¥¼ ìœ„í•œ ì¿¼ë¦¬ í¬ë§·íŒ…"""
    prefix = '<|im_start|>system\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be "yes" or "no".<|im_end|>\n<|im_start|>user\n'
    if instruction is None:
        instruction = "Given a query, retrieve relevant passages that answer the query"
    return f"{prefix}<Instruct>: {instruction}\n<Query>: {query}\n"


def format_document(document: str) -> str:
    """Qwen3-Rerankerë¥¼ ìœ„í•œ ë¬¸ì„œ í¬ë§·íŒ…"""
    suffix = "<|im_end|>\n<|im_start|>assistant\n<think>\n\n</think>\n\n"
    return f"<Document>: {document}{suffix}"


def get_optimal_batch_size():
    """GPU ë©”ëª¨ë¦¬ì— ë”°ë¥¸ ìµœì  ë°°ì¹˜ í¬ê¸° ê³„ì‚°"""
    import torch
    # VLLMê³¼ ê³µì¡´í•˜ê¸° ìœ„í•´ ë°°ì¹˜ í¬ê¸°ë¥¼ 16ìœ¼ë¡œ ê³ ì •
    return 16


def get_reranker(reranker_type: str = "qwen3"):
    """Reranker ëª¨ë¸ ë¡œë“œ (ìºì‹±)

    Args:
        reranker_type: 'qwen3', 'bge', ë˜ëŠ” 'korean' ì¤‘ ì„ íƒ

    Returns:
        ë¡œë“œëœ reranker ëª¨ë¸
    """
    global RERANKER_MODEL, RERANKER_TYPE

    # ì´ë¯¸ ë¡œë“œëœ ëª¨ë¸ê³¼ íƒ€ì…ì´ ê°™ìœ¼ë©´ ì¬ì‚¬ìš©
    if RERANKER_MODEL is not None and RERANKER_TYPE == reranker_type:
        return RERANKER_MODEL

    # ë‹¤ë¥¸ íƒ€ì…ì˜ ëª¨ë¸ì„ ë¡œë“œí•´ì•¼ í•˜ë©´ ê¸°ì¡´ ëª¨ë¸ ì œê±°
    if RERANKER_MODEL is not None and RERANKER_TYPE != reranker_type:
        print(f"ğŸ”„ ê¸°ì¡´ {RERANKER_TYPE} ëª¨ë¸ ì–¸ë¡œë“œ ì¤‘...")
        del RERANKER_MODEL
        RERANKER_MODEL = None
        import gc
        import torch
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    import torch
    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    if reranker_type == "qwen3":
        print(f"ğŸ”„ Qwen3-Reranker-4B-seq-cls ëª¨ë¸ ë¡œë”© ì¤‘... (device: {device})")
        RERANKER_MODEL = CrossEncoder(
            "tomaarsen/Qwen3-Reranker-4B-seq-cls",
            max_length=8192,
            device=device,
            trust_remote_code=True
        )
        print("âœ… Qwen3-Reranker-4B-seq-cls ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

    elif reranker_type == "bge":
        print(f"ğŸ”„ BGE-Reranker-v2-m3 ëª¨ë¸ ë¡œë”© ì¤‘... (device: {device})")
        RERANKER_MODEL = FlagReranker(
            'BAAI/bge-reranker-v2-m3',
            use_fp16=True,
            device=device
        )
        print("âœ… BGE-Reranker-v2-m3 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

    elif reranker_type == "korean":
        print(f"ğŸ”„ Korean-Reranker-8k ëª¨ë¸ ë¡œë”© ì¤‘... (device: {device})")
        RERANKER_MODEL = FlagReranker(
            'upskyy/ko-reranker-8k',
            use_fp16=True,
            device=device
        )
        print("âœ… Korean-Reranker-8k ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

    else:
        raise ValueError(f"Unknown reranker_type: {reranker_type}. Choose 'qwen3', 'bge', or 'korean'")

    RERANKER_TYPE = reranker_type

    # ìµœì  ë°°ì¹˜ í¬ê¸° ì¶œë ¥
    optimal_bs = get_optimal_batch_size()
    print(f"ğŸ’¡ ê¶Œì¥ ë°°ì¹˜ í¬ê¸°: {optimal_bs}")

    return RERANKER_MODEL


def rerank_documents(query: str, docs: list, top_k: int = 6, batch_size: int = None, reranker_type: str = "qwen3") -> list:
    """Reranker ëª¨ë¸ë¡œ ë¬¸ì„œ ì¬ìˆœìœ„í™”

    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬
        docs: ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (langchain Document ê°ì²´)
        top_k: ìµœì¢… ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜
        batch_size: ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸° (Noneì´ë©´ ìë™ ê³„ì‚°)
        reranker_type: 'qwen3', 'bge', ë˜ëŠ” 'korean' ì¤‘ ì„ íƒ

    Returns:
        ì¬ìˆœìœ„í™”ëœ ìƒìœ„ kê°œ ë¬¸ì„œ
    """
    import torch
    reranker = get_reranker(reranker_type)

    # ë°°ì¹˜ í¬ê¸° ìë™ ì„¤ì •
    if batch_size is None:
        batch_size = get_optimal_batch_size()

    # Reranker íƒ€ì…ì— ë”°ë¼ ë‹¤ë¥¸ í¬ë§· ì‚¬ìš©
    if reranker_type == "qwen3":
        # Qwen3-Reranker í¬ë§·
        formatted_query = format_queries(query)
        pairs = [
            [formatted_query, format_document(doc.page_content)]
            for doc in docs
        ]

        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì¬ìˆœìœ„í™” ì ìˆ˜ ê³„ì‚° (ë©”ëª¨ë¦¬ ì ˆì•½)
        all_scores = []
        for i in range(0, len(pairs), batch_size):
            batch_pairs = pairs[i:i + batch_size]
            batch_scores = reranker.predict(batch_pairs)
            all_scores.extend(batch_scores)

            # ë°°ì¹˜ ì²˜ë¦¬ í›„ GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

    elif reranker_type in ["bge", "korean"]:
        # BGE/Korean Reranker í¬ë§· (FlagReranker)
        pairs = [[query, doc.page_content] for doc in docs]
        all_scores = reranker.compute_score(pairs)

    else:
        raise ValueError(f"Unknown reranker_type: {reranker_type}")

    # ì ìˆ˜ì™€ ë¬¸ì„œë¥¼ í•¨ê»˜ ì •ë ¬
    doc_score_pairs = list(zip(docs, all_scores))
    doc_score_pairs.sort(key=lambda x: x[1], reverse=True)

    # ìƒìœ„ kê°œ ë¬¸ì„œë§Œ ë°˜í™˜
    reranked_docs = [doc for doc, score in doc_score_pairs[:top_k]]

    reranker_name = {"qwen3": "Qwen3", "bge": "BGE", "korean": "Korean"}[reranker_type]
    print(f"\nğŸ”„ {reranker_name} Reranking ì™„ë£Œ: {len(docs)}ê°œ â†’ {len(reranked_docs)}ê°œ (ë°°ì¹˜ í¬ê¸°: {batch_size})")
    print("Top 3 Reranked Scores:")
    for i, (doc, score) in enumerate(doc_score_pairs[:3], 1):
        print(f"  {i}. {doc.metadata.get('page_title', 'Unknown')}: {score:.4f}")

    return reranked_docs


def get_report_config(report_type: str) -> Dict[str, Any]:
    """ë³´ê³ ì„œ íƒ€ì…ì— ë”°ë¥¸ ì„¤ì • ë°˜í™˜"""
    if report_type not in ['executive', 'weekly']:
        raise ValueError(f"Invalid report_type: {report_type}. Must be 'executive' or 'weekly'")

    return {
        'test_questions': EVALUATION_CONFIG['test_questions'][f'{report_type}_report'],
        'retriever_configs': EVALUATION_CONFIG['simple_test_retrievers'][f'{report_type}_report'],
        'default_retriever_index': EVALUATION_CONFIG['default_retriever_index'][f'{report_type}_report'],
        'llm_configs': EVALUATION_CONFIG['test_llms'],
        'prompt_dir': f'{report_type}_report'
    }


def load_prompt(prompt_file: str, report_type: str) -> str:
    """í”„ë¡¬í”„íŠ¸ íŒŒì¼ ë¡œë“œ"""
    # evaluators/evaluation/test_report_reranker.pyì—ì„œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¡œ ì´ë™
    project_root = Path(__file__).parent.parent.parent
    prompt_path = project_root / "prompts" / "templates" / "service" / f"{report_type}_report" / prompt_file
    with open(prompt_path, 'r', encoding='utf-8') as f:
        return f.read()


def load_test_questions(n: int = 5) -> List[Dict[str, Any]]:
    """í…ŒìŠ¤íŠ¸ìš© ì§ˆë¬¸ ë¡œë“œ (ë ˆê±°ì‹œ, interactive ëª¨ë“œìš©)"""
    qa_file = Path(__file__).parent.parent / "data" / "evaluation" / "llm_generated_qa_v2.json"

    with open(qa_file, 'r', encoding='utf-8') as f:
        qa_data = json.load(f)

    return qa_data[:n]


def count_tokens(text: str, model_id: str = "gpt-4") -> int:
    """í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ë¥¼ ê³„ì‚°

    Args:
        text: í† í°ì„ ê³„ì‚°í•  í…ìŠ¤íŠ¸
        model_id: ëª¨ë¸ ID (ê¸°ë³¸: gpt-4)

    Returns:
        í† í° ìˆ˜
    """
    try:
        import tiktoken

        # ëª¨ë¸ì— ë”°ë¥¸ ì¸ì½”ë”© ì„ íƒ
        model_lower = model_id.lower()

        if "claude" in model_lower or "anthropic" in model_lower:
            # ClaudeëŠ” GPT-4ì™€ ìœ ì‚¬í•œ í† í°í™” ì‚¬ìš©
            encoding = tiktoken.get_encoding("cl100k_base")
        elif "gpt-4" in model_lower or "gpt-3.5" in model_lower or "gpt-35" in model_lower:
            # GPT-4, GPT-3.5 ê³„ì—´
            encoding = tiktoken.get_encoding("cl100k_base")
        elif "deepseek" in model_lower:
            # DeepSeekëŠ” cl100k_base ì‚¬ìš© (GPT-4ì™€ í˜¸í™˜)
            encoding = tiktoken.get_encoding("cl100k_base")
        elif "qwen" in model_lower or "qwq" in model_lower:
            # Qwen ê³„ì—´ë„ cl100k_base ì‚¬ìš©
            encoding = tiktoken.get_encoding("cl100k_base")
        elif "llama" in model_lower or "phi" in model_lower or "mistral" in model_lower:
            # Llama, Phi, Mistral ë“± ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ë“¤
            encoding = tiktoken.get_encoding("cl100k_base")
        else:
            # ê¸°ë³¸ê°’ìœ¼ë¡œ cl100k_base ì‚¬ìš©
            encoding = tiktoken.get_encoding("cl100k_base")

        return len(encoding.encode(text))
    except Exception as e:
        # tiktoken ì‹¤íŒ¨ ì‹œ ëŒ€ëµì ì¸ ê³„ì‚° (1 í† í° â‰ˆ 4 ê¸€ì)
        print(f"âš ï¸ tiktoken ê³„ì‚° ì‹¤íŒ¨, ê·¼ì‚¬ê°’ ì‚¬ìš©: {e}")
        return len(text) // 4


def calculate_message_tokens(messages: List[Dict[str, str]], model_id: str = "gpt-4") -> int:
    """ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ì˜ ì´ í† í° ìˆ˜ë¥¼ ê³„ì‚°

    Args:
        messages: ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ [{"role": "system", "content": "..."}, ...]
        model_id: ëª¨ë¸ ID

    Returns:
        ì´ ì…ë ¥ í† í° ìˆ˜
    """
    total_tokens = 0

    # ë©”ì‹œì§€ êµ¬ì¡° ì˜¤ë²„í—¤ë“œ (role, formatting ë“±)
    # OpenAI ê¸°ì¤€: ë©”ì‹œì§€ë‹¹ ì•½ 3~4 í† í° ì˜¤ë²„í—¤ë“œ
    message_overhead = 3

    for message in messages:
        content = message.get("content", "")
        role = message.get("role", "")

        # role í† í° ê³„ì‚°
        total_tokens += count_tokens(role, model_id)
        # content í† í° ê³„ì‚°
        total_tokens += count_tokens(content, model_id)
        # ë©”ì‹œì§€ í¬ë§· ì˜¤ë²„í—¤ë“œ
        total_tokens += message_overhead

    # ì „ì²´ ëŒ€í™” êµ¬ì¡° ì˜¤ë²„í—¤ë“œ
    total_tokens += 3

    return total_tokens


def calculate_cost(input_tokens: int, output_tokens: int, model_id: str) -> Dict[str, float]:
    """í† í° ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¹„ìš© ê³„ì‚°

    Args:
        input_tokens: ì…ë ¥ í† í° ìˆ˜
        output_tokens: ì¶œë ¥ í† í° ìˆ˜
        model_id: ëª¨ë¸ ID

    Returns:
        ë¹„ìš© ì •ë³´ ë”•ì…”ë„ˆë¦¬ {"input_cost": 0.001, "output_cost": 0.002, "total_cost": 0.003, "currency": "USD"}
    """
    # ëª¨ë¸ë³„ ê°€ê²© (1M í† í°ë‹¹ USD)
    # ì°¸ê³ : 2024-2025ë…„ ê¸°ì¤€ ê°€ê²©, ì‹¤ì œ ê°€ê²©ì€ ê³µì‹ ë¬¸ì„œ í™•ì¸ í•„ìš”
    PRICING = {
        # OpenAI GPT-4
        "gpt-4": {"input": 30.0, "output": 60.0},
        "gpt-4-turbo": {"input": 10.0, "output": 30.0},
        "gpt-4o": {"input": 2.5, "output": 10.0},
        "gpt-4o-mini": {"input": 0.15, "output": 0.6},
        "gpt-3.5-turbo": {"input": 0.5, "output": 1.5},

        # Anthropic Claude
        "claude-3-opus": {"input": 15.0, "output": 75.0},
        "claude-3-sonnet": {"input": 3.0, "output": 15.0},
        "claude-3-haiku": {"input": 0.25, "output": 1.25},
        "claude-3.5-sonnet": {"input": 3.0, "output": 15.0},
        "claude-3.5-haiku": {"input": 0.8, "output": 4.0},

        # DeepSeek (ë§¤ìš° ì €ë ´)
        "deepseek-chat": {"input": 0.14, "output": 0.28},
        "deepseek-v3": {"input": 0.27, "output": 1.1},
        "deepseek-v3.1": {"input": 0.27, "output": 1.1},

        # Qwen
        "qwen-max": {"input": 0.4, "output": 1.2},
        "qwen-plus": {"input": 0.08, "output": 0.24},
        "qwen-turbo": {"input": 0.03, "output": 0.06},

        # Meta Llama
        "llama-3-70b": {"input": 0.9, "output": 0.9},
        "llama-3-8b": {"input": 0.2, "output": 0.2},

        # Microsoft Phi
        "phi-3-mini": {"input": 0.1, "output": 0.1},
        "phi-3-medium": {"input": 0.2, "output": 0.2},

        # Mistral
        "mistral-large": {"input": 3.0, "output": 9.0},
        "mistral-medium": {"input": 2.7, "output": 8.1},
        "mistral-small": {"input": 0.2, "output": 0.6},
    }

    model_lower = model_id.lower()

    # ëª¨ë¸ ë§¤ì¹­
    pricing = None
    for model_key, price in PRICING.items():
        if model_key in model_lower:
            pricing = price
            break

    # ë§¤ì¹­ë˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ê°’ (GPT-4o ê°€ê²©)
    if pricing is None:
        pricing = {"input": 2.5, "output": 10.0}
        print(f"âš ï¸ ëª¨ë¸ '{model_id}'ì˜ ê°€ê²© ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ê°’(GPT-4o) ì‚¬ìš©")

    # ë¹„ìš© ê³„ì‚° (1M í† í°ë‹¹ ê°€ê²© â†’ ì‹¤ì œ í† í° ìˆ˜ë¡œ í™˜ì‚°)
    input_cost = (input_tokens / 1_000_000) * pricing["input"]
    output_cost = (output_tokens / 1_000_000) * pricing["output"]
    total_cost = input_cost + output_cost

    return {
        "input_cost": round(input_cost, 6),
        "output_cost": round(output_cost, 6),
        "total_cost": round(total_cost, 6),
        "currency": "USD"
    }


def generate_answer_with_llm(query: str, docs: list, llm_config: dict, report_type: str, langfuse=None, question_id: int = None, version: str = "v1") -> str:
    """ì§€ì •ëœ LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±"""
    from openai import OpenAI
    from config.settings import OPENROUTER_API_KEY, OPENROUTER_BASE_URL

    # LLMë³„ ì»¨í…ìŠ¤íŠ¸ ì œí•œ ì²˜ë¦¬
    if 'max_docs' in llm_config and llm_config['max_docs']:
        max_docs = llm_config['max_docs']
        if len(docs) > max_docs:
            docs = docs[:max_docs]
            print(f"  âš ï¸ {llm_config['display_name']} ì»¨í…ìŠ¤íŠ¸ ì œí•œìœ¼ë¡œ ë¬¸ì„œ ìˆ˜ë¥¼ {len(docs)}ê°œë¡œ ì¤„ì˜€ìŠµë‹ˆë‹¤.")

    # Context êµ¬ì„±
    context_parts = []
    for i, doc in enumerate(docs, 1):
        title = doc.metadata.get('page_title', 'Unknown')
        content = doc.page_content
        context_parts.append(f"[ë¬¸ì„œ {i}] {title}\n{content}\n")

    context_text = "\n".join(context_parts)

    # í”„ë¡¬í”„íŠ¸ ë¡œë“œ
    system_prompt = load_prompt("system_prompt.txt", report_type)
    answer_generation_template = load_prompt("answer_generation_prompt.txt", report_type)

    # í…œí”Œë¦¿ì— ë³€ìˆ˜ ëŒ€ì…
    user_prompt = answer_generation_template.replace("{context}", context_text).replace("{question}", query)

    # Azure AI ì„¤ì •
    os.environ['AZURE_AI_CREDENTIAL'] = AZURE_AI_CREDENTIAL
    os.environ['AZURE_AI_ENDPOINT'] = AZURE_AI_ENDPOINT

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]

    # LLM íƒ€ì…ì— ë”°ë¼ ë‹¤ë¥´ê²Œ í˜¸ì¶œ
    if llm_config['model_id'].startswith('anthropic:'):
        # Anthropic Claude ëª¨ë¸ - OpenRouter ì‚¬ìš©
        model_name = llm_config['model_id'].split(':')[1]
        client = OpenAI(
            api_key=OPENROUTER_API_KEY,
            base_url=OPENROUTER_BASE_URL
        )

        # Langfuseë¡œ ë‹µë³€ ìƒì„± ê¸°ë¡
        if langfuse and question_id:
            with langfuse.start_as_current_observation(
                as_type='generation',
                name=f"generation_{llm_config['name']}_q{question_id}",
                model=llm_config['model_id'],
                input={"question": query, "context": context_text[:500] + "..." if len(context_text) > 500 else context_text},
                metadata={"llm": llm_config['name'], "num_docs": len(docs)}
            ) as generation:
                response = client.chat.completions.create(
                    model=f"anthropic/{model_name}",
                    messages=messages,
                    max_tokens=1000,
                    temperature=0
                )
                answer = response.choices[0].message.content

                # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œ
                usage_dict = None
                if hasattr(response, 'usage') and response.usage:
                    usage_dict = {
                        "input": response.usage.prompt_tokens,
                        "output": response.usage.completion_tokens,
                        "total": response.usage.total_tokens
                    }

                # í† í° ì§ì ‘ ê³„ì‚° (ì¶”ê°€)
                calculated_input_tokens = calculate_message_tokens(messages, llm_config['model_id'])
                calculated_output_tokens = count_tokens(answer, llm_config['model_id'])
                calculated_total_tokens = calculated_input_tokens + calculated_output_tokens

                # ë¹„ìš© ê³„ì‚°
                cost_info = calculate_cost(calculated_input_tokens, calculated_output_tokens, llm_config['model_id'])

                generation.update(
                    output={"answer": answer},
                    usage=usage_dict if usage_dict else None
                )

                langfuse.update_current_trace(
                    name=f"llm_comparison_{llm_config['name']}_q{question_id}",
                    tags=[llm_config['name'], "reranker_test", f"{report_type}_report", version],
                    input={"question": query},
                    output={"answer": answer},
                    metadata={
                        "llm_name": llm_config['name'],
                        "llm_display_name": llm_config['display_name'],
                        "llm_description": llm_config['description'],
                        "model_id": llm_config['model_id'],
                        "question_id": question_id,
                        "num_docs": len(docs),
                        "report_type": report_type,
                        "calculated_tokens": {
                            "input": calculated_input_tokens,
                            "output": calculated_output_tokens,
                            "total": calculated_total_tokens
                        },
                        "estimated_cost": cost_info
                    }
                )
        else:
            response = client.chat.completions.create(
                model=f"anthropic/{model_name}",
                messages=messages,
                max_tokens=1000,
                temperature=0
            )
            answer = response.choices[0].message.content
    else:
        # Azure AI ëª¨ë¸ (LangChain)
        model = init_chat_model(
            llm_config['model_id'],
            temperature=0,
            max_completion_tokens=1000
        )

        # Langfuseë¡œ ë‹µë³€ ìƒì„± ê¸°ë¡
        if langfuse and question_id:
            with langfuse.start_as_current_observation(
                as_type='generation',
                name=f"generation_{llm_config['name']}_q{question_id}",
                model=llm_config['model_id'],
                input={"question": query, "context": context_text[:500] + "..." if len(context_text) > 500 else context_text},
                metadata={"llm": llm_config['name'], "num_docs": len(docs)}
            ) as generation:
                response = model.invoke(messages)
                answer = response.content

                # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œ
                usage_dict = None
                if hasattr(response, 'usage_metadata') and response.usage_metadata:
                    usage_dict = {
                        "input": response.usage_metadata.get('input_tokens', 0),
                        "output": response.usage_metadata.get('output_tokens', 0),
                        "total": response.usage_metadata.get('total_tokens', 0)
                    }
                elif hasattr(response, 'response_metadata') and response.response_metadata:
                    token_usage = response.response_metadata.get('token_usage', {})
                    if token_usage:
                        usage_dict = {
                            "input": token_usage.get('prompt_tokens', 0),
                            "output": token_usage.get('completion_tokens', 0),
                            "total": token_usage.get('total_tokens', 0)
                        }

                # í† í° ì§ì ‘ ê³„ì‚° (ì¶”ê°€)
                calculated_input_tokens = calculate_message_tokens(messages, llm_config['model_id'])
                calculated_output_tokens = count_tokens(answer, llm_config['model_id'])
                calculated_total_tokens = calculated_input_tokens + calculated_output_tokens

                # ë¹„ìš© ê³„ì‚°
                cost_info = calculate_cost(calculated_input_tokens, calculated_output_tokens, llm_config['model_id'])

                generation.update(
                    output={"answer": answer},
                    usage=usage_dict if usage_dict else None
                )

                langfuse.update_current_trace(
                    name=f"llm_comparison_{llm_config['name']}_q{question_id}",
                    tags=[llm_config['name'], "reranker_test", f"{report_type}_report", version],
                    input={"question": query},
                    output={"answer": answer},
                    metadata={
                        "llm_name": llm_config['name'],
                        "llm_display_name": llm_config['display_name'],
                        "llm_description": llm_config['description'],
                        "model_id": llm_config['model_id'],
                        "question_id": question_id,
                        "num_docs": len(docs),
                        "report_type": report_type,
                        "calculated_tokens": {
                            "input": calculated_input_tokens,
                            "output": calculated_output_tokens,
                            "total": calculated_total_tokens
                        },
                        "estimated_cost": cost_info
                    }
                )
        else:
            response = model.invoke(messages)
            answer = response.content

    return answer


def retrieve_and_rerank_documents(
    question: str,
    report_type: str,
    top_k: int = 6,
    date_filter: tuple = None,
    langfuse=None,
    question_id: int = None,
    batch_size: int = None,
    retriever_type: str = "bge-m3",
    reranker_type: str = "qwen3"
):
    """RRF Ensembleë¡œ ê²€ìƒ‰ í›„ Rerankerë¡œ ì¬ìˆœìœ„í™”

    Args:
        question: ì§ˆë¬¸
        report_type: ë³´ê³ ì„œ íƒ€ì… ('executive' ë˜ëŠ” 'weekly')
        top_k: ìµœì¢… ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜
        date_filter: ë‚ ì§œ í•„í„°
        langfuse: Langfuse í´ë¼ì´ì–¸íŠ¸
        question_id: ì§ˆë¬¸ ID
        batch_size: Reranker ë°°ì¹˜ í¬ê¸° (Noneì´ë©´ ìë™)
        retriever_type: ë¦¬íŠ¸ë¦¬ë²„ íƒ€ì… ('bge-m3', 'openai', 'openai-rrf-multiquery' ë“±)
        reranker_type: Reranker íƒ€ì… ('qwen3', 'bge', 'korean')

    Returns:
        ì¬ìˆœìœ„í™”ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
    """
    import gc
    import torch

    # ë°°ì¹˜ í¬ê¸° ìë™ ì„¤ì •
    if batch_size is None:
        batch_size = get_optimal_batch_size()

    # ë©”ëª¨ë¦¬ ì •ë¦¬
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    # Retriever íƒ€ì…ì— ë”°ë¥¸ ì„¤ì •
    if retriever_type == "openai-rrf-multiquery":
        retriever_config = {
            "name": "openai-rrf-multiquery",
            "display_name": "OpenAI + RRF MultiQuery",
            "description": "OpenAI embedding with RRF multiquery",
            "embedding_preset": "openai",
            "retriever_type": "rrf_multiquery",
            "bm25_weight": 0.5,
            "dense_weight": 0.5,
            "use_mmr": True,
            "lambda_mult": 0.5
        }
    elif retriever_type == "openai-large-rrf-multiquery":
        retriever_config = {
            "name": "openai-large-rrf-multiquery",
            "display_name": "OpenAI Large + RRF MultiQuery",
            "description": "OpenAI Large embedding with RRF multiquery",
            "embedding_preset": "openai-large",
            "retriever_type": "rrf_multiquery",
            "bm25_weight": 0.5,
            "dense_weight": 0.5,
            "use_mmr": True,
            "lambda_mult": 0.5
        }
    else:  # ê¸°ë³¸ê°’: bge-m3
        retriever_config = {
            "name": "bge-m3-rrf-ensemble",
            "display_name": "BGE-M3 + RRF Ensemble",
            "description": "BGE-M3 embedding with RRF ensemble",
            "embedding_preset": "bge-m3",
            "retriever_type": "rrf_ensemble",
            "bm25_weight": 0.5,
            "dense_weight": 0.5,
            "use_mmr": True,
            "lambda_mult": 0.5
        }

    # ì¬ìˆœìœ„í™”ë¥¼ ìœ„í•´ ë” ë§ì€ ë¬¸ì„œ ê²€ìƒ‰
    initial_k = max(20, top_k * 3)

    # Retriever ìƒì„± (OpenRouter API ì‚¬ìš©)
    retriever, _ = create_retriever_from_config(retriever_config, initial_k)

    print(f"ğŸ” {retriever_config['display_name']} ê²€ìƒ‰ ì¤‘ (ì´ˆê¸° k={initial_k})...")

    # Reranker ëª¨ë¸ëª… ì„¤ì •
    reranker_model_names = {
        "qwen3": "Qwen3-Reranker-4B",
        "bge": "BGE-Reranker-v2-m3",
        "korean": "Korean-Reranker-8k"
    }
    reranker_model_name = reranker_model_names.get(reranker_type, "Unknown")

    # Langfuseë¡œ ê²€ìƒ‰ ê³¼ì • ê¸°ë¡
    if langfuse and question_id:
        with langfuse.start_as_current_observation(
            as_type='span',
            name=f"retrieval_{retriever_config['name']}_q{question_id}",
            input={"question": question},
            metadata={
                "retriever_name": retriever_config['name'],
                "embedding_preset": retriever_config['embedding_preset'],
                "retrieval_strategy": retriever_config['retriever_type'],
                "initial_k": initial_k,
                "final_k": top_k,
                "reranker": reranker_model_name,
                "reranker_type": reranker_type,
                "batch_size": batch_size
            }
        ) as span:
            # ë¬¸ì„œ ê²€ìƒ‰
            initial_docs = retriever.invoke(question)
            print(f"ğŸ“„ ì´ˆê¸° ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜: {len(initial_docs)}")

            # Reranking
            reranked_docs = rerank_documents(question, initial_docs, top_k=top_k, batch_size=batch_size, reranker_type=reranker_type)

            span.update(output={
                "initial_num_docs": len(initial_docs),
                "final_num_docs": len(reranked_docs),
                "doc_titles": [doc.metadata.get('page_title', 'Unknown') for doc in reranked_docs]
            })

            # ì´ˆê¸° ë¬¸ì„œ ë©”ëª¨ë¦¬ í•´ì œ
            del initial_docs
    else:
        initial_docs = retriever.invoke(question)
        print(f"ğŸ“„ ì´ˆê¸° ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜: {len(initial_docs)}")
        reranked_docs = rerank_documents(question, initial_docs, top_k=top_k, batch_size=batch_size, reranker_type=reranker_type)

        # ì´ˆê¸° ë¬¸ì„œ ë©”ëª¨ë¦¬ í•´ì œ
        del initial_docs

    print(f"\nğŸ“„ ìµœì¢… ë¬¸ì„œ ìˆ˜: {len(reranked_docs)}")
    print("\nìµœì¢… ë¬¸ì„œ ì œëª©:")
    for i, doc in enumerate(reranked_docs, 1):
        print(f"  {i}. {doc.metadata.get('page_title', 'Unknown')}")

    # Retriever ë©”ëª¨ë¦¬ í•´ì œ
    del retriever

    # ë©”ëª¨ë¦¬ ì •ë¦¬
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    return reranked_docs


def test_single_llm(
    llm_config: Dict[str, Any],
    question: str,
    docs: list,
    report_type: str,
    langfuse=None,
    question_id: int = None
) -> Dict[str, Any]:
    """ë‹¨ì¼ LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±"""

    print(f"\n{'=' * 100}")
    print(f"ğŸ’¬ {llm_config['display_name']}")
    print(f"   {llm_config['description']}")
    print(f"{'=' * 100}")

    try:
        print("ğŸ’¬ ë‹µë³€ ìƒì„± ì¤‘...")
        answer = generate_answer_with_llm(
            question,
            docs,
            llm_config,
            report_type,
            langfuse,
            question_id
        )

        print(f"\nâœ… ìƒì„±ëœ ë‹µë³€:\n{answer}\n")

        return {
            "success": True,
            "llm_name": llm_config['name'],
            "llm_display_name": llm_config['display_name'],
            "model_id": llm_config['model_id'],
            "answer": answer
        }

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}\n")
        import traceback
        traceback.print_exc()
        return {
            "success": False,
            "llm_name": llm_config['name'],
            "llm_display_name": llm_config['display_name'],
            "error": str(e)
        }


def save_combination_results(
    all_results: List[Dict],
    top_k: int,
    llm_configs: List[Dict],
    timestamp: str,
    combination_base_dir: Path,
    questions: List[str],
    report_type: str
):
    """ì¤‘ê°„ ê²°ê³¼ë¥¼ ì¡°í•©ë³„ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜"""

    all_combinations_data = []
    all_answers_text = []

    for llm_config in llm_configs:
        llm_name = llm_config['name']

        # Retriever + LLM ì¡°í•© í´ë”ëª… ìƒì„±
        combination_name = f"bge-m3-rrf-reranker-k{top_k}_{llm_name}"
        combination_dir = combination_base_dir / combination_name
        combination_dir.mkdir(parents=True, exist_ok=True)

        # í•´ë‹¹ LLMì˜ ê²°ê³¼ë§Œ í•„í„°ë§
        llm_results = []
        for q_result in all_results:
            for llm_result in q_result['llms']:
                if llm_result['llm_name'] == llm_name:
                    llm_results.append({
                        "question_id": q_result['question_id'],
                        "question": q_result['question'],
                        "date_filter": q_result['date_filter'],
                        "num_docs": q_result['num_docs'],
                        "doc_titles": q_result['doc_titles'],
                        "result": llm_result
                    })

        # Retriever + LLM ì¡°í•©ë³„ JSON ì €ì¥
        combination_output = {
            "combination_name": combination_name,
            "retriever_name": f"bge-m3-rrf-reranker-k{top_k}",
            "retriever_display_name": f"BGE-M3 + RRF + Qwen3-Reranker (Top {top_k})",
            "retriever_description": f"BGE-M3 embedding with RRF ensemble and Qwen3-Reranker-4B reranking (final k={top_k})",
            "llm_name": llm_name,
            "llm_display_name": llm_config['display_name'],
            "llm_description": llm_config['description'],
            "num_questions": len(llm_results),
            "results": llm_results
        }

        all_combinations_data.append(combination_output)

        combination_file = combination_dir / "results.json"
        with open(combination_file, 'w', encoding='utf-8') as f:
            json.dump(combination_output, f, ensure_ascii=False, indent=2)

        # ë‹µë³€ë§Œ ë”°ë¡œ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥
        answer_file = combination_dir / "answers.txt"
        answer_text_parts = []
        answer_text_parts.append(f"=" * 100)
        answer_text_parts.append(f"BGE-M3 + RRF + Qwen3-Reranker (Top {top_k}) + {llm_config['display_name']} - ë‹µë³€ ëª¨ìŒ")
        answer_text_parts.append(f"=" * 100)
        answer_text_parts.append("")

        for i, result in enumerate(llm_results, 1):
            answer_text_parts.append(f"{'=' * 100}")
            answer_text_parts.append(f"ì§ˆë¬¸ {i}/{len(llm_results)}")
            answer_text_parts.append(f"{'=' * 100}")
            answer_text_parts.append(f"â“ {result['question']}")
            answer_text_parts.append("")

            if result['result'].get('success'):
                answer_text_parts.append(f"âœ… ë‹µë³€:")
                answer_text_parts.append(f"{result['result'].get('answer', 'N/A')}")
                answer_text_parts.append("")
            else:
                answer_text_parts.append(f"âŒ ì˜¤ë¥˜ ë°œìƒ:")
                answer_text_parts.append(f"{result['result'].get('error', 'Unknown error')}")
                answer_text_parts.append("")

        answer_text = "\n".join(answer_text_parts)
        with open(answer_file, 'w', encoding='utf-8') as f:
            f.write(answer_text)

        all_answers_text.append(answer_text)
        all_answers_text.append("\n\n")

    # ì „ì²´ ì¡°í•© ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ JSON íŒŒì¼ë¡œ ì €ì¥
    all_combinations_file = combination_base_dir / "all_combinations.json"
    all_combinations_output = {
        "test_date": datetime.now().isoformat(),
        "report_type": report_type,
        "retriever_name": f"bge-m3-rrf-reranker-k{top_k}",
        "retriever_display_name": f"BGE-M3 + RRF + Qwen3-Reranker (Top {top_k})",
        "num_llms": len(llm_configs),
        "num_questions": len(questions),
        "num_completed_questions": len(all_results),
        "total_combinations": len(all_combinations_data),
        "combinations": all_combinations_data
    }

    with open(all_combinations_file, 'w', encoding='utf-8') as f:
        json.dump(all_combinations_output, f, ensure_ascii=False, indent=2)

    # ì „ì²´ ë‹µë³€ì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥
    all_answers_file = combination_base_dir / "all_answers.txt"
    with open(all_answers_file, 'w', encoding='utf-8') as f:
        f.write("\n".join(all_answers_text))

    print(f"âœ… ì¤‘ê°„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {combination_base_dir}")


def evaluate_single_query_with_reranker(
    question: str,
    ground_truth: str,
    context_page_id: str,
    item_metadata: Dict[str, Any],
    report_type: str,
    top_k: int,
    system_prompt: str,
    answer_generation_prompt: str,
    langfuse,
    idx: int,
    version_tag: str = "v1",
    batch_size: int = 16,
    retriever_type: str = "bge-m3",
    reranker_type: str = "qwen3"
) -> Dict[str, Any]:
    """Rerankerë¥¼ ì‚¬ìš©í•œ ë‹¨ì¼ ì¿¼ë¦¬ í‰ê°€ (evaluate_report_types.py ìŠ¤íƒ€ì¼)

    Args:
        question: ì§ˆë¬¸
        ground_truth: ì •ë‹µ
        context_page_id: ì •ë‹µ ë¬¸ì„œì˜ page_id
        item_metadata: ì§ˆë¬¸ ë©”íƒ€ë°ì´í„°
        report_type: ë³´ê³ ì„œ íƒ€ì…
        top_k: Top-K ê°’
        system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        answer_generation_prompt: ë‹µë³€ ìƒì„± í”„ë¡¬í”„íŠ¸
        langfuse: Langfuse í´ë¼ì´ì–¸íŠ¸
        idx: ì§ˆë¬¸ ì¸ë±ìŠ¤
        version_tag: ë²„ì „ íƒœê·¸
        batch_size: Reranker ë°°ì¹˜ í¬ê¸°
        retriever_type: ë¦¬íŠ¸ë¦¬ë²„ íƒ€ì…
        reranker_type: Reranker íƒ€ì… ('qwen3', 'bge', 'korean')

    Returns:
        í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    import time

    start_time = time.time()

    # ë¬¸ì„œ ê²€ìƒ‰ ë° ì¬ìˆœìœ„í™”
    docs = retrieve_and_rerank_documents(
        question=question,
        report_type=report_type,
        top_k=top_k,
        date_filter=None,  # í‰ê°€ ë°ì´í„°ì…‹ì—ëŠ” ë‚ ì§œ í•„í„° ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
        langfuse=langfuse,
        question_id=idx,
        batch_size=batch_size,
        retriever_type=retriever_type,
        reranker_type=reranker_type
    )

    # ì»¨í…ìŠ¤íŠ¸ í…ìŠ¤íŠ¸ ë³€í™˜
    contexts = []
    context_metadata = []

    for result in docs:
        contexts.append(result.page_content)
        context_metadata.append({
            "page_title": result.metadata.get('page_title', 'Unknown'),
            "section_title": result.metadata.get('section_title', 'N/A'),
            "chunk_id": result.metadata.get('chunk_id', 'unknown'),
            "score": result.metadata.get('_combined_score') or result.metadata.get('_similarity_score')
        })

    if not contexts:
        print(f"  âš ï¸ [{idx}] No contexts found for question!")
        contexts = ["ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."]

    # LLM ë‹µë³€ ìƒì„± (common_utilsì˜ generate_llm_answer ì‚¬ìš©)
    from utils.common_utils import generate_llm_answer
    answer = generate_llm_answer(
        question=question,
        contexts=contexts,
        system_prompt=system_prompt,
        answer_generation_prompt=answer_generation_prompt,
        num_contexts=min(5, len(contexts)),
        temperature=0.1,
        max_tokens=1000
    )

    if not answer or answer.startswith("ë‹µë³€ ìƒì„± ì‹¤íŒ¨") or answer.startswith("Azure OpenAI ì„¤ì •"):
        print(f"  âš ï¸ [{idx}] LLM answer generation failed!")
        if not answer:
            answer = "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    total_time = time.time() - start_time

    # Reranker ëª¨ë¸ëª… ì„¤ì •
    reranker_model_names = {
        "qwen3": "Qwen3-Reranker-4B",
        "bge": "BGE-Reranker-v2-m3",
        "korean": "Korean-Reranker-8k"
    }
    reranker_model_name = reranker_model_names.get(reranker_type, "Unknown")
    reranker_tag = f"{reranker_type}-reranker"

    # Langfuse Trace & Generation ìƒì„±
    # retriever_typeì— ë”°ë¼ ë©”íƒ€ë°ì´í„° ë™ì  ìƒì„±
    if retriever_type == "openai-rrf-multiquery":
        base_name = "openai"
        embedding_preset = "openai"
        retrieval_strategy = "rrf_multiquery"
        display_name = f"OpenAI + RRF MultiQuery + {reranker_model_name} (Top {top_k})"
        retriever_tags = ["openai", "rrf_multiquery", reranker_tag, f"top_k_{top_k}", report_type]
    elif retriever_type == "openai-large-rrf-multiquery":
        base_name = "openai-large"
        embedding_preset = "openai-large"
        retrieval_strategy = "rrf_multiquery"
        display_name = f"OpenAI Large + RRF MultiQuery + {reranker_model_name} (Top {top_k})"
        retriever_tags = ["openai-large", "rrf_multiquery", reranker_tag, f"top_k_{top_k}", report_type]
    else:  # bge-m3
        base_name = "bge-m3"
        embedding_preset = "bge-m3"
        retrieval_strategy = "rrf_ensemble"
        display_name = f"BGE-M3 + RRF + {reranker_model_name} (Top {top_k})"
        retriever_tags = ["bge-m3", "rrf_ensemble", reranker_tag, f"top_k_{top_k}", report_type]

    retriever_name = f"{base_name}-rrf-{reranker_type}-reranker-k{top_k}"

    additional_metadata = {
        "context_page_id": context_page_id,
        "retriever_name": retriever_name,
        "display_name": display_name,
        "report_type": report_type,
        "top_k": top_k,
        "embedding_preset": embedding_preset,
        "retriever_type": retrieval_strategy,
        "reranker_model": reranker_model_name,
        "reranker_type": reranker_type
    }

    trace_id = create_trace_and_generation(
        langfuse=langfuse,
        retriever_name=retriever_name,
        question=question,
        contexts=contexts,
        answer=answer,
        ground_truth=ground_truth,
        context_metadata=context_metadata,
        item_metadata=item_metadata,
        total_time=total_time,
        idx=idx,
        version_tag=version_tag,
        retriever_tags=retriever_tags,
        additional_metadata=additional_metadata
    )

    # ê²€ìƒ‰ í’ˆì§ˆ ìŠ¤ì½”ì–´ ì¶”ê°€
    add_retrieval_quality_score(langfuse, trace_id, context_metadata)

    print(f"  [{idx}] {question[:50]}... ({len(contexts)}ê°œ ë¬¸ì„œ, {total_time*1000:.0f}ms)")

    return {
        "question": question,
        "answer": answer,
        "ground_truth": ground_truth,
        "num_contexts": len(contexts),
        "time": total_time,
        "trace_id": trace_id,
        "context_metadata": context_metadata
    }


def run_reranker_evaluation(
    report_type: str,
    dataset_path: str,
    top_k: int = 6,
    version: str = "v1",
    output_dir: str = None,
    batch_size: int = None,
    max_workers: int = 3,
    retriever_type: str = "bge-m3",
    reranker_type: str = "qwen3"
) -> Dict[str, Any]:
    """Rerankerë¥¼ ì‚¬ìš©í•œ í‰ê°€ ì‹¤í–‰ (evaluate_report_types.py ìŠ¤íƒ€ì¼)

    Args:
        report_type: ë³´ê³ ì„œ íƒ€ì… ('executive' ë˜ëŠ” 'weekly')
        dataset_path: í‰ê°€ ë°ì´í„°ì…‹ ê²½ë¡œ
        top_k: Top-K ê°’
        version: ë²„ì „ íƒœê·¸
        output_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        batch_size: Reranker ë°°ì¹˜ í¬ê¸° (Noneì´ë©´ ìë™)
        max_workers: ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ (ê¸°ë³¸: 3)
        retriever_type: ë¦¬íŠ¸ë¦¬ë²„ íƒ€ì… (ê¸°ë³¸: 'bge-m3')
        reranker_type: Reranker íƒ€ì… ('qwen3', 'bge', 'korean')

    Returns:
        í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    import yaml

    # ì„¤ì • ë¡œë“œ
    config = get_report_config(report_type)

    # í”„ë¡¬í”„íŠ¸ ë¡œë“œ (common_utils ì‚¬ìš©)
    from utils.common_utils import load_prompt as load_prompt_util

    report_type_suffix = "weekly_report" if report_type == "weekly" else "executive_report"
    system_prompt = load_prompt_util(f"prompts/templates/service/{report_type_suffix}/system_prompt.txt")
    answer_generation_prompt = load_prompt_util(f"prompts/templates/service/{report_type_suffix}/answer_generation_prompt.txt")

    # í‰ê°€ ë°ì´í„°ì…‹ ë¡œë“œ
    eval_data = load_evaluation_dataset(dataset_path)

    # Langfuse í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    langfuse = get_langfuse_client()
    if not langfuse:
        print("âŒ Langfuse í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return {}

    report_type_display = "ìµœì¢…ë³´ê³ ì„œ" if report_type == "executive" else "ì£¼ê°„ë³´ê³ ì„œ"

    # ë°°ì¹˜ í¬ê¸° ìë™ ì„¤ì •
    if batch_size is None:
        batch_size = get_optimal_batch_size()

    # Reranker ëª¨ë¸ëª… ì„¤ì •
    reranker_model_names = {
        "qwen3": "Qwen3-Reranker",
        "bge": "BGE-Reranker",
        "korean": "Korean-Reranker"
    }
    reranker_display = reranker_model_names.get(reranker_type, "Unknown")

    # Retriever íƒ€ì…ì— ë”°ë¥¸ ë””ìŠ¤í”Œë ˆì´ ì´ë¦„ ìƒì„±
    if retriever_type == "openai-rrf-multiquery":
        retriever_display = f"OpenAI + RRF MultiQuery + {reranker_display}"
    elif retriever_type == "openai-large-rrf-multiquery":
        retriever_display = f"OpenAI Large + RRF MultiQuery + {reranker_display}"
    else:
        retriever_display = f"BGE-M3 + RRF + {reranker_display}"

    print("\n" + "=" * 80)
    print(f"ğŸ” {report_type_display} {retriever_display} í‰ê°€ (Langfuse ì—°ë™)")
    print("=" * 80)
    print(f"ğŸ“‹ ë°ì´í„°ì…‹: {len(eval_data)} ê°œ ìƒ˜í”Œ")
    print(f"ğŸ”§ Retriever: {retriever_type}")
    print(f"ğŸ¤– Reranker: {reranker_type} ({reranker_model_names.get(reranker_type, 'Unknown')})")
    print(f"ğŸ“Š Top-K: {top_k}")
    print(f"ğŸ·ï¸  Version: {version}")
    print(f"ğŸ”¢ Batch Size: {batch_size} (ìë™ ìµœì í™”)")
    print(f"âš¡ ë³‘ë ¬ ì›Œì»¤: {max_workers}")
    print()

    stats = {
        "total_queries": len(eval_data),
        "total_time": 0,
        "evaluations": []
    }

    # ë³‘ë ¬ ì²˜ë¦¬ë¡œ í‰ê°€ ì‹¤í–‰
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_item = {
            executor.submit(
                evaluate_single_query_with_reranker,
                item["question"],
                item["ground_truth"],
                item.get("context_page_id"),
                item.get("metadata", {}),
                report_type,
                top_k,
                system_prompt,
                answer_generation_prompt,
                langfuse,
                idx,
                version,
                batch_size,
                retriever_type,
                reranker_type
            ): (idx, item)
            for idx, item in enumerate(eval_data, 1)
        }

        for future in as_completed(future_to_item):
            idx, item = future_to_item[future]
            try:
                eval_result = future.result()
                stats["evaluations"].append(eval_result)
                stats["total_time"] += eval_result["time"]

                # ìºì‹œ ì €ì¥ (ì£¼ê¸°ì ìœ¼ë¡œ)
                if idx % 5 == 0:
                    save_embedding_cache()

            except Exception as e:
                print(f"  âŒ [{idx}] í‰ê°€ ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()

    stats["avg_time"] = stats["total_time"] / stats["total_queries"] if stats["total_queries"] > 0 else 0
    stats["avg_contexts"] = sum(e["num_contexts"] for e in stats["evaluations"]) / len(stats["evaluations"]) if stats["evaluations"] else 0

    # ê²°ê³¼ ì €ì¥
    if output_dir:
        output_path = Path(output_dir)
    else:
        output_path = Path("data/langfuse/evaluation_results") / f"{report_type}_report"

    output_path.mkdir(parents=True, exist_ok=True)

    # retriever_typeì— ë”°ë¥¸ íŒŒì¼ëª… ë° ì„¤ì • ìƒì„±
    if retriever_type == "openai-rrf-multiquery":
        file_prefix = "openai-rrf-reranker"
        config_name = f"openai-rrf-reranker-k{top_k}"
        config_display = f"OpenAI + RRF MultiQuery + Qwen3-Reranker (Top {top_k})"
        config_embedding = "openai"
        config_retrieval = "rrf_multiquery_reranker"
    elif retriever_type == "openai-large-rrf-multiquery":
        file_prefix = "openai-large-rrf-reranker"
        config_name = f"openai-large-rrf-reranker-k{top_k}"
        config_display = f"OpenAI Large + RRF MultiQuery + Qwen3-Reranker (Top {top_k})"
        config_embedding = "openai-large"
        config_retrieval = "rrf_multiquery_reranker"
    else:
        file_prefix = "bge-m3-rrf-reranker"
        config_name = f"bge-m3-rrf-reranker-k{top_k}"
        config_display = f"BGE-M3 + RRF + Qwen3-Reranker (Top {top_k})"
        config_embedding = "bge-m3"
        config_retrieval = "rrf_ensemble_reranker"

    output_file = output_path / f"{file_prefix}_k{top_k}_stats.json"
    save_result = {k: v for k, v in stats.items() if k != "evaluations"}
    save_result["num_evaluations"] = len(stats.get("evaluations", []))
    save_result["config"] = {
        "retriever_name": config_name,
        "display_name": config_display,
        "report_type": report_type,
        "embedding_preset": config_embedding,
        "retriever_type": config_retrieval,
        "reranker_model": "Qwen3-Reranker-4B",
        "top_k": top_k,
        "version": version,
    }

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(save_result, f, indent=2, ensure_ascii=False, default=str)

    print(f"\nâœ… í‰ê°€ ì™„ë£Œ!")
    print(f"   - í‰ê·  ì‹œê°„: {stats['avg_time']*1000:.2f}ms")
    print(f"   - í‰ê·  ì»¨í…ìŠ¤íŠ¸ ìˆ˜: {stats['avg_contexts']:.2f}")
    print(f"   - ê²°ê³¼ ì €ì¥: {output_file}")

    # Langfuse flush
    print("\nâ³ Langfuseì— ë°ì´í„° ì „ì†¡ ì¤‘...")
    langfuse.flush()

    # ì„ë² ë”© ìºì‹œ ì €ì¥
    print("\nğŸ’¾ ì„ë² ë”© ìºì‹œ ì €ì¥ ì¤‘...")
    save_embedding_cache()

    return stats


def run_reranker_test(
    questions: List[str],
    report_type: str,
    top_k_values: List[int] = [6, 8, 10],
    output_file: str = None,
    global_date_filter: tuple = None,
    retriever_type: str = "bge-m3",
    reranker_type: str = "qwen3"
):
    """Reranker í…ŒìŠ¤íŠ¸ (ì—¬ëŸ¬ k ê°’)

    Args:
        questions: ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
        report_type: ë³´ê³ ì„œ íƒ€ì… ('executive' ë˜ëŠ” 'weekly')
        top_k_values: í…ŒìŠ¤íŠ¸í•  k ê°’ ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸: [6, 8, 10])
        output_file: ê²°ê³¼ ì €ì¥ ê²½ë¡œ
        global_date_filter: ì „ì—­ ë‚ ì§œ í•„í„°
        retriever_type: ë¦¬íŠ¸ë¦¬ë²„ íƒ€ì… (ê¸°ë³¸: 'bge-m3')
        reranker_type: Reranker íƒ€ì… ('qwen3', 'bge', 'korean')
    """

    config = get_report_config(report_type)
    llm_configs = config['llm_configs']
    langfuse = get_langfuse_client()

    report_type_display = "ìµœì¢…ë³´ê³ ì„œìš©" if report_type == "executive" else "ì£¼ê°„ë³´ê³ ì„œìš©"

    print("\n" + "=" * 100)
    print(f"ğŸ§ª {report_type_display} BGE-M3 + RRF + Qwen3-Reranker í…ŒìŠ¤íŠ¸ (Langfuse ì—°ë™)")
    print("=" * 100)
    print(f"\nğŸ” ë¦¬íŠ¸ë¦¬ë²„: BGE-M3 + RRF Ensemble + Qwen3-Reranker-4B")
    print(f"ğŸ“Š í…ŒìŠ¤íŠ¸í•  k ê°’: {top_k_values}")
    if global_date_filter:
        print(f"ğŸ“… ì „ì—­ ë‚ ì§œ í•„í„°: {global_date_filter[0][:10]} ~ {global_date_filter[1][:10]}")
    print("\nğŸ’¬ í‰ê°€ ëŒ€ìƒ LLM:")
    for i, llm_config in enumerate(llm_configs, 1):
        print(f"  {i}. {llm_config['display_name']} - {llm_config['description']}")
    print()

    # ê° k ê°’ì— ëŒ€í•´ í…ŒìŠ¤íŠ¸ ìˆ˜í–‰
    for top_k in top_k_values:
        print(f"\n{'#' * 100}")
        print(f"ğŸ¯ Top-{top_k} í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print(f"{'#' * 100}\n")

        all_results = []
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        combination_base_dir = Path("data/results/reranker_combinations") / f"k{top_k}_{timestamp}"
        combination_base_dir.mkdir(parents=True, exist_ok=True)

        for q_idx, question in enumerate(questions, 1):
            print(f"\n{'=' * 100}")
            print(f"ğŸ“‹ ì§ˆë¬¸ {q_idx}/{len(questions)} (k={top_k})")
            print(f"{'=' * 100}")
            print(f"â“ {question}\n")

            # ë‚ ì§œ í•„í„° ê²°ì •
            if global_date_filter:
                date_filter = global_date_filter
            else:
                date_filter = extract_date_filter_from_question(question)
                if date_filter:
                    print(f"ğŸ“… ê°ì§€ëœ ë‚ ì§œ í•„í„°: {date_filter[0][:10]} ~ {date_filter[1][:10]}\n")

            # ë¬¸ì„œ ê²€ìƒ‰ ë° ì¬ìˆœìœ„í™”
            docs = retrieve_and_rerank_documents(
                question=question,
                report_type=report_type,
                top_k=top_k,
                date_filter=date_filter,
                langfuse=langfuse,
                question_id=q_idx,
                retriever_type=retriever_type,
                reranker_type=reranker_type
            )

            question_result = {
                "question_id": q_idx,
                "question": question,
                "date_filter": f"{date_filter[0][:10]} ~ {date_filter[1][:10]}" if date_filter else None,
                "retriever": f"bge-m3-rrf-reranker-k{top_k}",
                "num_docs": len(docs),
                "doc_titles": [doc.metadata.get('page_title', 'Unknown') for doc in docs],
                "llms": []
            }

            # ê° LLMìœ¼ë¡œ ë³‘ë ¬ í…ŒìŠ¤íŠ¸
            print(f"\nğŸš€ {len(llm_configs)}ê°œ LLMìœ¼ë¡œ ë³‘ë ¬ ë‹µë³€ ìƒì„± ì‹œì‘...\n")

            with ThreadPoolExecutor(max_workers=7) as executor:
                future_to_llm = {
                    executor.submit(test_single_llm, llm_config, question, docs, report_type, langfuse, q_idx): (rank, llm_config)
                    for rank, llm_config in enumerate(llm_configs, 1)
                }

                for future in as_completed(future_to_llm):
                    rank, llm_config = future_to_llm[future]
                    try:
                        result = future.result()
                        result["rank"] = rank
                        question_result["llms"].append(result)

                        # ì¤‘ê°„ ì €ì¥
                        temp_results = all_results + [question_result]
                        try:
                            save_combination_results(temp_results, top_k, llm_configs, timestamp, combination_base_dir, questions, report_type)
                            print(f"ğŸ’¾ {llm_config['display_name']} ê²°ê³¼ ì €ì¥ ì™„ë£Œ")
                        except Exception as e:
                            print(f"âš ï¸ ì €ì¥ ì‹¤íŒ¨: {e}")

                    except Exception as e:
                        print(f"âŒ {llm_config['display_name']} ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
                        result = {
                            "success": False,
                            "llm_name": llm_config['name'],
                            "llm_display_name": llm_config['display_name'],
                            "error": str(e),
                            "rank": rank
                        }
                        question_result["llms"].append(result)

            question_result["llms"].sort(key=lambda x: x["rank"])
            all_results.append(question_result)

            # ì§ˆë¬¸ ì™„ë£Œ ì‹œ ìµœì¢… ì €ì¥
            print(f"\nğŸ’¾ ì§ˆë¬¸ {q_idx} ì™„ë£Œ - ìµœì¢… ê²°ê³¼ ì €ì¥ ì¤‘...")
            save_combination_results(all_results, top_k, llm_configs, timestamp, combination_base_dir, questions, report_type)

        # k ê°’ë³„ summary ì €ì¥
        base_output_dir = Path("data/results/reranker_summary") / f"k{top_k}_{timestamp}"

        if output_file:
            output_path = Path(output_file)
        else:
            output_path = base_output_dir / "summary.json"

        output_path.parent.mkdir(parents=True, exist_ok=True)

        final_result = {
            "test_date": datetime.now().isoformat(),
            "report_type": report_type,
            "top_k": top_k,
            "num_questions": len(questions),
            "retriever_config": {
                "name": f"bge-m3-rrf-reranker-k{top_k}",
                "display_name": f"BGE-M3 + RRF + Qwen3-Reranker (Top {top_k})",
                "description": f"BGE-M3 embedding with RRF ensemble and Qwen3-Reranker-4B reranking (final k={top_k})",
                "type": "rrf_ensemble_reranker",
                "embedding": "baai-bge-m3",
                "reranker": "Qwen3-Reranker-4B",
                "top_k": top_k
            },
            "llm_configs": llm_configs,
            "results": all_results
        }

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(final_result, f, ensure_ascii=False, indent=2)

        print(f"\n{'=' * 100}")
        print(f"âœ… Top-{top_k} í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print(f"ğŸ“ Summary ì €ì¥: {output_path}")
        print(f"ğŸ“ ì¡°í•©ë³„ ê²°ê³¼: {combination_base_dir}")
        print(f"{'=' * 100}\n")

    print(f"\nğŸ”— Langfuseì—ì„œ ìƒì„¸ ì¶”ì  ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”")
    langfuse.flush()


def main():
    import argparse

    parser = argparse.ArgumentParser(
        description="BGE-M3 + RRF + Qwen3-Reranker ë³´ê³ ì„œ ìƒì„± ë° ë¹„êµ (Langfuse ì—°ë™)"
    )
    parser.add_argument(
        "--report-type",
        type=str,
        choices=["executive", "weekly"],
        default="executive",
        help="ë³´ê³ ì„œ íƒ€ì…: executive(ìµœì¢…ë³´ê³ ì„œ), weekly(ì£¼ê°„ë³´ê³ ì„œ) (ê¸°ë³¸: executive)"
    )
    parser.add_argument(
        "--mode",
        type=str,
        choices=["single", "multi", "interactive", "evaluate"],
        default="interactive",
        help="í…ŒìŠ¤íŠ¸ ëª¨ë“œ: single(ë‹¨ì¼ ì§ˆë¬¸), multi(íŒŒì¼ì—ì„œ ì—¬ëŸ¬ ì§ˆë¬¸), interactive(ëŒ€í™”í˜• ì…ë ¥), evaluate(í‰ê°€ ë°ì´í„°ì…‹ ì‚¬ìš©)"
    )
    parser.add_argument(
        "--num-questions",
        type=int,
        default=5,
        help="multi ëª¨ë“œì—ì„œ í…ŒìŠ¤íŠ¸í•  ì§ˆë¬¸ ê°œìˆ˜ (ê¸°ë³¸: 5)"
    )
    parser.add_argument(
        "--question",
        type=str,
        default=None,
        help="single ëª¨ë“œì—ì„œ ì‚¬ìš©í•  ì§ˆë¬¸"
    )
    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="ê²°ê³¼ ì €ì¥ ê²½ë¡œ (ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ì €ì¥í•˜ì§€ ì•ŠìŒ)"
    )
    parser.add_argument(
        "--start-date",
        type=str,
        default=None,
        help="ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD í˜•ì‹)"
    )
    parser.add_argument(
        "--end-date",
        type=str,
        default=None,
        help="ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD í˜•ì‹)"
    )
    parser.add_argument(
        "--date-range",
        type=str,
        default=None,
        help="ìì—°ì–´ ë‚ ì§œ ë²”ìœ„ (ì˜ˆ: 'ì´ë²ˆ ì£¼', 'ì§€ë‚œì£¼', '12ì›” 2ì£¼ì°¨')"
    )
    parser.add_argument(
        "--top-k",
        type=str,
        default="6,8,10",
        help="í…ŒìŠ¤íŠ¸í•  k ê°’ (ì½¤ë§ˆë¡œ êµ¬ë¶„, ê¸°ë³¸: 6,8,10). evaluate ëª¨ë“œì—ì„œëŠ” ë‹¨ì¼ ê°’ë§Œ ì‚¬ìš©"
    )
    parser.add_argument(
        "--dataset",
        type=str,
        default="/home/work/rag/Project/rag-report-generator/data/evaluation/merged_qa_dataset.json",
        help="evaluate ëª¨ë“œì—ì„œ ì‚¬ìš©í•  í‰ê°€ ë°ì´í„°ì…‹ ê²½ë¡œ (ê¸°ë³¸: merged_qa_dataset.json)"
    )
    parser.add_argument(
        "--version",
        type=str,
        default="v1",
        help="í‰ê°€ ë²„ì „ íƒœê·¸ (ê¸°ë³¸: v1)"
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=None,
        help="Reranker ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸: None=ìë™, ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ì¤„ì´ê¸°)"
    )
    parser.add_argument(
        "--max-workers",
        type=int,
        default=3,
        help="ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜ (ê¸°ë³¸: 3, ë©”ëª¨ë¦¬ ë§ìœ¼ë©´ ëŠ˜ë¦¬ê¸°)"
    )
    parser.add_argument(
        "--retriever-type",
        type=str,
        default="bge-m3",
        choices=["bge-m3", "openai-rrf-multiquery", "openai-large-rrf-multiquery"],
        help="ë¦¬íŠ¸ë¦¬ë²„ íƒ€ì… (ê¸°ë³¸: bge-m3, ì„ íƒ: openai-rrf-multiquery, openai-large-rrf-multiquery)"
    )
    parser.add_argument(
        "--reranker-type",
        type=str,
        default="qwen3",
        choices=["qwen3", "bge", "korean"],
        help="Reranker íƒ€ì… (ê¸°ë³¸: qwen3, ì„ íƒ: bge=BGE-Reranker-v2-m3, korean=Korean-Reranker-8k)"
    )

    args = parser.parse_args()

    # k ê°’ íŒŒì‹±
    top_k_values = [int(k.strip()) for k in args.top_k.split(',')]

    # ë‚ ì§œ í•„í„° íŒŒì‹±
    date_filter = parse_date_range(
        date_input=args.date_range,
        start_date=args.start_date,
        end_date=args.end_date
    )

    # ë³´ê³ ì„œ íƒ€ì…ì— ë§ëŠ” ì„¤ì • ë¡œë“œ
    config = get_report_config(args.report_type)

    if args.mode == "evaluate":
        # í‰ê°€ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•œ í‰ê°€ ëª¨ë“œ
        # evaluate ëª¨ë“œì—ì„œëŠ” top_k_valuesì˜ ì²« ë²ˆì§¸ ê°’ë§Œ ì‚¬ìš©
        top_k = top_k_values[0]

        print("\n" + "=" * 80)
        print(f"ğŸ“Š í‰ê°€ ëª¨ë“œ: í‰ê°€ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•œ ì„±ëŠ¥ í‰ê°€")
        print("=" * 80)
        print(f"ë³´ê³ ì„œ íƒ€ì…: {args.report_type}")
        print(f"ë°ì´í„°ì…‹: {args.dataset}")
        print(f"Top-K: {top_k}")
        print(f"ë²„ì „: {args.version}")
        print()

        run_reranker_evaluation(
            report_type=args.report_type,
            dataset_path=args.dataset,
            top_k=top_k,
            version=args.version,
            output_dir=args.output,
            batch_size=args.batch_size,
            max_workers=args.max_workers,
            retriever_type=args.retriever_type,
            reranker_type=args.reranker_type
        )

    elif args.mode == "interactive":
        print("\n" + "=" * 100)
        print(f"ğŸ¯ ëŒ€í™”í˜• ëª¨ë“œ: ì§ˆë¬¸ì„ ì§ì ‘ ì…ë ¥í•˜ì„¸ìš” (ë³´ê³ ì„œ íƒ€ì…: {args.report_type})")
        print("=" * 100)
        print("ğŸ’¡ íŒ:")
        print("   - ì—¬ëŸ¬ ì§ˆë¬¸ì„ ì…ë ¥í•˜ë ¤ë©´ ì„¸ë¯¸ì½œë¡ (;)ìœ¼ë¡œ êµ¬ë¶„í•˜ì„¸ìš”")
        print("   - ì…ë ¥ ì—†ì´ ì—”í„°ë¥¼ ëˆ„ë¥´ë©´ ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ 5ê°œê°€ ì‚¬ìš©ë©ë‹ˆë‹¤")
        print()

        user_input = input("ğŸ“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì—”í„°: ê¸°ë³¸ ì§ˆë¬¸ ì‚¬ìš©): ").strip()

        if not user_input:
            print("\nâœ… ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤:")
            questions = config['test_questions']
            for i, q in enumerate(questions, 1):
                print(f"  {i}. {q}")
        else:
            questions = [q.strip() for q in user_input.split(';') if q.strip()]
            print(f"\nâœ… ì´ {len(questions)}ê°œì˜ ì§ˆë¬¸ì´ ì…ë ¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            for i, q in enumerate(questions, 1):
                print(f"  {i}. {q}")

        run_reranker_test(questions, args.report_type, top_k_values, args.output, date_filter, args.retriever_type, args.reranker_type)

    elif args.mode == "single":
        if not args.question:
            print("âŒ --question ì¸ìê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return
        run_reranker_test([args.question], args.report_type, top_k_values, args.output, date_filter, args.retriever_type, args.reranker_type)

    else:
        # ì—¬ëŸ¬ ì§ˆë¬¸ ëª¨ë“œ (íŒŒì¼ì—ì„œ ë¡œë“œ)
        test_questions_data = load_test_questions(n=args.num_questions)
        questions = [q["question"] for q in test_questions_data]

        run_reranker_test(questions, args.report_type, top_k_values, args.output, date_filter, args.retriever_type, args.reranker_type)


if __name__ == "__main__":
    main()
