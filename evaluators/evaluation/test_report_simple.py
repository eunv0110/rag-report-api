#!/usr/bin/env python3
"""ë³´ê³ ì„œìš© Top 3 ë¦¬íŠ¸ë¦¬ë²„ ë³´ê³ ì„œ ìë™ ìƒì„± ë° ë¹„êµ (Langfuse ì—°ë™)

ì„¸ ê°€ì§€ ìµœê³  ì„±ëŠ¥ ë¦¬íŠ¸ë¦¬ë²„ë¡œ ì‹¤ì œ ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ê³  ê²°ê³¼ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.
Langfuseë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  ì‹¤í–‰ì„ ì¶”ì í•˜ê³  ë¹„êµí•©ë‹ˆë‹¤.

ë³´ê³ ì„œ íƒ€ì…:
- executive: ìµœì¢…ë³´ê³ ì„œ (Gemini, BGE-M3, OpenAI)
- weekly: ì£¼ê°„ë³´ê³ ì„œ (BGE-M3, Upstage, OpenAI)
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

from dotenv import load_dotenv
load_dotenv()

import json
import os
from datetime import datetime
from typing import List, Dict, Any
from langchain.chat_models import init_chat_model
from concurrent.futures import ThreadPoolExecutor, as_completed

from config.settings import AZURE_AI_CREDENTIAL, AZURE_AI_ENDPOINT, EVALUATION_CONFIG
from utils.langfuse import get_langfuse_client
from utils.dates import parse_date_range, extract_date_filter_from_question
from utils.common import load_prompt

# ë¦¬íŠ¸ë¦¬ë²„ ì„í¬íŠ¸
from retrievers.ensemble_retriever import get_ensemble_retriever


def get_report_config(report_type: str) -> Dict[str, Any]:
    """ë³´ê³ ì„œ íƒ€ì…ì— ë”°ë¥¸ ì„¤ì • ë°˜í™˜"""
    if report_type not in ['executive', 'weekly']:
        raise ValueError(f"Invalid report_type: {report_type}. Must be 'executive' or 'weekly'")

    return {
        'test_questions': EVALUATION_CONFIG['test_questions'][f'{report_type}_report'],
        'retriever_configs': EVALUATION_CONFIG['simple_test_retrievers'][f'{report_type}_report'],
        'default_retriever_index': EVALUATION_CONFIG['default_retriever_index'][f'{report_type}_report'],
        'llm_configs': EVALUATION_CONFIG['test_llms'],
        'prompt_dir': f'{report_type}_report',
        'qdrant_locks': {
            'executive': [
                "/home/work/rag/Project/rag-report-generator/data/qdrant_data/gemini-embedding-001/.lock",
                "/home/work/rag/Project/rag-report-generator/data/qdrant_data/baai-bge-m3/.lock",
                "/home/work/rag/Project/rag-report-generator/data/qdrant_data/openai-large/.lock"
            ],
            'weekly': [
                "/home/work/rag/Project/rag-report-generator/data/qdrant_data/baai-bge-m3/.lock",
                "/home/work/rag/Project/rag-report-generator/data/qdrant_data/upstage-solar-embedding/.lock",
                "/home/work/rag/Project/rag-report-generator/data/qdrant_data/openai-large/.lock"
            ]
        }[report_type]
    }


def _load_evaluation_prompt(prompt_file: str, report_type: str) -> str:
    """í‰ê°€ìš© í”„ë¡¬í”„íŠ¸ íŒŒì¼ ë¡œë“œ (í—¬í¼ í•¨ìˆ˜)"""
    prompt_path = f"prompts/templates/evaluation/{report_type}_report/{prompt_file}"
    return load_prompt(prompt_path)


def load_test_questions(n: int = 5) -> List[Dict[str, Any]]:
    """í…ŒìŠ¤íŠ¸ìš© ì§ˆë¬¸ ë¡œë“œ"""
    qa_file = Path(__file__).parent.parent / "data" / "evaluation" / "llm_generated_qa_v2.json"

    with open(qa_file, 'r', encoding='utf-8') as f:
        qa_data = json.load(f)

    return qa_data[:n]


def generate_answer_with_llm(query: str, docs: list, llm_config: dict, report_type: str, langfuse=None, question_id: int = None) -> str:
    """ì§€ì •ëœ LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±"""
    from openai import OpenAI
    from config.settings import OPENROUTER_API_KEY, OPENROUTER_BASE_URL

    # LLMë³„ ì»¨í…ìŠ¤íŠ¸ ì œí•œ ì²˜ë¦¬
    if 'max_docs' in llm_config and llm_config['max_docs']:
        max_docs = llm_config['max_docs']
        if len(docs) > max_docs:
            docs = docs[:max_docs]
            print(f"  âš ï¸ {llm_config['display_name']} ì»¨í…ìŠ¤íŠ¸ ì œí•œìœ¼ë¡œ ë¬¸ì„œ ìˆ˜ë¥¼ {len(docs)}ê°œë¡œ ì¤„ì˜€ìŠµë‹ˆë‹¤.")

    # Context êµ¬ì„±
    context_parts = []
    for i, doc in enumerate(docs, 1):
        title = doc.metadata.get('page_title', 'Unknown')
        content = doc.page_content
        context_parts.append(f"[ë¬¸ì„œ {i}] {title}\n{content}\n")

    context_text = "\n".join(context_parts)

    # í”„ë¡¬í”„íŠ¸ ë¡œë“œ
    system_prompt = _load_evaluation_prompt("system_prompt.txt", report_type)
    answer_generation_template = _load_evaluation_prompt("answer_generation_prompt.txt", report_type)

    # í…œí”Œë¦¿ì— ë³€ìˆ˜ ëŒ€ì…
    user_prompt = answer_generation_template.replace("{context}", context_text).replace("{question}", query)

    # Azure AI ì„¤ì •
    os.environ['AZURE_AI_CREDENTIAL'] = AZURE_AI_CREDENTIAL
    os.environ['AZURE_AI_ENDPOINT'] = AZURE_AI_ENDPOINT

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]

    # LLM íƒ€ì…ì— ë”°ë¼ ë‹¤ë¥´ê²Œ í˜¸ì¶œ
    if llm_config['model_id'].startswith('anthropic:'):
        # Anthropic Claude ëª¨ë¸ - OpenRouter ì‚¬ìš©
        model_name = llm_config['model_id'].split(':')[1]
        client = OpenAI(
            api_key=OPENROUTER_API_KEY,
            base_url=OPENROUTER_BASE_URL
        )

        # Langfuseë¡œ ë‹µë³€ ìƒì„± ê¸°ë¡
        if langfuse and question_id:
            with langfuse.start_as_current_observation(
                as_type='generation',
                name=f"generation_{llm_config['name']}_q{question_id}",
                model=llm_config['model_id'],
                input={"question": query, "context": context_text[:500] + "..." if len(context_text) > 500 else context_text},
                metadata={"llm": llm_config['name'], "num_docs": len(docs)}
            ) as generation:
                response = client.chat.completions.create(
                    model=f"anthropic/{model_name}",
                    messages=messages,
                    max_tokens=1000,
                    temperature=0
                )
                answer = response.choices[0].message.content

                # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œ (OpenAI API í˜•ì‹)
                usage_dict = None
                if hasattr(response, 'usage') and response.usage:
                    usage_dict = {
                        "input": response.usage.prompt_tokens,
                        "output": response.usage.completion_tokens,
                        "total": response.usage.total_tokens
                    }

                generation.update(
                    output={"answer": answer},
                    usage=usage_dict if usage_dict else None
                )

                # Trace ì—…ë°ì´íŠ¸ (LLMê³¼ retriever ì •ë³´ í¬í•¨)
                langfuse.update_current_trace(
                    name=f"llm_comparison_{llm_config['name']}_q{question_id}",
                    tags=[llm_config['name'], "llm_comparison", f"{report_type}_report"],
                    input={"question": query},
                    output={"answer": answer},
                    metadata={
                        "llm_name": llm_config['name'],
                        "llm_display_name": llm_config['display_name'],
                        "llm_description": llm_config['description'],
                        "model_id": llm_config['model_id'],
                        "question_id": question_id,
                        "num_docs": len(docs),
                        "report_type": report_type
                    }
                )
        else:
            response = client.chat.completions.create(
                model=f"anthropic/{model_name}",
                messages=messages,
                max_tokens=1000,
                temperature=0
            )
            answer = response.choices[0].message.content
    else:
        # Azure AI ëª¨ë¸ (LangChain)
        model = init_chat_model(
            llm_config['model_id'],
            temperature=0,
            max_completion_tokens=1000
        )

        # Langfuseë¡œ ë‹µë³€ ìƒì„± ê¸°ë¡
        if langfuse and question_id:
            with langfuse.start_as_current_observation(
                as_type='generation',
                name=f"generation_{llm_config['name']}_q{question_id}",
                model=llm_config['model_id'],
                input={"question": query, "context": context_text[:500] + "..." if len(context_text) > 500 else context_text},
                metadata={"llm": llm_config['name'], "num_docs": len(docs)}
            ) as generation:
                response = model.invoke(messages)
                answer = response.content

                # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œ (LangChain response)
                usage_dict = None
                if hasattr(response, 'usage_metadata') and response.usage_metadata:
                    usage_dict = {
                        "input": response.usage_metadata.get('input_tokens', 0),
                        "output": response.usage_metadata.get('output_tokens', 0),
                        "total": response.usage_metadata.get('total_tokens', 0)
                    }
                elif hasattr(response, 'response_metadata') and response.response_metadata:
                    # ì¼ë¶€ LangChain ëª¨ë¸ì€ response_metadataì— token_usage í¬í•¨
                    token_usage = response.response_metadata.get('token_usage', {})
                    if token_usage:
                        usage_dict = {
                            "input": token_usage.get('prompt_tokens', 0),
                            "output": token_usage.get('completion_tokens', 0),
                            "total": token_usage.get('total_tokens', 0)
                        }

                generation.update(
                    output={"answer": answer},
                    usage=usage_dict if usage_dict else None
                )

                # Trace ì—…ë°ì´íŠ¸ (LLMê³¼ retriever ì •ë³´ í¬í•¨)
                langfuse.update_current_trace(
                    name=f"llm_comparison_{llm_config['name']}_q{question_id}",
                    tags=[llm_config['name'], "llm_comparison", f"{report_type}_report"],
                    input={"question": query},
                    output={"answer": answer},
                    metadata={
                        "llm_name": llm_config['name'],
                        "llm_display_name": llm_config['display_name'],
                        "llm_description": llm_config['description'],
                        "model_id": llm_config['model_id'],
                        "question_id": question_id,
                        "num_docs": len(docs),
                        "report_type": report_type
                    }
                )
        else:
            response = model.invoke(messages)
            answer = response.content

    return answer


def retrieve_documents(question: str, retriever_config: dict, qdrant_locks: List[str], date_filter: tuple = None, langfuse=None, question_id: int = None):
    """ë¦¬íŠ¸ë¦¬ë²„ë¡œ ë¬¸ì„œ ê²€ìƒ‰ (1ë²ˆë§Œ ìˆ˜í–‰)"""
    import time
    import gc

    # Qdrant ë½ íŒŒì¼ ì •ë¦¬ (ë§¤ë²ˆ ì‹¤í–‰ ì „)
    for lock_file in qdrant_locks:
        if os.path.exists(lock_file):
            try:
                os.remove(lock_file)
            except:
                pass

    # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
    gc.collect()
    time.sleep(0.5)

    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    os.environ["MODEL_PRESET"] = retriever_config['embedding']

    # ì„ë² ë”© ìºì‹œ í™œì„±í™”
    os.environ["USE_EMBEDDING_CACHE"] = "true"

    # ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±
    if retriever_config['type'] == 'rrf_ensemble':
        retriever = get_ensemble_retriever(
            k=retriever_config['top_k'],
            bm25_weight=0.5,
            dense_weight=0.5,
            date_filter=date_filter
        )
    elif retriever_config['type'] == 'rrf_multiquery_lc':
        # MultiQuery + RRF Ensemble
        from retrievers.multiquery_retriever import get_multiquery_retriever

        # ê¸°ë³¸ RRF Ensemble ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±
        base_retriever = get_ensemble_retriever(
            k=retriever_config['top_k'],
            bm25_weight=0.5,
            dense_weight=0.5,
            date_filter=date_filter
        )

        # MultiQueryë¡œ ë˜í•‘
        retriever = get_multiquery_retriever(
            base_retriever=base_retriever,
            num_queries=3,
            temperature=0.7
        )
    else:
        raise ValueError(f"Unknown retriever type: {retriever_config['type']}")

    # ë¬¸ì„œ ê²€ìƒ‰
    print("ğŸ” ê²€ìƒ‰ ì¤‘...")

    # Langfuseë¡œ ê²€ìƒ‰ ê³¼ì • ê¸°ë¡
    if langfuse and question_id:
        with langfuse.start_as_current_observation(
            as_type='span',
            name=f"retrieval_{retriever_config['name']}_q{question_id}",
            input={"question": question},
            metadata={
                "retriever_name": retriever_config['name'],
                "retriever_display_name": retriever_config['display_name'],
                "retriever_type": retriever_config['type'],
                "embedding": retriever_config['embedding'],
                "top_k": retriever_config['top_k'],
                "description": retriever_config['description']
            }
        ) as span:
            docs = retriever.invoke(question)
            span.update(output={
                "num_docs": len(docs),
                "doc_titles": [doc.metadata.get('page_title', 'Unknown') for doc in docs]
            })
    else:
        docs = retriever.invoke(question)

    print(f"\nğŸ“„ ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(docs)}")
    print("\nê²€ìƒ‰ëœ ë¬¸ì„œ ì œëª©:")
    for i, doc in enumerate(docs, 1):
        print(f"  {i}. {doc.metadata.get('page_title', 'Unknown')}")

    return docs


def test_single_llm(
    llm_config: Dict[str, Any],
    question: str,
    docs: list,
    report_type: str,
    langfuse=None,
    question_id: int = None
) -> Dict[str, Any]:
    """ë‹¨ì¼ LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±"""

    print(f"\n{'=' * 100}")
    print(f"ğŸ’¬ {llm_config['display_name']}")
    print(f"   {llm_config['description']}")
    print(f"{'=' * 100}")

    try:
        # ë‹µë³€ ìƒì„±
        print("ğŸ’¬ ë‹µë³€ ìƒì„± ì¤‘...")
        answer = generate_answer_with_llm(
            question,
            docs,
            llm_config,
            report_type,
            langfuse,
            question_id
        )

        print(f"\nâœ… ìƒì„±ëœ ë‹µë³€:\n{answer}\n")

        return {
            "success": True,
            "llm_name": llm_config['name'],
            "llm_display_name": llm_config['display_name'],
            "model_id": llm_config['model_id'],
            "answer": answer
        }

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}\n")
        import traceback
        traceback.print_exc()
        return {
            "success": False,
            "llm_name": llm_config['name'],
            "llm_display_name": llm_config['display_name'],
            "error": str(e)
        }


def save_combination_results(all_results: List[Dict], retriever_config: Dict, llm_configs: List[Dict], timestamp: str, combination_base_dir: Path, questions: List[str]):
    """ì¤‘ê°„ ê²°ê³¼ë¥¼ ì¡°í•©ë³„ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜"""

    # ì „ì²´ ì¡°í•© ê²°ê³¼ë¥¼ ëª¨ì„ ë¦¬ìŠ¤íŠ¸
    all_combinations_data = []
    all_answers_text = []

    for llm_config in llm_configs:
        llm_name = llm_config['name']

        # Retriever + LLM ì¡°í•© í´ë”ëª… ìƒì„±
        combination_name = f"{retriever_config['name']}_{llm_name}"
        combination_dir = combination_base_dir / combination_name
        combination_dir.mkdir(parents=True, exist_ok=True)

        # í•´ë‹¹ LLMì˜ ê²°ê³¼ë§Œ í•„í„°ë§
        llm_results = []
        for q_result in all_results:
            for llm_result in q_result['llms']:
                if llm_result['llm_name'] == llm_name:
                    llm_results.append({
                        "question_id": q_result['question_id'],
                        "question": q_result['question'],
                        "date_filter": q_result['date_filter'],
                        "num_docs": q_result['num_docs'],
                        "doc_titles": q_result['doc_titles'],
                        "result": llm_result
                    })

        # Retriever + LLM ì¡°í•©ë³„ JSON ì €ì¥
        combination_output = {
            "combination_name": combination_name,
            "retriever_name": retriever_config['name'],
            "retriever_display_name": retriever_config['display_name'],
            "retriever_description": retriever_config['description'],
            "llm_name": llm_name,
            "llm_display_name": llm_config['display_name'],
            "llm_description": llm_config['description'],
            "num_questions": len(llm_results),
            "results": llm_results
        }

        # ì „ì²´ ì¡°í•© ë°ì´í„°ì— ì¶”ê°€
        all_combinations_data.append(combination_output)

        combination_file = combination_dir / "results.json"
        with open(combination_file, 'w', encoding='utf-8') as f:
            json.dump(combination_output, f, ensure_ascii=False, indent=2)

        # ë‹µë³€ë§Œ ë”°ë¡œ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥
        answer_file = combination_dir / "answers.txt"
        answer_text_parts = []
        answer_text_parts.append(f"=" * 100)
        answer_text_parts.append(f"{retriever_config['display_name']} + {llm_config['display_name']} - ë‹µë³€ ëª¨ìŒ")
        answer_text_parts.append(f"=" * 100)
        answer_text_parts.append("")

        for i, result in enumerate(llm_results, 1):
            answer_text_parts.append(f"{'=' * 100}")
            answer_text_parts.append(f"ì§ˆë¬¸ {i}/{len(llm_results)}")
            answer_text_parts.append(f"{'=' * 100}")
            answer_text_parts.append(f"â“ {result['question']}")
            answer_text_parts.append("")

            if result['result'].get('success'):
                answer_text_parts.append(f"âœ… ë‹µë³€:")
                answer_text_parts.append(f"{result['result'].get('answer', 'N/A')}")
                answer_text_parts.append("")
            else:
                answer_text_parts.append(f"âŒ ì˜¤ë¥˜ ë°œìƒ:")
                answer_text_parts.append(f"{result['result'].get('error', 'Unknown error')}")
                answer_text_parts.append("")

        answer_text = "\n".join(answer_text_parts)
        with open(answer_file, 'w', encoding='utf-8') as f:
            f.write(answer_text)

        # ì „ì²´ ë‹µë³€ í…ìŠ¤íŠ¸ì— ì¶”ê°€
        all_answers_text.append(answer_text)
        all_answers_text.append("\n\n")  # ì¡°í•© ê°„ êµ¬ë¶„

    # ì „ì²´ ì¡°í•© ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ JSON íŒŒì¼ë¡œ ì €ì¥
    all_combinations_file = combination_base_dir / "all_combinations.json"
    all_combinations_output = {
        "test_date": datetime.now().isoformat(),
        "retriever_name": retriever_config['name'],
        "retriever_display_name": retriever_config['display_name'],
        "num_llms": len(llm_configs),
        "num_questions": len(questions),
        "num_completed_questions": len(all_results),
        "total_combinations": len(all_combinations_data),
        "combinations": all_combinations_data
    }

    with open(all_combinations_file, 'w', encoding='utf-8') as f:
        json.dump(all_combinations_output, f, ensure_ascii=False, indent=2)

    # ì „ì²´ ë‹µë³€ì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥
    all_answers_file = combination_base_dir / "all_answers.txt"
    with open(all_answers_file, 'w', encoding='utf-8') as f:
        f.write("\n".join(all_answers_text))

    print(f"âœ… ì¤‘ê°„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {combination_base_dir}")


def run_comparison_test(
    questions: List[str],
    report_type: str,
    output_file: str = None,
    global_date_filter: tuple = None,
    retriever_index: int = None
):
    """ì—¬ëŸ¬ ì§ˆë¬¸ì— ëŒ€í•´ ì—¬ëŸ¬ LLM ë¹„êµ í…ŒìŠ¤íŠ¸ (1ê°œ ë¦¬íŠ¸ë¦¬ë²„ ì‚¬ìš©)

    Args:
        questions: ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
        report_type: ë³´ê³ ì„œ íƒ€ì… ('executive' ë˜ëŠ” 'weekly')
        output_file: ê²°ê³¼ ì €ì¥ ê²½ë¡œ
        global_date_filter: ì „ì—­ ë‚ ì§œ í•„í„° (ëª…ì‹œì ìœ¼ë¡œ ì§€ì •ëœ ê²½ìš°, ì§ˆë¬¸ë³„ ì¶”ì¶œë³´ë‹¤ ìš°ì„ )
        retriever_index: ì‚¬ìš©í•  ë¦¬íŠ¸ë¦¬ë²„ ì¸ë±ìŠ¤ (Noneì´ë©´ DEFAULT_RETRIEVER_INDEX ì‚¬ìš©)
    """

    # ë³´ê³ ì„œ íƒ€ì…ì— ë§ëŠ” ì„¤ì • ë¡œë“œ
    config = get_report_config(report_type)

    # ì‚¬ìš©í•  ë¦¬íŠ¸ë¦¬ë²„ ê²°ì •
    if retriever_index is None:
        retriever_index = config['default_retriever_index']

    retriever_config = config['retriever_configs'][retriever_index]
    llm_configs = config['llm_configs']

    # Langfuse ì´ˆê¸°í™”
    langfuse = get_langfuse_client()

    report_type_display = "ìµœì¢…ë³´ê³ ì„œìš©" if report_type == "executive" else "ì£¼ê°„ë³´ê³ ì„œìš©"

    print("\n" + "=" * 100)
    print(f"ğŸ§ª {report_type_display} LLM ë¹„êµ í…ŒìŠ¤íŠ¸ (Langfuse ì—°ë™)")
    print("=" * 100)
    print(f"\nğŸ” ë¦¬íŠ¸ë¦¬ë²„: {retriever_config['display_name']} - {retriever_config['description']}")
    if global_date_filter:
        print(f"ğŸ“… ì „ì—­ ë‚ ì§œ í•„í„°: {global_date_filter[0][:10]} ~ {global_date_filter[1][:10]}")
    print("\nğŸ’¬ í‰ê°€ ëŒ€ìƒ LLM:")
    for i, llm_config in enumerate(llm_configs, 1):
        print(f"  {i}. {llm_config['display_name']} - {llm_config['description']}")
    print()

    all_results = []

    # íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ë¨¼ì € ìƒì„± (ëª¨ë“  ì €ì¥ì— ë™ì¼í•œ íƒ€ì„ìŠ¤íƒ¬í”„ ì‚¬ìš©)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    combination_base_dir = Path("data/results/combinations") / timestamp
    combination_base_dir.mkdir(parents=True, exist_ok=True)

    for q_idx, question in enumerate(questions, 1):
        print(f"\n{'=' * 100}")
        print(f"ğŸ“‹ ì§ˆë¬¸ {q_idx}/{len(questions)}")
        print(f"{'=' * 100}")
        print(f"â“ {question}\n")

        # ë‚ ì§œ í•„í„° ê²°ì •: ì „ì—­ í•„í„°ê°€ ìˆìœ¼ë©´ ê·¸ê²ƒ ì‚¬ìš©, ì—†ìœ¼ë©´ ì§ˆë¬¸ì—ì„œ ì¶”ì¶œ
        if global_date_filter:
            date_filter = global_date_filter
        else:
            date_filter = extract_date_filter_from_question(question)
            if date_filter:
                print(f"ğŸ“… ê°ì§€ëœ ë‚ ì§œ í•„í„°: {date_filter[0][:10]} ~ {date_filter[1][:10]}\n")

        # ë¬¸ì„œ ê²€ìƒ‰ (1ë²ˆë§Œ ìˆ˜í–‰)
        docs = retrieve_documents(question, retriever_config, config['qdrant_locks'], date_filter, langfuse, q_idx)

        question_result = {
            "question_id": q_idx,
            "question": question,
            "date_filter": f"{date_filter[0][:10]} ~ {date_filter[1][:10]}" if date_filter else None,
            "retriever": retriever_config['name'],
            "num_docs": len(docs),
            "doc_titles": [doc.metadata.get('page_title', 'Unknown') for doc in docs],
            "llms": []
        }

        # ê° LLMìœ¼ë¡œ ë³‘ë ¬ í…ŒìŠ¤íŠ¸
        print(f"\nğŸš€ 7ê°œ LLMìœ¼ë¡œ ë³‘ë ¬ ë‹µë³€ ìƒì„± ì‹œì‘...\n")

        with ThreadPoolExecutor(max_workers=7) as executor:
            # ëª¨ë“  LLM ì‘ì—…ì„ ë™ì‹œì— ì œì¶œ
            future_to_llm = {
                executor.submit(test_single_llm, llm_config, question, docs, report_type, langfuse, q_idx): (rank, llm_config)
                for rank, llm_config in enumerate(llm_configs, 1)
            }

            # ì™„ë£Œë˜ëŠ” ìˆœì„œëŒ€ë¡œ ê²°ê³¼ ìˆ˜ì§‘
            for future in as_completed(future_to_llm):
                rank, llm_config = future_to_llm[future]
                try:
                    result = future.result()
                    result["rank"] = rank
                    question_result["llms"].append(result)

                    # LLM ë‹µë³€ì´ ìƒì„±ë  ë•Œë§ˆë‹¤ ì¦‰ì‹œ ì €ì¥ (ì¤‘ê°„ì— ë©ˆì¶°ë„ ê²°ê³¼ ë³´ì¡´)
                    temp_results = all_results + [question_result]
                    try:
                        save_combination_results(temp_results, retriever_config, llm_configs, timestamp, combination_base_dir, questions)
                        print(f"ğŸ’¾ {llm_config['display_name']} ê²°ê³¼ ì €ì¥ ì™„ë£Œ")
                    except Exception as e:
                        print(f"âš ï¸ ì €ì¥ ì‹¤íŒ¨: {e}")

                except Exception as e:
                    print(f"âŒ {llm_config['display_name']} ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
                    result = {
                        "success": False,
                        "llm_name": llm_config['name'],
                        "llm_display_name": llm_config['display_name'],
                        "error": str(e),
                        "rank": rank
                    }
                    question_result["llms"].append(result)

        # rank ìˆœì„œëŒ€ë¡œ ì •ë ¬ (ë³‘ë ¬ ì‹¤í–‰ìœ¼ë¡œ ìˆœì„œê°€ ì„ì˜€ì„ ìˆ˜ ìˆìŒ)
        question_result["llms"].sort(key=lambda x: x["rank"])

        all_results.append(question_result)

        # ì§ˆë¬¸ì´ ëë‚  ë•Œë§ˆë‹¤ ìµœì¢… ì €ì¥
        print(f"\nğŸ’¾ ì§ˆë¬¸ {q_idx} ì™„ë£Œ - ìµœì¢… ê²°ê³¼ ì €ì¥ ì¤‘...")
        save_combination_results(all_results, retriever_config, llm_configs, timestamp, combination_base_dir, questions)

    # ìµœì¢… ì €ì¥ (ì´ë¯¸ ì¤‘ê°„ ì €ì¥ë˜ì—ˆìœ¼ë¯€ë¡œ summary.jsonë§Œ ì¶”ê°€ ì €ì¥)
    base_output_dir = Path("data/results/llm_comparison") / f"{retriever_config['name']}_{timestamp}"

    # ì „ì²´ ê²°ê³¼ ì €ì¥
    if output_file:
        output_path = Path(output_file)
    else:
        output_path = base_output_dir / "summary.json"

    output_path.parent.mkdir(parents=True, exist_ok=True)

    final_result = {
        "test_date": datetime.now().isoformat(),
        "report_type": report_type,
        "num_questions": len(questions),
        "retriever_config": retriever_config,
        "llm_configs": llm_configs,
        "results": all_results
    }

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(final_result, f, ensure_ascii=False, indent=2)

    print(f"\n{'=' * 100}")
    print(f"âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print(f"ğŸ“ Summary ì €ì¥: {output_path}")
    print(f"ğŸ“ ì¡°í•©ë³„ ê²°ê³¼: {combination_base_dir}")

    print(f"\nğŸ”— Langfuseì—ì„œ ìƒì„¸ ì¶”ì  ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”")
    print(f"{'=' * 100}")

    # Langfuse flush
    langfuse.flush()


def main():
    import argparse

    parser = argparse.ArgumentParser(
        description="ë³´ê³ ì„œìš© Top 3 ë¦¬íŠ¸ë¦¬ë²„ë¡œ ë³´ê³ ì„œ ìƒì„± ë° ë¹„êµ (Langfuse ì—°ë™)"
    )
    parser.add_argument(
        "--report-type",
        type=str,
        choices=["executive", "weekly"],
        default="executive",
        help="ë³´ê³ ì„œ íƒ€ì…: executive(ìµœì¢…ë³´ê³ ì„œ), weekly(ì£¼ê°„ë³´ê³ ì„œ) (ê¸°ë³¸: executive)"
    )
    parser.add_argument(
        "--mode",
        type=str,
        choices=["single", "multi", "interactive"],
        default="interactive",
        help="í…ŒìŠ¤íŠ¸ ëª¨ë“œ: single(ë‹¨ì¼ ì§ˆë¬¸), multi(íŒŒì¼ì—ì„œ ì—¬ëŸ¬ ì§ˆë¬¸), interactive(ëŒ€í™”í˜• ì…ë ¥)"
    )
    parser.add_argument(
        "--num-questions",
        type=int,
        default=5,
        help="multi ëª¨ë“œì—ì„œ í…ŒìŠ¤íŠ¸í•  ì§ˆë¬¸ ê°œìˆ˜ (ê¸°ë³¸: 5)"
    )
    parser.add_argument(
        "--question",
        type=str,
        default=None,
        help="single ëª¨ë“œì—ì„œ ì‚¬ìš©í•  ì§ˆë¬¸"
    )
    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="ê²°ê³¼ ì €ì¥ ê²½ë¡œ (ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ì €ì¥í•˜ì§€ ì•ŠìŒ)"
    )
    parser.add_argument(
        "--start-date",
        type=str,
        default=None,
        help="ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD í˜•ì‹)"
    )
    parser.add_argument(
        "--end-date",
        type=str,
        default=None,
        help="ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD í˜•ì‹)"
    )
    parser.add_argument(
        "--date-range",
        type=str,
        default=None,
        help="ìì—°ì–´ ë‚ ì§œ ë²”ìœ„ (ì˜ˆ: 'ì´ë²ˆ ì£¼', 'ì§€ë‚œì£¼', '12ì›” 2ì£¼ì°¨')"
    )
    parser.add_argument(
        "--retriever",
        type=int,
        choices=[0, 1, 2],
        default=None,
        help="ì‚¬ìš©í•  ë¦¬íŠ¸ë¦¬ë²„ ì¸ë±ìŠ¤ (0~2, ë³´ê³ ì„œ íƒ€ì…ë³„ë¡œ ë‹¤ë¦„)"
    )

    args = parser.parse_args()

    # ë‚ ì§œ í•„í„° íŒŒì‹±
    date_filter = parse_date_range(
        date_input=args.date_range,
        start_date=args.start_date,
        end_date=args.end_date
    )

    # ë³´ê³ ì„œ íƒ€ì…ì— ë§ëŠ” ì„¤ì • ë¡œë“œ
    config = get_report_config(args.report_type)

    if args.mode == "interactive":
        # ëŒ€í™”í˜• ëª¨ë“œ - ì‚¬ìš©ìê°€ ì§ì ‘ ì§ˆë¬¸ ì…ë ¥
        print("\n" + "=" * 100)
        print(f"ğŸ¯ ëŒ€í™”í˜• ëª¨ë“œ: ì§ˆë¬¸ì„ ì§ì ‘ ì…ë ¥í•˜ì„¸ìš” (ë³´ê³ ì„œ íƒ€ì…: {args.report_type})")
        print("=" * 100)
        print("ğŸ’¡ íŒ:")
        print("   - ì—¬ëŸ¬ ì§ˆë¬¸ì„ ì…ë ¥í•˜ë ¤ë©´ ì„¸ë¯¸ì½œë¡ (;)ìœ¼ë¡œ êµ¬ë¶„í•˜ì„¸ìš”")
        print("   - ì…ë ¥ ì—†ì´ ì—”í„°ë¥¼ ëˆ„ë¥´ë©´ ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ 5ê°œê°€ ì‚¬ìš©ë©ë‹ˆë‹¤")
        print()

        user_input = input("ğŸ“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì—”í„°: ê¸°ë³¸ ì§ˆë¬¸ ì‚¬ìš©): ").strip()

        if not user_input:
            print("\nâœ… ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤:")
            questions = config['test_questions']
            for i, q in enumerate(questions, 1):
                print(f"  {i}. {q}")
        else:
            # ì„¸ë¯¸ì½œë¡ ìœ¼ë¡œ êµ¬ë¶„ëœ ì—¬ëŸ¬ ì§ˆë¬¸ ì²˜ë¦¬
            questions = [q.strip() for q in user_input.split(';') if q.strip()]

            print(f"\nâœ… ì´ {len(questions)}ê°œì˜ ì§ˆë¬¸ì´ ì…ë ¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            for i, q in enumerate(questions, 1):
                print(f"  {i}. {q}")

        run_comparison_test(questions, args.report_type, args.output, date_filter, args.retriever)

    elif args.mode == "single":
        # ë‹¨ì¼ ì§ˆë¬¸ ëª¨ë“œ
        if not args.question:
            print("âŒ --question ì¸ìê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return
        run_comparison_test([args.question], args.report_type, args.output, date_filter, args.retriever)

    else:
        # ì—¬ëŸ¬ ì§ˆë¬¸ ëª¨ë“œ (íŒŒì¼ì—ì„œ ë¡œë“œ)
        test_questions_data = load_test_questions(n=args.num_questions)
        questions = [q["question"] for q in test_questions_data]

        if args.output is None:
            args.output = "data/final/llm_comparison_langfuse.json"

        run_comparison_test(questions, args.report_type, args.output, date_filter, args.retriever)


if __name__ == "__main__":
    main()
