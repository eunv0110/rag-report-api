#!/usr/bin/env python3
"""4ê°€ì§€ Retrieval ì „ëµ ì„±ëŠ¥ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

í‰ê°€ ì „ëµ:
  1. RRF (Baseline)
  2. RRF + MultiQuery
  3. RRF + MultiQuery + LongContext
  4. RRF + LongContext + TimeWeighted

ì‚¬ìš©ë²•:
    python evaluators/evaluate_4_strategies_new.py

    # íŠ¹ì • ì „ëµë§Œ í‰ê°€
    python evaluators/evaluate_4_strategies_new.py --strategies 1 2

    # Top-K ì„¤ì •
    python evaluators/evaluate_4_strategies_new.py --top-k 10
"""

import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

from dotenv import load_dotenv
load_dotenv()

import json
import time
from datetime import datetime
from typing import List, Dict, Any

from utils.langfuse import get_langfuse_client
from models.embeddings.factory import get_embedder
from utils.common import (
    load_prompt,
    load_evaluation_dataset,
    generate_llm_answer,
    create_trace_and_generation,
    add_retrieval_quality_score,
    save_embedding_cache
)
from utils.retriever_factory import create_strategy_retriever

# ìƒìˆ˜ ì •ì˜
DEFAULT_DATASET_PATH = "/home/work/rag/Project/rag-report-generator/data/evaluation/merged_qa_dataset.json"
DEFAULT_NUM_CONTEXTS_FOR_ANSWER = 5
DEFAULT_TEMPERATURE = 0.1
DEFAULT_MAX_TOKENS = 500
DEFAULT_TOP_K = 10


def generate_version_tag(retriever_name: str, version: str = "v1") -> str:
    """ë²„ì „ íƒœê·¸ ìƒì„±"""
    date_str = datetime.now().strftime("%Y%m%d")
    return f"{retriever_name}_{date_str}_{version}"


def evaluate_single_query(
    retriever,
    retriever_name: str,
    item: Dict[str, Any],
    langfuse,
    idx: int,
    top_k: int,
    base_version: str = "v1",
    retriever_tags: List[str] = None,
    prompt_version: int = 5
) -> Dict[str, Any]:
    """ë‹¨ì¼ ì¿¼ë¦¬ í‰ê°€"""
    question = item["question"]
    ground_truth = item["ground_truth"]
    context_page_id = item.get("context_page_id")
    item_metadata = item.get("metadata", {})

    version_tag = generate_version_tag(retriever_name, base_version)
    start_time = time.time()

    # ê²€ìƒ‰ ìˆ˜í–‰ (invoke ë©”ì„œë“œ ì‚¬ìš©)
    search_results = retriever.invoke(question)

    # LangChain Documentë¥¼ contextsë¡œ ë³€í™˜
    contexts = []
    context_metadata = []

    for result in search_results[:top_k]:
        contexts.append(result.page_content)
        context_metadata.append({
            "page_title": result.metadata.get('page_title', 'Unknown'),
            "section_title": result.metadata.get('section_title', 'N/A'),
            "chunk_id": result.metadata.get('chunk_id', 'unknown'),
            "score": result.metadata.get('_combined_score') or result.metadata.get('_similarity_score')
        })

    if not contexts:
        print(f"  âš ï¸ [{idx}] No contexts found for question!")
        contexts = ["ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."]

    # ë‹µë³€ ìƒì„± í”„ë¡¬í”„íŠ¸ ë¡œë“œ
    answer_prompt_file = f"prompts/templates/evaluation/weekly_report/answer_generation_prompt_ver{prompt_version}.txt"
    answer_prompt_template = load_prompt(answer_prompt_file)
    system_prompt = ""  # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ëŠ” answer_generation_promptì— í¬í•¨ë˜ì–´ ìˆìŒ

    # LLM ë‹µë³€ ìƒì„±
    answer = generate_llm_answer(
        question=question,
        contexts=contexts,
        system_prompt=system_prompt,
        answer_generation_prompt=answer_prompt_template,
        num_contexts=DEFAULT_NUM_CONTEXTS_FOR_ANSWER,
        temperature=DEFAULT_TEMPERATURE,
        max_tokens=DEFAULT_MAX_TOKENS
    )

    if not answer or answer.startswith("ë‹µë³€ ìƒì„± ì‹¤íŒ¨") or answer.startswith("Azure OpenAI ì„¤ì •"):
        print(f"  âš ï¸ [{idx}] LLM answer generation failed!")
        if not answer:
            answer = "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    total_time = time.time() - start_time

    # Langfuse Trace & Generation
    additional_metadata = {"context_page_id": context_page_id}
    trace_id = create_trace_and_generation(
        langfuse=langfuse,
        retriever_name=retriever_name,
        question=question,
        contexts=contexts,
        answer=answer,
        ground_truth=ground_truth,
        context_metadata=context_metadata,
        item_metadata=item_metadata,
        total_time=total_time,
        idx=idx,
        version_tag=version_tag,
        retriever_tags=retriever_tags,
        additional_metadata=additional_metadata
    )

    # ê²€ìƒ‰ í’ˆì§ˆ ìŠ¤ì½”ì–´ ì¶”ê°€
    add_retrieval_quality_score(langfuse, trace_id, context_metadata)

    print(f"  [{idx}] {question[:50]}... ({len(contexts)}ê°œ ë¬¸ì„œ, {total_time*1000:.0f}ms)")

    return {
        "question": question,
        "answer": answer,
        "ground_truth": ground_truth,
        "num_contexts": len(contexts),
        "time": total_time,
        "trace_id": trace_id
    }


def evaluate_retriever(
    retriever,
    retriever_name: str,
    eval_data: List[Dict[str, Any]],
    langfuse,
    top_k: int = DEFAULT_TOP_K,
    base_version: str = "v1",
    retriever_tags: List[str] = None,
    prompt_version: int = 5
) -> Dict[str, Any]:
    """ë¦¬íŠ¸ë¦¬ë²„ í‰ê°€"""
    print(f"\n{'=' * 60}")
    print(f"ğŸ” {retriever_name} í‰ê°€ ì¤‘...")
    print(f"{'=' * 60}")

    stats = {
        "total_queries": len(eval_data),
        "total_time": 0,
        "evaluations": []
    }

    for idx, item in enumerate(eval_data, 1):
        eval_result = evaluate_single_query(
            retriever=retriever,
            retriever_name=retriever_name,
            item=item,
            langfuse=langfuse,
            idx=idx,
            top_k=top_k,
            base_version=base_version,
            retriever_tags=retriever_tags,
            prompt_version=prompt_version
        )

        stats["evaluations"].append(eval_result)
        stats["total_time"] += eval_result["time"]

        # ê° ì§ˆë¬¸ í‰ê°€ í›„ ìºì‹œ ì €ì¥
        save_embedding_cache()

    stats["avg_time"] = stats["total_time"] / stats["total_queries"]
    stats["avg_contexts"] = sum(e["num_contexts"] for e in stats["evaluations"]) / stats["total_queries"]

    return stats




def run_evaluation(
    strategy_num: int,
    dataset: str,
    top_k: int,
    version: str,
    langfuse,
    prompt_version: int = 5
) -> bool:
    """ë‹¨ì¼ í‰ê°€ ì „ëµ ì‹¤í–‰

    Args:
        strategy_num: ì „ëµ ë²ˆí˜¸
        dataset: í‰ê°€ ë°ì´í„°ì…‹ ê²½ë¡œ
        top_k: Top-K ê°’
        version: ë²„ì „ íƒœê·¸
        langfuse: Langfuse í´ë¼ì´ì–¸íŠ¸
        prompt_version: í”„ë¡¬í”„íŠ¸ ë²„ì „ (2-6)

    Returns:
        ì„±ê³µ ì—¬ë¶€
    """
    STRATEGIES = {
        1: "RRF (Baseline)",
        2: "RRF + MultiQuery",
        3: "RRF + MultiQuery + LongContext",
        4: "RRF + LongContext + TimeWeighted"
    }

    strategy_name = STRATEGIES[strategy_num]

    print("\n" + "=" * 70)
    print(f"{strategy_num}ï¸âƒ£  {strategy_name} í‰ê°€ ì¤‘...")
    print("=" * 70)

    try:
        # í‰ê°€ ë°ì´í„°ì…‹ ë¡œë“œ
        eval_data = load_evaluation_dataset(dataset)

        # ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±
        retriever, retriever_name, retriever_tags = create_strategy_retriever(strategy_num, top_k)

        print(f"âœ… ë¦¬íŠ¸ë¦¬ë²„ ì´ˆê¸°í™” ì™„ë£Œ: {retriever_name}")
        print(f"   - íƒœê·¸: {', '.join(retriever_tags)}")

        # í‰ê°€ ìˆ˜í–‰
        stats = evaluate_retriever(
            retriever=retriever,
            retriever_name=retriever_name,
            eval_data=eval_data,
            langfuse=langfuse,
            top_k=top_k,
            base_version=version,
            retriever_tags=retriever_tags,
            prompt_version=prompt_version
        )

        # ê²°ê³¼ ì €ì¥
        output_file = Path(dataset).parent / f"{retriever_name}_prompt_v{prompt_version}_evaluation_stats.json"
        save_result = {k: v for k, v in stats.items() if k != "evaluations"}
        save_result["num_evaluations"] = len(stats.get("evaluations", []))
        save_result["config"] = {
            "strategy_num": strategy_num,
            "strategy_name": strategy_name,
            "retriever_name": retriever_name,
            "retriever_tags": retriever_tags,
            "top_k": top_k,
            "version": version,
            "prompt_version": prompt_version,
        }

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(save_result, f, indent=2, ensure_ascii=False, default=str)

        print(f"\nâœ… {strategy_name} í‰ê°€ ì™„ë£Œ")
        print(f"   - í‰ê·  ì‹œê°„: {stats['avg_time']*1000:.2f}ms")
        print(f"   - í‰ê·  ì»¨í…ìŠ¤íŠ¸ ìˆ˜: {stats['avg_contexts']:.2f}")
        print(f"   - ê²°ê³¼ ì €ì¥: {output_file}")

        return True

    except Exception as e:
        print(f"âŒ {strategy_name} í‰ê°€ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse

    parser = argparse.ArgumentParser(
        description="4ê°€ì§€ Retrieval ì „ëµ ì„±ëŠ¥ í‰ê°€",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )

    parser.add_argument(
        "--strategies",
        type=int,
        nargs="+",
        choices=[1, 2, 3, 4],
        default=[1, 2, 3, 4],
        help="í‰ê°€í•  ì „ëµ ë²ˆí˜¸ (ê¸°ë³¸ê°’: ëª¨ë‘)"
    )

    parser.add_argument(
        "--dataset",
        type=str,
        default=DEFAULT_DATASET_PATH,
        help="í‰ê°€ ë°ì´í„°ì…‹ ê²½ë¡œ"
    )

    parser.add_argument(
        "--top-k",
        type=int,
        default=DEFAULT_TOP_K,
        help="Top-K ê°’ (ê¸°ë³¸ê°’: 10)"
    )

    parser.add_argument(
        "--version",
        type=str,
        default="v1",
        help="ë²„ì „ íƒœê·¸ (ê¸°ë³¸ê°’: v1)"
    )

    parser.add_argument(
        "--prompt-version",
        type=int,
        choices=[2, 3, 4, 5, 6, 7],
        default=5,
        help="í”„ë¡¬í”„íŠ¸ ë²„ì „ (2-7, ê¸°ë³¸ê°’: 5)"
    )

    args = parser.parse_args()

    # ì‹œì‘ ë©”ì‹œì§€
    print("=" * 70)
    print("ğŸš€ 4ê°€ì§€ Retrieval ì „ëµ ì„±ëŠ¥ í‰ê°€ ì‹œì‘")
    print("=" * 70)
    print(f"\nğŸ“Š í‰ê°€ ì„¤ì •:")
    print(f"   - Dataset: {args.dataset}")
    print(f"   - Top-K: {args.top_k}")
    print(f"   - Version: {args.version}")
    print(f"   - Prompt Version: {args.prompt_version}")
    print(f"   - í‰ê°€í•  ì „ëµ: {args.strategies}")

    # Langfuse í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    langfuse = get_langfuse_client()
    if not langfuse:
        print("âŒ Langfuse í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ê° ì „ëµ í‰ê°€
    results = {}
    for strategy_num in sorted(args.strategies):
        success = run_evaluation(
            strategy_num=strategy_num,
            dataset=args.dataset,
            top_k=args.top_k,
            version=args.version,
            langfuse=langfuse,
            prompt_version=args.prompt_version
        )
        results[strategy_num] = success

    # Langfuse flush
    print("\nâ³ Langfuseì— ë°ì´í„° ì „ì†¡ ì¤‘...")
    langfuse.flush()

    # ì„ë² ë”© ìºì‹œ ì €ì¥
    print("\nğŸ’¾ ì„ë² ë”© ìºì‹œ ì €ì¥ ì¤‘...")
    try:
        embedder = get_embedder()
        if hasattr(embedder, 'save_cache'):
            embedder.save_cache()
        else:
            print("  â„¹ï¸  ìºì‹œ ì €ì¥ ë©”ì„œë“œ ì—†ìŒ (ìºì‹œ ë¹„í™œì„±í™” ìƒíƒœ)")
    except Exception as e:
        print(f"  âš ï¸  ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

    # ê²°ê³¼ ìš”ì•½
    STRATEGY_NAMES = {
        1: "RRF (Baseline)",
        2: "RRF + MultiQuery",
        3: "RRF + MultiQuery + LongContext",
        4: "RRF + LongContext + TimeWeighted"
    }

    print("\n" + "=" * 70)
    print("ğŸ“Š í‰ê°€ ê²°ê³¼ ìš”ì•½")
    print("=" * 70)

    for strategy_num in sorted(args.strategies):
        strategy_name = STRATEGY_NAMES[strategy_num]
        status = "âœ… ì„±ê³µ" if results[strategy_num] else "âŒ ì‹¤íŒ¨"
        print(f"{strategy_num}. {strategy_name:<40} {status}")

    # ì™„ë£Œ ë©”ì‹œì§€
    success_count = sum(results.values())
    total_count = len(results)

    print("\n" + "=" * 70)
    if success_count == total_count:
        print("âœ… ëª¨ë“  í‰ê°€ ì™„ë£Œ!")
    else:
        print(f"âš ï¸  ì¼ë¶€ í‰ê°€ ì‹¤íŒ¨ ({success_count}/{total_count} ì„±ê³µ)")
    print("=" * 70)

    print("\nğŸ“Š ë‹¤ìŒ ë‹¨ê³„:")
    print("   1. Langfuse ëŒ€ì‹œë³´ë“œì—ì„œ ê²°ê³¼ í™•ì¸: https://cloud.langfuse.com")
    print("   2. ê° ì „ëµë³„ stats íŒŒì¼ í™•ì¸:")
    print("      - data/evaluation/ensemble_rrf_evaluation_stats.json")
    print("      - data/evaluation/rrf_multiquery_evaluation_stats.json")
    print("      - data/evaluation/rrf_multiquery_longcontext_evaluation_stats.json")
    print("      - data/evaluation/rrf_longcontext_timeweighted_evaluation_stats.json")
    print()

    # ì‹¤íŒ¨í•œ ê²½ìš° ì¢…ë£Œ ì½”ë“œ 1 ë°˜í™˜
    if success_count < total_count:
        sys.exit(1)


if __name__ == "__main__":
    main()
