#!/usr/bin/env python3
"""ì—¬ëŸ¬ LLM ëª¨ë¸ì„ ì‚¬ìš©í•œ ë³´ê³ ì„œ í…ŒìŠ¤íŠ¸ (Langfuse ì—°ë™)

evaluation_config.yamlì˜ ì„¤ì •ì„ ì‚¬ìš©í•˜ì—¬ ê° ë³´ê³ ì„œ íƒ€ì…ë³„ë¡œ ì§€ì •ëœ ë¦¬íŠ¸ë¦¬ë²„ì™€ LLM ì¡°í•©ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤:
- weekly: bge_m3_rrf_ensemble + Qwen3-Reranker-4B (Top 6)
- executive: openai_rrf_multiquery (Top 8)
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from dotenv import load_dotenv
load_dotenv()

import json
from datetime import datetime
from typing import List, Dict, Any
from concurrent.futures import ThreadPoolExecutor, as_completed

from config.settings import EVALUATION_CONFIG
from utils.langfuse_utils import get_langfuse_client
from utils.common_utils import save_embedding_cache

# Reranker í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ í•¨ìˆ˜ ì¬ì‚¬ìš©
from evaluators.evaluation.evaluate_reranker import (
    retrieve_and_rerank_documents,
    generate_answer_with_llm,
    get_report_config,
    load_prompt
)


def get_test_llms():
    """evaluation_config.yamlì—ì„œ í…ŒìŠ¤íŠ¸ LLM ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    return EVALUATION_CONFIG.get('test_llms', [])


def get_retriever_config_from_yaml(report_type: str) -> Dict[str, Any]:
    """evaluation_config.yamlì—ì„œ ë³´ê³ ì„œ íƒ€ì…ë³„ ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì • ë°˜í™˜

    Args:
        report_type: 'weekly' ë˜ëŠ” 'executive'

    Returns:
        ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì • ë”•ì…”ë„ˆë¦¬
    """
    key = f"{report_type}_report"
    retrievers = EVALUATION_CONFIG.get('simple_test_retrievers', {}).get(key, [])

    if not retrievers:
        raise ValueError(f"No retrievers found for {report_type}_report in evaluation_config.yaml")

    # ë¦¬íŠ¸ë¦¬ë²„ ì„ íƒ
    if report_type == "weekly":
        # weekly: ì²« ë²ˆì§¸ (bge_m3_rrf_ensemble, Top 6)
        retriever = retrievers[0]
    elif report_type == "executive":
        # executive: openai_rrf_multiquery (Top 8) ì„ íƒ
        # configì—ì„œ nameì´ "openai_rrf_multiquery"ì¸ ê²ƒ ì°¾ê¸°
        retriever = next(
            (r for r in retrievers if r['name'] == 'openai_rrf_multiquery'),
            retrievers[0]  # ëª» ì°¾ìœ¼ë©´ ì²« ë²ˆì§¸ ì‚¬ìš©
        )
    else:
        retriever = retrievers[0]

    # ë¦¬íŠ¸ë¦¬ë²„ íƒ€ì… ë§¤í•‘
    retriever_type_map = {
        "rrf_ensemble": "bge-m3",
        "rrf_multiquery_lc": "openai-large-rrf-multiquery",
        "rrf_multiquery": "openai-rrf-multiquery"
    }

    retriever_type = retriever_type_map.get(retriever['type'], "bge-m3")

    # Reranker ì‚¬ìš© ì—¬ë¶€ íŒë‹¨ (rrf_ensemble íƒ€ì…ì´ë©´ reranker ì‚¬ìš©)
    use_reranker = retriever['type'] == 'rrf_ensemble'

    return {
        "name": retriever['name'],
        "display_name": retriever['display_name'],
        "retriever_type": retriever_type,
        "reranker_type": "qwen3" if use_reranker else None,
        "top_k": retriever['top_k'],
        "description": retriever['description'],
        "embedding": retriever['embedding']
    }


def retrieve_with_multiquery_llm(
    question: str,
    retriever_config: Dict[str, Any],
    llm_config: Dict[str, Any],
    langfuse,
    question_id: int = None
) -> list:
    """íŠ¹ì • LLMì„ ì‚¬ìš©í•˜ì—¬ MultiQuery ê²€ìƒ‰ ìˆ˜í–‰

    Args:
        question: ê²€ìƒ‰ ì§ˆë¬¸
        retriever_config: ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì •
        llm_config: MultiQueryì— ì‚¬ìš©í•  LLM ì„¤ì •
        langfuse: Langfuse í´ë¼ì´ì–¸íŠ¸
        question_id: ì§ˆë¬¸ ID

    Returns:
        ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
    """
    from utils.retriever_factory import create_retriever_from_config
    from retrievers.multiquery_retriever import MultiQueryRetriever

    # ê¸°ë³¸ ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì •
    base_retriever_cfg = {
        "name": f"{retriever_config['name']}_base",
        "display_name": retriever_config['display_name'],
        "description": retriever_config['description'],
        "embedding_preset": "openai-large",
        "retriever_type": "rrf_ensemble",  # MultiQueryì˜ baseëŠ” rrf_ensemble ì‚¬ìš©
        "bm25_weight": 0.5,
        "dense_weight": 0.5,
    }

    # Langfuse spanìœ¼ë¡œ ê²€ìƒ‰ ê³¼ì • ê¸°ë¡
    if langfuse and question_id:
        with langfuse.start_as_current_observation(
            as_type='span',
            name=f"retrieval_multiquery_{llm_config['name']}_q{question_id}",
            input={"question": question},
            metadata={
                "retriever_name": retriever_config['name'],
                "retriever_type": "rrf_multiquery",
                "top_k": retriever_config['top_k'],
                "multiquery_llm": llm_config['model_id'],
                "multiquery_llm_name": llm_config['name']
            }
        ) as span:
            # Base retriever ìƒì„±
            base_retriever, _ = create_retriever_from_config(base_retriever_cfg, retriever_config['top_k'] * 2)

            # MultiQueryRetriever ìƒì„± (LLM ì§€ì •)
            multiquery_retriever = MultiQueryRetriever(
                base_retriever=base_retriever,
                num_queries=3,
                temperature=0.7,
                llm_model=llm_config['model_id']
            )

            print(f"ğŸ” {llm_config['display_name']}ë¡œ MultiQuery ê²€ìƒ‰ ì¤‘ (k={retriever_config['top_k']})...")
            docs = multiquery_retriever.invoke(question)

            # Top-Kë¡œ ì œí•œ
            docs = docs[:retriever_config['top_k']]

            span.update(output={
                "num_docs": len(docs),
                "doc_titles": [doc.metadata.get('page_title', 'Unknown') for doc in docs]
            })

            print(f"ğŸ“„ ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(docs)}")

            return docs
    else:
        # Langfuse ì—†ì´ ì‹¤í–‰
        base_retriever, _ = create_retriever_from_config(base_retriever_cfg, retriever_config['top_k'] * 2)
        multiquery_retriever = MultiQueryRetriever(
            base_retriever=base_retriever,
            num_queries=3,
            temperature=0.7,
            llm_model=llm_config['model_id']
        )

        print(f"ğŸ” {llm_config['display_name']}ë¡œ MultiQuery ê²€ìƒ‰ ì¤‘ (k={retriever_config['top_k']})...")
        docs = multiquery_retriever.invoke(question)
        docs = docs[:retriever_config['top_k']]
        print(f"ğŸ“„ ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(docs)}")

        return docs


def test_single_llm_simple(
    llm_config: Dict[str, Any],
    question: str,
    docs: list,
    report_type: str,
    langfuse,
    question_id: int = None,
    retriever_config: Dict[str, Any] = None,
    use_multiquery_per_llm: bool = False,
    version: str = "v1"
) -> Dict[str, Any]:
    """ë‹¨ì¼ LLMìœ¼ë¡œ ë‹µë³€ ìƒì„± (ê°„ë‹¨ ë²„ì „)

    Args:
        llm_config: LLM ì„¤ì •
        question: ì§ˆë¬¸
        docs: ì‚¬ì „ ê²€ìƒ‰ëœ ë¬¸ì„œ (use_multiquery_per_llm=Falseì¼ ë•Œ ì‚¬ìš©)
        report_type: ë³´ê³ ì„œ íƒ€ì…
        langfuse: Langfuse í´ë¼ì´ì–¸íŠ¸
        question_id: ì§ˆë¬¸ ID
        retriever_config: ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì • (use_multiquery_per_llm=Trueì¼ ë•Œ í•„ìš”)
        use_multiquery_per_llm: ê° LLMë§ˆë‹¤ ë³„ë„ë¡œ MultiQuery ê²€ìƒ‰ ìˆ˜í–‰ ì—¬ë¶€
        version: í‰ê°€ ë²„ì „ íƒœê·¸
    """

    print(f"\n{'=' * 100}")
    print(f"ğŸ’¬ {llm_config['display_name']}")
    print(f"   {llm_config['description']}")
    print(f"{'=' * 100}")

    try:
        # MultiQueryë¥¼ ê° LLMë§ˆë‹¤ ì‚¬ìš©í•˜ëŠ” ê²½ìš°
        if use_multiquery_per_llm and retriever_config:
            docs = retrieve_with_multiquery_llm(
                question=question,
                retriever_config=retriever_config,
                llm_config=llm_config,
                langfuse=langfuse,
                question_id=question_id
            )

        print("ğŸ’¬ ë‹µë³€ ìƒì„± ì¤‘...")
        answer = generate_answer_with_llm(
            question,
            docs,
            llm_config,
            report_type,
            langfuse,
            question_id,
            version
        )

        print(f"\nâœ… ìƒì„±ëœ ë‹µë³€ (ì•ë¶€ë¶„):\n{answer[:500]}...\n")

        return {
            "success": True,
            "llm_name": llm_config['name'],
            "llm_display_name": llm_config['display_name'],
            "model_id": llm_config['model_id'],
            "answer": answer,
            "answer_length": len(answer),
            "num_docs": len(docs),
            "doc_titles": [doc.metadata.get('page_title', 'Unknown') for doc in docs]
        }

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}\n")
        import traceback
        traceback.print_exc()
        return {
            "success": False,
            "llm_name": llm_config['name'],
            "llm_display_name": llm_config['display_name'],
            "model_id": llm_config.get('model_id', 'unknown'),
            "error": str(e)
        }


def run_multi_llm_test(
    questions: List[str],
    report_type: str,
    output_dir: str = None,
    version: str = "v1"
):
    """ì—¬ëŸ¬ LLMìœ¼ë¡œ ë³´ê³ ì„œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰

    Args:
        questions: í…ŒìŠ¤íŠ¸í•  ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
        report_type: ë³´ê³ ì„œ íƒ€ì… ('weekly' ë˜ëŠ” 'executive')
        output_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        version: í‰ê°€ ë²„ì „ íƒœê·¸ (ê¸°ë³¸: v1)
    """

    # Langfuse í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    langfuse = get_langfuse_client()
    if not langfuse:
        print("âŒ Langfuse í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # evaluation_config.yamlì—ì„œ ë¦¬íŠ¸ë¦¬ë²„ ë° LLM ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    retriever_config = get_retriever_config_from_yaml(report_type)
    test_llms = get_test_llms()

    if not test_llms:
        print("âŒ evaluation_config.yamlì— test_llms ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    report_type_display = "ì£¼ê°„ë³´ê³ ì„œ" if report_type == "weekly" else "ìµœì¢…ë³´ê³ ì„œ"

    print("\n" + "=" * 100)
    print(f"ğŸ§ª {report_type_display} - ì—¬ëŸ¬ LLM í…ŒìŠ¤íŠ¸ (Langfuse ì—°ë™)")
    print("=" * 100)
    print(f"\nğŸ” ë¦¬íŠ¸ë¦¬ë²„: {retriever_config['display_name']}")
    print(f"   ì„¤ëª…: {retriever_config['description']}")
    print(f"ğŸ“Š Top-K: {retriever_config['top_k']}")
    print(f"\nğŸ’¬ í…ŒìŠ¤íŠ¸í•  LLM ({len(test_llms)}ê°œ):")
    for i, llm_config in enumerate(test_llms, 1):
        print(f"  {i}. {llm_config['display_name']} - {llm_config['description']}")
    print(f"\nğŸ“‹ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ({len(questions)}ê°œ):")
    for i, q in enumerate(questions, 1):
        print(f"  {i}. {q}")
    print()

    # ê²°ê³¼ ì €ì¥ìš©
    all_results = []
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # ê° ì§ˆë¬¸ì— ëŒ€í•´ í…ŒìŠ¤íŠ¸
    for q_idx, question in enumerate(questions, 1):
        print(f"\n{'#' * 100}")
        print(f"ğŸ“‹ ì§ˆë¬¸ {q_idx}/{len(questions)}")
        print(f"{'#' * 100}")
        print(f"â“ {question}\n")

        # MultiQuery ì‚¬ìš© ì—¬ë¶€ ê²°ì •
        use_multiquery_per_llm = retriever_config['reranker_type'] is None  # executiveëŠ” True

        # ë¬¸ì„œ ê²€ìƒ‰ (ë¦¬íŠ¸ë¦¬ë²„ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì²˜ë¦¬)
        if retriever_config['reranker_type']:
            # Reranker ì‚¬ìš© (weekly - BGE-M3 + RRF Ensemble)
            # í•œ ë²ˆë§Œ ê²€ìƒ‰í•˜ê³  ëª¨ë“  LLMì´ ë™ì¼í•œ ë¬¸ì„œ ì‚¬ìš©
            docs = retrieve_and_rerank_documents(
                question=question,
                report_type=report_type,
                top_k=retriever_config['top_k'],
                date_filter=None,
                langfuse=langfuse,
                question_id=q_idx,
                batch_size=16,
                retriever_type=retriever_config['retriever_type'],
                reranker_type=retriever_config['reranker_type']
            )
        else:
            # MultiQuery ì‚¬ìš© (executive)
            # ê° LLMë§ˆë‹¤ ë³„ë„ë¡œ ê²€ìƒ‰í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ë¹ˆ ë¦¬ìŠ¤íŠ¸
            print(f"ğŸ” ê° LLMì´ ë…ë¦½ì ìœ¼ë¡œ MultiQuery ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
            docs = []  # ê° LLMì´ ìì²´ì ìœ¼ë¡œ ê²€ìƒ‰

        question_result = {
            "question_id": q_idx,
            "question": question,
            "retriever": retriever_config['display_name'],
            "top_k": retriever_config['top_k'],
            "use_multiquery_per_llm": use_multiquery_per_llm,
            "num_docs": len(docs) if docs else 0,
            "llms": []
        }

        # ê° LLMìœ¼ë¡œ ë³‘ë ¬ í…ŒìŠ¤íŠ¸
        if use_multiquery_per_llm:
            print(f"\nğŸš€ {len(test_llms)}ê°œ LLMìœ¼ë¡œ ë³‘ë ¬ MultiQuery ê²€ìƒ‰ + ë‹µë³€ ìƒì„± ì‹œì‘...\n")
        else:
            print(f"\nğŸš€ {len(test_llms)}ê°œ LLMìœ¼ë¡œ ë³‘ë ¬ ë‹µë³€ ìƒì„± ì‹œì‘...\n")

        with ThreadPoolExecutor(max_workers=min(len(test_llms), 7)) as executor:
            future_to_llm = {
                executor.submit(
                    test_single_llm_simple,
                    llm_config,
                    question,
                    docs,
                    report_type,
                    langfuse,
                    q_idx,
                    retriever_config,
                    use_multiquery_per_llm,
                    version
                ): (rank, llm_config)
                for rank, llm_config in enumerate(test_llms, 1)
            }

            for future in as_completed(future_to_llm):
                rank, llm_config = future_to_llm[future]
                try:
                    result = future.result()
                    result["rank"] = rank
                    question_result["llms"].append(result)
                except Exception as e:
                    print(f"âŒ {llm_config['display_name']} ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
                    result = {
                        "success": False,
                        "llm_name": llm_config['name'],
                        "llm_display_name": llm_config['display_name'],
                        "error": str(e),
                        "rank": rank
                    }
                    question_result["llms"].append(result)

        # rank ìˆœìœ¼ë¡œ ì •ë ¬
        question_result["llms"].sort(key=lambda x: x["rank"])
        all_results.append(question_result)

        # ì¤‘ê°„ ì €ì¥
        save_results(all_results, report_type, retriever_config, test_llms, timestamp, output_dir)

    # ìµœì¢… ì €ì¥
    final_result = save_results(all_results, report_type, retriever_config, test_llms, timestamp, output_dir, is_final=True)

    print(f"\n{'=' * 100}")
    print(f"âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print(f"ğŸ“ ê²°ê³¼ ì €ì¥: {final_result}")
    print(f"{'=' * 100}\n")

    # Langfuse flush
    print("\nâ³ Langfuseì— ë°ì´í„° ì „ì†¡ ì¤‘...")
    langfuse.flush()

    # ì„ë² ë”© ìºì‹œ ì €ì¥
    print("\nğŸ’¾ ì„ë² ë”© ìºì‹œ ì €ì¥ ì¤‘...")
    save_embedding_cache()


def save_results(
    all_results: List[Dict],
    report_type: str,
    retriever_config: Dict,
    test_llms: List[Dict],
    timestamp: str,
    output_dir: str = None,
    is_final: bool = False
) -> str:
    """ê²°ê³¼ ì €ì¥

    Args:
        all_results: ì „ì²´ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        report_type: ë³´ê³ ì„œ íƒ€ì…
        retriever_config: ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì •
        test_llms: í…ŒìŠ¤íŠ¸ LLM ëª©ë¡
        timestamp: íƒ€ì„ìŠ¤íƒ¬í”„
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        is_final: ìµœì¢… ì €ì¥ ì—¬ë¶€

    Returns:
        ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
    """
    if output_dir:
        base_dir = Path(output_dir)
    else:
        base_dir = Path("data/results/multi_llm_test")

    # ë³´ê³ ì„œ íƒ€ì…ë³„ ë””ë ‰í† ë¦¬
    output_path = base_dir / report_type / timestamp
    output_path.mkdir(parents=True, exist_ok=True)

    # JSON ì €ì¥
    output_file = output_path / ("results_final.json" if is_final else "results_temp.json")

    result_data = {
        "test_date": datetime.now().isoformat(),
        "report_type": report_type,
        "retriever_config": retriever_config,
        "num_llms": len(test_llms),
        "llm_configs": test_llms,
        "num_questions": len(all_results),
        "results": all_results
    }

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result_data, f, ensure_ascii=False, indent=2)

    # í…ìŠ¤íŠ¸ ìš”ì•½ë³¸ ì €ì¥
    if is_final:
        summary_file = output_path / "summary.txt"
        summary_lines = []

        summary_lines.append("=" * 100)
        summary_lines.append(f"{retriever_config['display_name']} - ì—¬ëŸ¬ LLM í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
        summary_lines.append("=" * 100)
        summary_lines.append("")

        for q_result in all_results:
            summary_lines.append(f"{'#' * 100}")
            summary_lines.append(f"ì§ˆë¬¸ {q_result['question_id']}: {q_result['question']}")
            summary_lines.append(f"{'#' * 100}")
            summary_lines.append(f"ê²€ìƒ‰ëœ ë¬¸ì„œ: {q_result['num_docs']}ê°œ")
            summary_lines.append("")

            for llm_result in q_result['llms']:
                summary_lines.append(f"\n{'=' * 80}")
                summary_lines.append(f"LLM: {llm_result['llm_display_name']}")
                summary_lines.append(f"{'=' * 80}")

                if llm_result.get('success'):
                    answer = llm_result.get('answer', 'N/A')
                    summary_lines.append(f"ë‹µë³€ (ê¸¸ì´: {llm_result.get('answer_length', 0)}ì):")
                    summary_lines.append(answer)
                else:
                    summary_lines.append(f"âŒ ì˜¤ë¥˜: {llm_result.get('error', 'Unknown error')}")

                summary_lines.append("")

        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write("\n".join(summary_lines))

    return str(output_file)


def main():
    import argparse

    parser = argparse.ArgumentParser(
        description="ì—¬ëŸ¬ LLM ëª¨ë¸ì„ ì‚¬ìš©í•œ ë³´ê³ ì„œ í…ŒìŠ¤íŠ¸ (Langfuse ì—°ë™)"
    )
    parser.add_argument(
        "--report-type",
        type=str,
        choices=["weekly", "executive"],
        required=True,
        help="ë³´ê³ ì„œ íƒ€ì…: weekly(ì£¼ê°„ë³´ê³ ì„œ), executive(ìµœì¢…ë³´ê³ ì„œ)"
    )
    parser.add_argument(
        "--questions",
        type=str,
        nargs="+",
        default=None,
        help="í…ŒìŠ¤íŠ¸í•  ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ (ê³µë°±ìœ¼ë¡œ êµ¬ë¶„). ë¯¸ì§€ì •ì‹œ ê¸°ë³¸ ì§ˆë¬¸ ì‚¬ìš©"
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default=None,
        help="ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: data/results/multi_llm_test)"
    )
    parser.add_argument(
        "--version",
        type=str,
        default="v1",
        help="í‰ê°€ ë²„ì „ íƒœê·¸ (ê¸°ë³¸: v1)"
    )

    args = parser.parse_args()

    # ê¸°ë³¸ ì§ˆë¬¸ ì„¤ì •
    if args.questions:
        questions = args.questions
    else:
        # evaluation_config.yamlì—ì„œ ê¸°ë³¸ ì§ˆë¬¸ ë¡œë“œ
        config = get_report_config(args.report_type)
        questions = config['test_questions']  # ëª¨ë“  ì§ˆë¬¸ ì‚¬ìš© (5ê°œ)

        print(f"\nâœ… ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤ ({len(questions)}ê°œ):")
        for i, q in enumerate(questions, 1):
            print(f"  {i}. {q}")

    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    run_multi_llm_test(
        questions=questions,
        report_type=args.report_type,
        output_dir=args.output_dir,
        version=args.version
    )


if __name__ == "__main__":
    main()
