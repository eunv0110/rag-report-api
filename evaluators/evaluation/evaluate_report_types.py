#!/usr/bin/env python3
"""ì£¼ê°„ ë³´ê³ ì„œ vs ì„ì› ë³´ê³ ì„œ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

ê° ë³´ê³ ì„œ íƒ€ì…ì— ìµœì í™”ëœ ë¦¬íŠ¸ë¦¬ë²„ ì¡°í•©ì„ í‰ê°€í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    # ëª¨ë“  í‰ê°€ ì‹¤í–‰ (ì£¼ê°„ + ì„ì›)
    python evaluators/evaluate_report_types.py --report-type both

    # ì£¼ê°„ ë³´ê³ ì„œë§Œ í‰ê°€
    python evaluators/evaluate_report_types.py --report-type weekly

    # ì„ì› ë³´ê³ ì„œë§Œ í‰ê°€
    python evaluators/evaluate_report_types.py --report-type executive

    # íŠ¹ì • ë¦¬íŠ¸ë¦¬ë²„ ì¡°í•©ë§Œ í‰ê°€
    python evaluators/evaluate_report_types.py --report-type weekly --retrievers upstage_rrf_multiquery_lc
"""

import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

from dotenv import load_dotenv
load_dotenv()

import json
import time
import yaml
from datetime import datetime
from typing import List, Dict, Any

from utils.langfuse_utils import get_langfuse_client
from utils.common_utils import (
    load_prompt,
    load_evaluation_dataset,
    generate_llm_answer,
    add_retrieval_quality_score,
    save_embedding_cache
)
from utils.retriever_factory import create_retriever_from_config

# ìƒìˆ˜ ì •ì˜
DEFAULT_DATASET_PATH = "/home/work/rag/Project/rag-report-generator/data/evaluation/merged_qa_dataset.json"
DEFAULT_NUM_CONTEXTS_FOR_ANSWER = 5
DEFAULT_TEMPERATURE = 0.1
DEFAULT_MAX_TOKENS = 1000
DEFAULT_TOP_K = 10


def load_evaluation_config() -> Dict[str, Any]:
    """í‰ê°€ ì„¤ì • íŒŒì¼ ë¡œë“œ"""
    config_path = Path(__file__).parent.parent / "config" / "evaluation_config.yaml"
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)




def evaluate_single_query(
    retriever,
    retriever_config: Dict[str, Any],
    report_type: str,
    item: Dict[str, Any],
    langfuse,
    idx: int,
    top_k: int,
    system_prompt: str,
    answer_generation_prompt: str,
    version_tag: str = "v1"
) -> Dict[str, Any]:
    """ë‹¨ì¼ ì¿¼ë¦¬ í‰ê°€"""
    question = item["question"]
    ground_truth = item["ground_truth"]
    context_page_id = item.get("context_page_id")
    item_metadata = item.get("metadata", {})

    start_time = time.time()

    # ê²€ìƒ‰ ìˆ˜í–‰
    search_results = retriever.invoke(question)

    # LangChain Documentë¥¼ contextsë¡œ ë³€í™˜
    contexts = []
    context_metadata = []

    for result in search_results[:top_k]:
        contexts.append(result.page_content)
        context_metadata.append({
            "page_title": result.metadata.get('page_title', 'Unknown'),
            "section_title": result.metadata.get('section_title', 'N/A'),
            "chunk_id": result.metadata.get('chunk_id', 'unknown'),
            "score": result.metadata.get('_combined_score') or result.metadata.get('_similarity_score')
        })

    if not contexts:
        print(f"  âš ï¸ [{idx}] No contexts found for question!")
        contexts = ["ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."]

    # LLM ë‹µë³€ ìƒì„±
    answer = generate_llm_answer(
        question=question,
        contexts=contexts,
        system_prompt=system_prompt,
        answer_generation_prompt=answer_generation_prompt,
        num_contexts=DEFAULT_NUM_CONTEXTS_FOR_ANSWER,
        temperature=DEFAULT_TEMPERATURE,
        max_tokens=DEFAULT_MAX_TOKENS
    )

    if not answer or answer.startswith("ë‹µë³€ ìƒì„± ì‹¤íŒ¨") or answer.startswith("Azure OpenAI ì„¤ì •"):
        print(f"  âš ï¸ [{idx}] LLM answer generation failed!")
        if not answer:
            answer = "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    total_time = time.time() - start_time

    # Langfuse Trace & Generation - common_utils í•¨ìˆ˜ ì‚¬ìš©
    retriever_name = retriever_config['name']
    retriever_tags = [
        retriever_config['embedding_preset'],
        retriever_config['retriever_type'],
        f"top_k_{top_k}",
        report_type
    ]

    additional_metadata = {
        "context_page_id": context_page_id,
        "retriever_name": retriever_name,
        "display_name": retriever_config['display_name'],
        "report_type": report_type,
        "top_k": top_k,
        "embedding_preset": retriever_config['embedding_preset'],
        "retriever_type": retriever_config['retriever_type']
    }

    from utils.common_utils import create_trace_and_generation as create_trace
    trace_id = create_trace(
        langfuse=langfuse,
        retriever_name=retriever_name,
        question=question,
        contexts=contexts,
        answer=answer,
        ground_truth=ground_truth,
        context_metadata=context_metadata,
        item_metadata=item_metadata,
        total_time=total_time,
        idx=idx,
        version_tag=version_tag,
        retriever_tags=retriever_tags,
        additional_metadata=additional_metadata
    )

    # ê²€ìƒ‰ í’ˆì§ˆ ìŠ¤ì½”ì–´ ì¶”ê°€
    add_retrieval_quality_score(langfuse, trace_id, context_metadata)

    print(f"  [{idx}] {question[:50]}... ({len(contexts)}ê°œ ë¬¸ì„œ, {total_time*1000:.0f}ms)")

    return {
        "question": question,
        "answer": answer,
        "ground_truth": ground_truth,
        "num_contexts": len(contexts),
        "time": total_time,
        "trace_id": trace_id
    }


def evaluate_retriever(
    retriever,
    retriever_config: Dict[str, Any],
    report_type: str,
    eval_data: List[Dict[str, Any]],
    langfuse,
    top_k: int,
    system_prompt: str,
    answer_generation_prompt: str,
    version_tag: str = "v1"
) -> Dict[str, Any]:
    """ë¦¬íŠ¸ë¦¬ë²„ í‰ê°€"""
    print(f"\n{'=' * 80}")
    print(f"ğŸ” {retriever_config['display_name']} - {report_type} í‰ê°€ ì¤‘...")
    print(f"{'=' * 80}")

    stats = {
        "total_queries": len(eval_data),
        "total_time": 0,
        "evaluations": []
    }

    for idx, item in enumerate(eval_data, 1):
        eval_result = evaluate_single_query(
            retriever=retriever,
            retriever_config=retriever_config,
            report_type=report_type,
            item=item,
            langfuse=langfuse,
            idx=idx,
            top_k=top_k,
            system_prompt=system_prompt,
            answer_generation_prompt=answer_generation_prompt,
            version_tag=version_tag
        )

        stats["evaluations"].append(eval_result)
        stats["total_time"] += eval_result["time"]

        # ìºì‹œ ì €ì¥
        save_embedding_cache()

    stats["avg_time"] = stats["total_time"] / stats["total_queries"]
    stats["avg_contexts"] = sum(e["num_contexts"] for e in stats["evaluations"]) / stats["total_queries"]

    return stats


def run_report_evaluation(
    report_type: str,
    config: Dict[str, Any],
    dataset_path: str,
    top_k: int,
    version: str,
    langfuse,
    selected_retrievers: List[str] = None
) -> Dict[str, Any]:
    """íŠ¹ì • ë³´ê³ ì„œ íƒ€ì…ì— ëŒ€í•œ í‰ê°€ ì‹¤í–‰

    Args:
        report_type: 'weekly_report' or 'executive_report'
        config: ì „ì²´ ì„¤ì • ë”•ì…”ë„ˆë¦¬
        dataset_path: í‰ê°€ ë°ì´í„°ì…‹ ê²½ë¡œ
        top_k: Top-K ê°’
        version: ë²„ì „ íƒœê·¸
        langfuse: Langfuse í´ë¼ì´ì–¸íŠ¸
        selected_retrievers: í‰ê°€í•  ë¦¬íŠ¸ë¦¬ë²„ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ëª¨ë‘)

    Returns:
        í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    report_config = config[report_type]

    print("\n" + "=" * 80)
    print(f"ğŸ“Š {report_config['name']} í‰ê°€ ì‹œì‘")
    print("=" * 80)
    print(f"ìš°ì„ ìˆœìœ„: {' > '.join(report_config['priority'])}")

    # í”„ë¡¬í”„íŠ¸ ë¡œë“œ
    system_prompt = load_prompt(report_config['system_prompt_path'])
    answer_generation_prompt = load_prompt(report_config['answer_generation_prompt_path'])

    # ë°ì´í„°ì…‹ ë¡œë“œ
    eval_data = load_evaluation_dataset(dataset_path)
    print(f"ğŸ“‹ ë°ì´í„°ì…‹: {len(eval_data)} ê°œ ìƒ˜í”Œ")

    # í‰ê°€í•  ë¦¬íŠ¸ë¦¬ë²„ í•„í„°ë§
    retrievers_to_eval = report_config['retrievers']
    if selected_retrievers:
        retrievers_to_eval = [
            r for r in retrievers_to_eval
            if r['name'] in selected_retrievers
        ]

    print(f"ğŸ” í‰ê°€ ëŒ€ìƒ: {len(retrievers_to_eval)}ê°œ ë¦¬íŠ¸ë¦¬ë²„\n")

    results = {}

    for retriever_config in retrievers_to_eval:
        # Top-K ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ ì»¤ë§¨ë“œë¼ì¸ top_k ì‚¬ìš©)
        top_k_list = retriever_config.get('top_k_list', [top_k])

        for current_top_k in top_k_list:
            print(f"\n{'=' * 80}")
            print(f"ğŸš€ {retriever_config['display_name']}")
            print(f"   ì„ë² ë”©: {retriever_config['embedding_preset']}")
            print(f"   ë¦¬íŠ¸ë¦¬ë²„: {retriever_config['retriever_type']}")
            print(f"   Top-K: {current_top_k}")
            print(f"   {retriever_config['description']}")
            print(f"{'=' * 80}")

            try:
                # ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±
                retriever, retriever_tags = create_retriever_from_config(retriever_config, current_top_k)

                # í‰ê°€ ìˆ˜í–‰
                stats = evaluate_retriever(
                    retriever=retriever,
                    retriever_config=retriever_config,
                    report_type=report_type,
                    eval_data=eval_data,
                    langfuse=langfuse,
                    top_k=current_top_k,
                    system_prompt=system_prompt,
                    answer_generation_prompt=answer_generation_prompt,
                    version_tag=version
                )

                # ê²°ê³¼ ì €ì¥
                output_dir = Path(config['evaluation']['output_dir']) / report_type
                output_dir.mkdir(parents=True, exist_ok=True)

                # íŒŒì¼ëª…ì— top-k í¬í•¨
                output_file = output_dir / f"{retriever_config['name']}_k{current_top_k}_stats.json"
                save_result = {k: v for k, v in stats.items() if k != "evaluations"}
                save_result["num_evaluations"] = len(stats.get("evaluations", []))
                save_result["config"] = {
                    "retriever_name": retriever_config['name'],
                    "display_name": retriever_config['display_name'],
                    "report_type": report_type,
                    "embedding_preset": retriever_config['embedding_preset'],
                    "retriever_type": retriever_config['retriever_type'],
                    "top_k": current_top_k,
                    "version": version,
                }

                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(save_result, f, indent=2, ensure_ascii=False, default=str)

                print(f"\nâœ… í‰ê°€ ì™„ë£Œ (Top-K={current_top_k})")
                print(f"   - í‰ê·  ì‹œê°„: {stats['avg_time']*1000:.2f}ms")
                print(f"   - í‰ê·  ì»¨í…ìŠ¤íŠ¸ ìˆ˜: {stats['avg_contexts']:.2f}")
                print(f"   - ê²°ê³¼ ì €ì¥: {output_file}")

                result_key = f"{retriever_config['name']}_k{current_top_k}"
                results[result_key] = {
                    "success": True,
                    "stats": stats,
                    "output_file": str(output_file),
                    "top_k": current_top_k
                }

            except Exception as e:
                print(f"âŒ í‰ê°€ ì‹¤íŒ¨ (Top-K={current_top_k}): {e}")
                import traceback
                traceback.print_exc()

                result_key = f"{retriever_config['name']}_k{current_top_k}"
                results[result_key] = {
                    "success": False,
                    "error": str(e),
                    "top_k": current_top_k
                }

    return results


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse

    parser = argparse.ArgumentParser(
        description="ì£¼ê°„ ë³´ê³ ì„œ vs ì„ì› ë³´ê³ ì„œ í‰ê°€",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )

    parser.add_argument(
        "--report-type",
        type=str,
        choices=["weekly", "executive", "both"],
        default="both",
        help="í‰ê°€í•  ë³´ê³ ì„œ íƒ€ì… (ê¸°ë³¸ê°’: both)"
    )

    parser.add_argument(
        "--retrievers",
        type=str,
        nargs="+",
        help="í‰ê°€í•  ë¦¬íŠ¸ë¦¬ë²„ ì´ë¦„ (ë¯¸ì§€ì • ì‹œ ëª¨ë‘ í‰ê°€)"
    )

    parser.add_argument(
        "--dataset",
        type=str,
        default=DEFAULT_DATASET_PATH,
        help="í‰ê°€ ë°ì´í„°ì…‹ ê²½ë¡œ"
    )

    parser.add_argument(
        "--top-k",
        type=int,
        default=DEFAULT_TOP_K,
        help="Top-K ê°’ (ê¸°ë³¸ê°’: 10)"
    )

    parser.add_argument(
        "--version",
        type=str,
        default="v1",
        help="ë²„ì „ íƒœê·¸ (ê¸°ë³¸ê°’: v1)"
    )

    args = parser.parse_args()

    # ì„¤ì • ë¡œë“œ
    config = load_evaluation_config()

    print("=" * 80)
    print("ğŸ¯ ì£¼ê°„ ë³´ê³ ì„œ vs ì„ì› ë³´ê³ ì„œ í‰ê°€ ì‹œìŠ¤í…œ")
    print("=" * 80)
    print(f"\nğŸ“Š í‰ê°€ ì„¤ì •:")
    print(f"   - ë³´ê³ ì„œ íƒ€ì…: {args.report_type}")
    print(f"   - Dataset: {args.dataset}")
    print(f"   - Top-K: {args.top_k}")
    print(f"   - Version: {args.version}")
    if args.retrievers:
        print(f"   - ì„ íƒëœ ë¦¬íŠ¸ë¦¬ë²„: {', '.join(args.retrievers)}")

    # Langfuse í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    langfuse = get_langfuse_client()
    if not langfuse:
        print("âŒ Langfuse í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # í‰ê°€ ì‹¤í–‰
    all_results = {}

    if args.report_type in ["weekly", "both"]:
        weekly_results = run_report_evaluation(
            report_type="weekly_report",
            config=config,
            dataset_path=args.dataset,
            top_k=args.top_k,
            version=args.version,
            langfuse=langfuse,
            selected_retrievers=args.retrievers
        )
        all_results["weekly_report"] = weekly_results

    if args.report_type in ["executive", "both"]:
        exec_results = run_report_evaluation(
            report_type="executive_report",
            config=config,
            dataset_path=args.dataset,
            top_k=args.top_k,
            version=args.version,
            langfuse=langfuse,
            selected_retrievers=args.retrievers
        )
        all_results["executive_report"] = exec_results

    # Langfuse flush
    print("\nâ³ Langfuseì— ë°ì´í„° ì „ì†¡ ì¤‘...")
    langfuse.flush()

    # ì„ë² ë”© ìºì‹œ ì €ì¥
    print("\nğŸ’¾ ì„ë² ë”© ìºì‹œ ì €ì¥ ì¤‘...")
    save_embedding_cache()

    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 80)
    print("ğŸ“Š í‰ê°€ ê²°ê³¼ ìš”ì•½")
    print("=" * 80)

    for report_type, results in all_results.items():
        report_name = "ì£¼ê°„ ë³´ê³ ì„œ (ìš´ì˜íŒ€)" if report_type == "weekly_report" else "ì„ì› ë³´ê³ ì„œ (ì˜ì‚¬ê²°ì •)"
        print(f"\n[{report_name}]")

        for retriever_name, result in results.items():
            status = "âœ… ì„±ê³µ" if result['success'] else "âŒ ì‹¤íŒ¨"
            print(f"  {retriever_name}: {status}")

    print("\n" + "=" * 80)
    print("âœ… ëª¨ë“  í‰ê°€ ì™„ë£Œ!")
    print("=" * 80)

    print("\nğŸ“Š ë‹¤ìŒ ë‹¨ê³„:")
    print("   1. Langfuse ëŒ€ì‹œë³´ë“œì—ì„œ ê²°ê³¼ í™•ì¸: https://cloud.langfuse.com")
    print("   2. ê²°ê³¼ íŒŒì¼ í™•ì¸:")
    print(f"      - {config['evaluation']['output_dir']}/weekly_report/")
    print(f"      - {config['evaluation']['output_dir']}/executive_report/")
    print()


if __name__ == "__main__":
    main()
