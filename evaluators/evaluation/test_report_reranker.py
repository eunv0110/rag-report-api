#!/usr/bin/env python3
"""BGE-M3 + RRF Ensemble + Qwen3-Reranker-4B ë³´ê³ ì„œ ìë™ ìƒì„± ë° ë¹„êµ (Langfuse ì—°ë™)

BGE-M3 RRF Ensembleë¡œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•œ í›„, Qwen3-Reranker-4Bë¡œ ì¬ìˆœìœ„í™”í•˜ì—¬
ìµœì¢… kê°œ(6, 8, 10)ì˜ ë¬¸ì„œë¥¼ LLMì— ì „ë‹¬í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.

ë³´ê³ ì„œ íƒ€ì…:
- executive: ìµœì¢…ë³´ê³ ì„œ
- weekly: ì£¼ê°„ë³´ê³ ì„œ
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

from dotenv import load_dotenv
load_dotenv()

import json
import os
from datetime import datetime
from typing import List, Dict, Any
from langchain.chat_models import init_chat_model
from concurrent.futures import ThreadPoolExecutor, as_completed
from sentence_transformers import CrossEncoder

from config.settings import AZURE_AI_CREDENTIAL, AZURE_AI_ENDPOINT, EVALUATION_CONFIG
from utils.langfuse_utils import get_langfuse_client
from utils.date_utils import parse_date_range, extract_date_filter_from_question
from utils.common_utils import (
    load_evaluation_dataset,
    create_trace_and_generation,
    add_retrieval_quality_score,
    save_embedding_cache
)

# ë¦¬íŠ¸ë¦¬ë²„ ì„í¬íŠ¸
from retrievers.ensemble_retriever import get_ensemble_retriever


# Qwen3 Reranker ì´ˆê¸°í™” (ì „ì—­ ë³€ìˆ˜ë¡œ í•œ ë²ˆë§Œ ë¡œë“œ)
RERANKER_MODEL = None

def format_queries(query: str, instruction: str = None) -> str:
    """Qwen3-Rerankerë¥¼ ìœ„í•œ ì¿¼ë¦¬ í¬ë§·íŒ…"""
    prefix = '<|im_start|>system\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be "yes" or "no".<|im_end|>\n<|im_start|>user\n'
    if instruction is None:
        instruction = "Given a query, retrieve relevant passages that answer the query"
    return f"{prefix}<Instruct>: {instruction}\n<Query>: {query}\n"


def format_document(document: str) -> str:
    """Qwen3-Rerankerë¥¼ ìœ„í•œ ë¬¸ì„œ í¬ë§·íŒ…"""
    suffix = "<|im_end|>\n<|im_start|>assistant\n<think>\n\n</think>\n\n"
    return f"<Document>: {document}{suffix}"


def get_reranker():
    """Qwen3-Reranker-4B-seq-cls ëª¨ë¸ ë¡œë“œ (ìºì‹±)"""
    global RERANKER_MODEL
    if RERANKER_MODEL is None:
        import torch
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"ğŸ”„ Qwen3-Reranker-4B-seq-cls ëª¨ë¸ ë¡œë”© ì¤‘... (device: {device})")
        RERANKER_MODEL = CrossEncoder(
            "tomaarsen/Qwen3-Reranker-4B-seq-cls",
            max_length=8192,
            device=device,
            trust_remote_code=True
        )
        print("âœ… Qwen3-Reranker-4B-seq-cls ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
    return RERANKER_MODEL


def rerank_documents(query: str, docs: list, top_k: int = 6) -> list:
    """Qwen3-Reranker-4Bë¡œ ë¬¸ì„œ ì¬ìˆœìœ„í™”

    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬
        docs: ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (langchain Document ê°ì²´)
        top_k: ìµœì¢… ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜

    Returns:
        ì¬ìˆœìœ„í™”ëœ ìƒìœ„ kê°œ ë¬¸ì„œ
    """
    reranker = get_reranker()

    # Qwen3-Reranker í¬ë§·ì— ë§ê²Œ ì¿¼ë¦¬ì™€ ë¬¸ì„œ ë³€í™˜
    formatted_query = format_queries(query)
    pairs = [
        [formatted_query, format_document(doc.page_content)]
        for doc in docs
    ]

    # ì¬ìˆœìœ„í™” ì ìˆ˜ ê³„ì‚°
    scores = reranker.predict(pairs)

    # ì ìˆ˜ì™€ ë¬¸ì„œë¥¼ í•¨ê»˜ ì •ë ¬
    doc_score_pairs = list(zip(docs, scores))
    doc_score_pairs.sort(key=lambda x: x[1], reverse=True)

    # ìƒìœ„ kê°œ ë¬¸ì„œë§Œ ë°˜í™˜
    reranked_docs = [doc for doc, score in doc_score_pairs[:top_k]]

    print(f"\nğŸ”„ Reranking ì™„ë£Œ: {len(docs)}ê°œ â†’ {len(reranked_docs)}ê°œ")
    print("Top 3 Reranked Scores:")
    for i, (doc, score) in enumerate(doc_score_pairs[:3], 1):
        print(f"  {i}. {doc.metadata.get('page_title', 'Unknown')}: {score:.4f}")

    return reranked_docs


def get_report_config(report_type: str) -> Dict[str, Any]:
    """ë³´ê³ ì„œ íƒ€ì…ì— ë”°ë¥¸ ì„¤ì • ë°˜í™˜"""
    if report_type not in ['executive', 'weekly']:
        raise ValueError(f"Invalid report_type: {report_type}. Must be 'executive' or 'weekly'")

    return {
        'test_questions': EVALUATION_CONFIG['test_questions'][f'{report_type}_report'],
        'retriever_configs': EVALUATION_CONFIG['simple_test_retrievers'][f'{report_type}_report'],
        'default_retriever_index': EVALUATION_CONFIG['default_retriever_index'][f'{report_type}_report'],
        'llm_configs': EVALUATION_CONFIG['test_llms'],
        'prompt_dir': f'{report_type}_report',
        'qdrant_locks': {
            'executive': [
                "/home/work/rag/Project/rag-report-generator/data/qdrant_data/gemini-embedding-001/.lock",
                "/home/work/rag/Project/rag-report-generator/data/qdrant_data/baai-bge-m3/.lock",
                "/home/work/rag/Project/rag-report-generator/data/qdrant_data/openai-large/.lock"
            ],
            'weekly': [
                "/home/work/rag/Project/rag-report-generator/data/qdrant_data/baai-bge-m3/.lock",
                "/home/work/rag/Project/rag-report-generator/data/qdrant_data/upstage-solar-embedding/.lock",
                "/home/work/rag/Project/rag-report-generator/data/qdrant_data/openai-large/.lock"
            ]
        }[report_type]
    }


def load_prompt(prompt_file: str, report_type: str) -> str:
    """í”„ë¡¬í”„íŠ¸ íŒŒì¼ ë¡œë“œ"""
    # evaluators/evaluation/test_report_reranker.pyì—ì„œ í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¡œ ì´ë™
    project_root = Path(__file__).parent.parent.parent
    prompt_path = project_root / "prompts" / "templates" / "evaluation" / f"{report_type}_report" / prompt_file
    with open(prompt_path, 'r', encoding='utf-8') as f:
        return f.read()


def load_test_questions(n: int = 5) -> List[Dict[str, Any]]:
    """í…ŒìŠ¤íŠ¸ìš© ì§ˆë¬¸ ë¡œë“œ (ë ˆê±°ì‹œ, interactive ëª¨ë“œìš©)"""
    qa_file = Path(__file__).parent.parent / "data" / "evaluation" / "llm_generated_qa_v2.json"

    with open(qa_file, 'r', encoding='utf-8') as f:
        qa_data = json.load(f)

    return qa_data[:n]


def generate_answer_with_llm(query: str, docs: list, llm_config: dict, report_type: str, langfuse=None, question_id: int = None) -> str:
    """ì§€ì •ëœ LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±"""
    from openai import OpenAI
    from config.settings import OPENROUTER_API_KEY, OPENROUTER_BASE_URL

    # LLMë³„ ì»¨í…ìŠ¤íŠ¸ ì œí•œ ì²˜ë¦¬
    if 'max_docs' in llm_config and llm_config['max_docs']:
        max_docs = llm_config['max_docs']
        if len(docs) > max_docs:
            docs = docs[:max_docs]
            print(f"  âš ï¸ {llm_config['display_name']} ì»¨í…ìŠ¤íŠ¸ ì œí•œìœ¼ë¡œ ë¬¸ì„œ ìˆ˜ë¥¼ {len(docs)}ê°œë¡œ ì¤„ì˜€ìŠµë‹ˆë‹¤.")

    # Context êµ¬ì„±
    context_parts = []
    for i, doc in enumerate(docs, 1):
        title = doc.metadata.get('page_title', 'Unknown')
        content = doc.page_content
        context_parts.append(f"[ë¬¸ì„œ {i}] {title}\n{content}\n")

    context_text = "\n".join(context_parts)

    # í”„ë¡¬í”„íŠ¸ ë¡œë“œ
    system_prompt = load_prompt("system_prompt.txt", report_type)
    answer_generation_template = load_prompt("answer_generation_prompt.txt", report_type)

    # í…œí”Œë¦¿ì— ë³€ìˆ˜ ëŒ€ì…
    user_prompt = answer_generation_template.replace("{context}", context_text).replace("{question}", query)

    # Azure AI ì„¤ì •
    os.environ['AZURE_AI_CREDENTIAL'] = AZURE_AI_CREDENTIAL
    os.environ['AZURE_AI_ENDPOINT'] = AZURE_AI_ENDPOINT

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]

    # LLM íƒ€ì…ì— ë”°ë¼ ë‹¤ë¥´ê²Œ í˜¸ì¶œ
    if llm_config['model_id'].startswith('anthropic:'):
        # Anthropic Claude ëª¨ë¸ - OpenRouter ì‚¬ìš©
        model_name = llm_config['model_id'].split(':')[1]
        client = OpenAI(
            api_key=OPENROUTER_API_KEY,
            base_url=OPENROUTER_BASE_URL
        )

        # Langfuseë¡œ ë‹µë³€ ìƒì„± ê¸°ë¡
        if langfuse and question_id:
            with langfuse.start_as_current_observation(
                as_type='generation',
                name=f"generation_{llm_config['name']}_q{question_id}",
                model=llm_config['model_id'],
                input={"question": query, "context": context_text[:500] + "..." if len(context_text) > 500 else context_text},
                metadata={"llm": llm_config['name'], "num_docs": len(docs)}
            ) as generation:
                response = client.chat.completions.create(
                    model=f"anthropic/{model_name}",
                    messages=messages,
                    max_tokens=1000,
                    temperature=0
                )
                answer = response.choices[0].message.content

                # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œ
                usage_dict = None
                if hasattr(response, 'usage') and response.usage:
                    usage_dict = {
                        "input": response.usage.prompt_tokens,
                        "output": response.usage.completion_tokens,
                        "total": response.usage.total_tokens
                    }

                generation.update(
                    output={"answer": answer},
                    usage=usage_dict if usage_dict else None
                )

                langfuse.update_current_trace(
                    name=f"llm_comparison_{llm_config['name']}_q{question_id}",
                    tags=[llm_config['name'], "reranker_test", f"{report_type}_report"],
                    input={"question": query},
                    output={"answer": answer},
                    metadata={
                        "llm_name": llm_config['name'],
                        "llm_display_name": llm_config['display_name'],
                        "llm_description": llm_config['description'],
                        "model_id": llm_config['model_id'],
                        "question_id": question_id,
                        "num_docs": len(docs),
                        "report_type": report_type
                    }
                )
        else:
            response = client.chat.completions.create(
                model=f"anthropic/{model_name}",
                messages=messages,
                max_tokens=1000,
                temperature=0
            )
            answer = response.choices[0].message.content
    else:
        # Azure AI ëª¨ë¸ (LangChain)
        model = init_chat_model(
            llm_config['model_id'],
            temperature=0,
            max_completion_tokens=1000
        )

        # Langfuseë¡œ ë‹µë³€ ìƒì„± ê¸°ë¡
        if langfuse and question_id:
            with langfuse.start_as_current_observation(
                as_type='generation',
                name=f"generation_{llm_config['name']}_q{question_id}",
                model=llm_config['model_id'],
                input={"question": query, "context": context_text[:500] + "..." if len(context_text) > 500 else context_text},
                metadata={"llm": llm_config['name'], "num_docs": len(docs)}
            ) as generation:
                response = model.invoke(messages)
                answer = response.content

                # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œ
                usage_dict = None
                if hasattr(response, 'usage_metadata') and response.usage_metadata:
                    usage_dict = {
                        "input": response.usage_metadata.get('input_tokens', 0),
                        "output": response.usage_metadata.get('output_tokens', 0),
                        "total": response.usage_metadata.get('total_tokens', 0)
                    }
                elif hasattr(response, 'response_metadata') and response.response_metadata:
                    token_usage = response.response_metadata.get('token_usage', {})
                    if token_usage:
                        usage_dict = {
                            "input": token_usage.get('prompt_tokens', 0),
                            "output": token_usage.get('completion_tokens', 0),
                            "total": token_usage.get('total_tokens', 0)
                        }

                generation.update(
                    output={"answer": answer},
                    usage=usage_dict if usage_dict else None
                )

                langfuse.update_current_trace(
                    name=f"llm_comparison_{llm_config['name']}_q{question_id}",
                    tags=[llm_config['name'], "reranker_test", f"{report_type}_report"],
                    input={"question": query},
                    output={"answer": answer},
                    metadata={
                        "llm_name": llm_config['name'],
                        "llm_display_name": llm_config['display_name'],
                        "llm_description": llm_config['description'],
                        "model_id": llm_config['model_id'],
                        "question_id": question_id,
                        "num_docs": len(docs),
                        "report_type": report_type
                    }
                )
        else:
            response = model.invoke(messages)
            answer = response.content

    return answer


def retrieve_and_rerank_documents(
    question: str,
    qdrant_locks: List[str],
    top_k: int = 6,
    date_filter: tuple = None,
    langfuse=None,
    question_id: int = None
):
    """BGE-M3 RRF Ensembleë¡œ ê²€ìƒ‰ í›„ Qwen3-Rerankerë¡œ ì¬ìˆœìœ„í™”"""
    import time
    import gc

    # Qdrant ë½ íŒŒì¼ ì •ë¦¬
    for lock_file in qdrant_locks:
        if os.path.exists(lock_file):
            try:
                os.remove(lock_file)
            except:
                pass

    gc.collect()
    time.sleep(0.5)

    # BGE-M3 ì„ë² ë”© ì„¤ì •
    os.environ["MODEL_PRESET"] = "bge-m3"
    os.environ["USE_EMBEDDING_CACHE"] = "true"

    # RRF Ensemble ë¦¬íŠ¸ë¦¬ë²„ ìƒì„± (ë” ë§ì€ í›„ë³´ ê²€ìƒ‰)
    initial_k = max(20, top_k * 3)  # ì¬ìˆœìœ„í™”ë¥¼ ìœ„í•´ ë” ë§ì€ ë¬¸ì„œ ê²€ìƒ‰
    retriever = get_ensemble_retriever(
        k=initial_k,
        bm25_weight=0.5,
        dense_weight=0.5,
        date_filter=date_filter
    )

    print(f"ğŸ” BGE-M3 RRF Ensemble ê²€ìƒ‰ ì¤‘ (ì´ˆê¸° k={initial_k})...")

    # Langfuseë¡œ ê²€ìƒ‰ ê³¼ì • ê¸°ë¡
    if langfuse and question_id:
        with langfuse.start_as_current_observation(
            as_type='span',
            name=f"retrieval_bge-m3-rrf_q{question_id}",
            input={"question": question},
            metadata={
                "retriever_name": "bge-m3-rrf-reranker",
                "initial_k": initial_k,
                "final_k": top_k,
                "reranker": "Qwen3-Reranker-4B"
            }
        ) as span:
            # ë¬¸ì„œ ê²€ìƒ‰
            initial_docs = retriever.invoke(question)
            print(f"ğŸ“„ ì´ˆê¸° ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜: {len(initial_docs)}")

            # Reranking
            reranked_docs = rerank_documents(question, initial_docs, top_k=top_k)

            span.update(output={
                "initial_num_docs": len(initial_docs),
                "final_num_docs": len(reranked_docs),
                "doc_titles": [doc.metadata.get('page_title', 'Unknown') for doc in reranked_docs]
            })
    else:
        initial_docs = retriever.invoke(question)
        print(f"ğŸ“„ ì´ˆê¸° ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜: {len(initial_docs)}")
        reranked_docs = rerank_documents(question, initial_docs, top_k=top_k)

    print(f"\nğŸ“„ ìµœì¢… ë¬¸ì„œ ìˆ˜: {len(reranked_docs)}")
    print("\nìµœì¢… ë¬¸ì„œ ì œëª©:")
    for i, doc in enumerate(reranked_docs, 1):
        print(f"  {i}. {doc.metadata.get('page_title', 'Unknown')}")

    return reranked_docs


def test_single_llm(
    llm_config: Dict[str, Any],
    question: str,
    docs: list,
    report_type: str,
    langfuse=None,
    question_id: int = None
) -> Dict[str, Any]:
    """ë‹¨ì¼ LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±"""

    print(f"\n{'=' * 100}")
    print(f"ğŸ’¬ {llm_config['display_name']}")
    print(f"   {llm_config['description']}")
    print(f"{'=' * 100}")

    try:
        print("ğŸ’¬ ë‹µë³€ ìƒì„± ì¤‘...")
        answer = generate_answer_with_llm(
            question,
            docs,
            llm_config,
            report_type,
            langfuse,
            question_id
        )

        print(f"\nâœ… ìƒì„±ëœ ë‹µë³€:\n{answer}\n")

        return {
            "success": True,
            "llm_name": llm_config['name'],
            "llm_display_name": llm_config['display_name'],
            "model_id": llm_config['model_id'],
            "answer": answer
        }

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}\n")
        import traceback
        traceback.print_exc()
        return {
            "success": False,
            "llm_name": llm_config['name'],
            "llm_display_name": llm_config['display_name'],
            "error": str(e)
        }


def save_combination_results(
    all_results: List[Dict],
    top_k: int,
    llm_configs: List[Dict],
    timestamp: str,
    combination_base_dir: Path,
    questions: List[str],
    report_type: str
):
    """ì¤‘ê°„ ê²°ê³¼ë¥¼ ì¡°í•©ë³„ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜"""

    all_combinations_data = []
    all_answers_text = []

    for llm_config in llm_configs:
        llm_name = llm_config['name']

        # Retriever + LLM ì¡°í•© í´ë”ëª… ìƒì„±
        combination_name = f"bge-m3-rrf-reranker-k{top_k}_{llm_name}"
        combination_dir = combination_base_dir / combination_name
        combination_dir.mkdir(parents=True, exist_ok=True)

        # í•´ë‹¹ LLMì˜ ê²°ê³¼ë§Œ í•„í„°ë§
        llm_results = []
        for q_result in all_results:
            for llm_result in q_result['llms']:
                if llm_result['llm_name'] == llm_name:
                    llm_results.append({
                        "question_id": q_result['question_id'],
                        "question": q_result['question'],
                        "date_filter": q_result['date_filter'],
                        "num_docs": q_result['num_docs'],
                        "doc_titles": q_result['doc_titles'],
                        "result": llm_result
                    })

        # Retriever + LLM ì¡°í•©ë³„ JSON ì €ì¥
        combination_output = {
            "combination_name": combination_name,
            "retriever_name": f"bge-m3-rrf-reranker-k{top_k}",
            "retriever_display_name": f"BGE-M3 + RRF + Qwen3-Reranker (Top {top_k})",
            "retriever_description": f"BGE-M3 embedding with RRF ensemble and Qwen3-Reranker-4B reranking (final k={top_k})",
            "llm_name": llm_name,
            "llm_display_name": llm_config['display_name'],
            "llm_description": llm_config['description'],
            "num_questions": len(llm_results),
            "results": llm_results
        }

        all_combinations_data.append(combination_output)

        combination_file = combination_dir / "results.json"
        with open(combination_file, 'w', encoding='utf-8') as f:
            json.dump(combination_output, f, ensure_ascii=False, indent=2)

        # ë‹µë³€ë§Œ ë”°ë¡œ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥
        answer_file = combination_dir / "answers.txt"
        answer_text_parts = []
        answer_text_parts.append(f"=" * 100)
        answer_text_parts.append(f"BGE-M3 + RRF + Qwen3-Reranker (Top {top_k}) + {llm_config['display_name']} - ë‹µë³€ ëª¨ìŒ")
        answer_text_parts.append(f"=" * 100)
        answer_text_parts.append("")

        for i, result in enumerate(llm_results, 1):
            answer_text_parts.append(f"{'=' * 100}")
            answer_text_parts.append(f"ì§ˆë¬¸ {i}/{len(llm_results)}")
            answer_text_parts.append(f"{'=' * 100}")
            answer_text_parts.append(f"â“ {result['question']}")
            answer_text_parts.append("")

            if result['result'].get('success'):
                answer_text_parts.append(f"âœ… ë‹µë³€:")
                answer_text_parts.append(f"{result['result'].get('answer', 'N/A')}")
                answer_text_parts.append("")
            else:
                answer_text_parts.append(f"âŒ ì˜¤ë¥˜ ë°œìƒ:")
                answer_text_parts.append(f"{result['result'].get('error', 'Unknown error')}")
                answer_text_parts.append("")

        answer_text = "\n".join(answer_text_parts)
        with open(answer_file, 'w', encoding='utf-8') as f:
            f.write(answer_text)

        all_answers_text.append(answer_text)
        all_answers_text.append("\n\n")

    # ì „ì²´ ì¡°í•© ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ JSON íŒŒì¼ë¡œ ì €ì¥
    all_combinations_file = combination_base_dir / "all_combinations.json"
    all_combinations_output = {
        "test_date": datetime.now().isoformat(),
        "report_type": report_type,
        "retriever_name": f"bge-m3-rrf-reranker-k{top_k}",
        "retriever_display_name": f"BGE-M3 + RRF + Qwen3-Reranker (Top {top_k})",
        "num_llms": len(llm_configs),
        "num_questions": len(questions),
        "num_completed_questions": len(all_results),
        "total_combinations": len(all_combinations_data),
        "combinations": all_combinations_data
    }

    with open(all_combinations_file, 'w', encoding='utf-8') as f:
        json.dump(all_combinations_output, f, ensure_ascii=False, indent=2)

    # ì „ì²´ ë‹µë³€ì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥
    all_answers_file = combination_base_dir / "all_answers.txt"
    with open(all_answers_file, 'w', encoding='utf-8') as f:
        f.write("\n".join(all_answers_text))

    print(f"âœ… ì¤‘ê°„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {combination_base_dir}")


def evaluate_single_query_with_reranker(
    question: str,
    ground_truth: str,
    context_page_id: str,
    item_metadata: Dict[str, Any],
    report_type: str,
    top_k: int,
    qdrant_locks: List[str],
    system_prompt: str,
    answer_generation_prompt: str,
    langfuse,
    idx: int,
    version_tag: str = "v1"
) -> Dict[str, Any]:
    """Rerankerë¥¼ ì‚¬ìš©í•œ ë‹¨ì¼ ì¿¼ë¦¬ í‰ê°€ (evaluate_report_types.py ìŠ¤íƒ€ì¼)

    Args:
        question: ì§ˆë¬¸
        ground_truth: ì •ë‹µ
        context_page_id: ì •ë‹µ ë¬¸ì„œì˜ page_id
        item_metadata: ì§ˆë¬¸ ë©”íƒ€ë°ì´í„°
        report_type: ë³´ê³ ì„œ íƒ€ì…
        top_k: Top-K ê°’
        qdrant_locks: Qdrant ë½ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        answer_generation_prompt: ë‹µë³€ ìƒì„± í”„ë¡¬í”„íŠ¸
        langfuse: Langfuse í´ë¼ì´ì–¸íŠ¸
        idx: ì§ˆë¬¸ ì¸ë±ìŠ¤
        version_tag: ë²„ì „ íƒœê·¸

    Returns:
        í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    import time

    start_time = time.time()

    # ë¬¸ì„œ ê²€ìƒ‰ ë° ì¬ìˆœìœ„í™”
    docs = retrieve_and_rerank_documents(
        question=question,
        qdrant_locks=qdrant_locks,
        top_k=top_k,
        date_filter=None,  # í‰ê°€ ë°ì´í„°ì…‹ì—ëŠ” ë‚ ì§œ í•„í„° ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
        langfuse=langfuse,
        question_id=idx
    )

    # ì»¨í…ìŠ¤íŠ¸ í…ìŠ¤íŠ¸ ë³€í™˜
    contexts = []
    context_metadata = []

    for result in docs:
        contexts.append(result.page_content)
        context_metadata.append({
            "page_title": result.metadata.get('page_title', 'Unknown'),
            "section_title": result.metadata.get('section_title', 'N/A'),
            "chunk_id": result.metadata.get('chunk_id', 'unknown'),
            "score": result.metadata.get('_combined_score') or result.metadata.get('_similarity_score')
        })

    if not contexts:
        print(f"  âš ï¸ [{idx}] No contexts found for question!")
        contexts = ["ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."]

    # LLM ë‹µë³€ ìƒì„± (common_utilsì˜ generate_llm_answer ì‚¬ìš©)
    from utils.common_utils import generate_llm_answer
    answer = generate_llm_answer(
        question=question,
        contexts=contexts,
        system_prompt=system_prompt,
        answer_generation_prompt=answer_generation_prompt,
        num_contexts=min(5, len(contexts)),
        temperature=0.1,
        max_tokens=1000
    )

    if not answer or answer.startswith("ë‹µë³€ ìƒì„± ì‹¤íŒ¨") or answer.startswith("Azure OpenAI ì„¤ì •"):
        print(f"  âš ï¸ [{idx}] LLM answer generation failed!")
        if not answer:
            answer = "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    total_time = time.time() - start_time

    # Langfuse Trace & Generation ìƒì„±
    retriever_name = f"bge-m3-rrf-reranker-k{top_k}"
    retriever_tags = [
        "bge-m3",
        "rrf_ensemble",
        "qwen3-reranker",
        f"top_k_{top_k}",
        report_type
    ]

    additional_metadata = {
        "context_page_id": context_page_id,
        "retriever_name": retriever_name,
        "display_name": f"BGE-M3 + RRF + Qwen3-Reranker (Top {top_k})",
        "report_type": report_type,
        "top_k": top_k,
        "embedding_preset": "bge-m3",
        "retriever_type": "rrf_ensemble_reranker",
        "reranker_model": "Qwen3-Reranker-4B"
    }

    trace_id = create_trace_and_generation(
        langfuse=langfuse,
        retriever_name=retriever_name,
        question=question,
        contexts=contexts,
        answer=answer,
        ground_truth=ground_truth,
        context_metadata=context_metadata,
        item_metadata=item_metadata,
        total_time=total_time,
        idx=idx,
        version_tag=version_tag,
        retriever_tags=retriever_tags,
        additional_metadata=additional_metadata
    )

    # ê²€ìƒ‰ í’ˆì§ˆ ìŠ¤ì½”ì–´ ì¶”ê°€
    add_retrieval_quality_score(langfuse, trace_id, context_metadata)

    print(f"  [{idx}] {question[:50]}... ({len(contexts)}ê°œ ë¬¸ì„œ, {total_time*1000:.0f}ms)")

    return {
        "question": question,
        "answer": answer,
        "ground_truth": ground_truth,
        "num_contexts": len(contexts),
        "time": total_time,
        "trace_id": trace_id,
        "context_metadata": context_metadata
    }


def run_reranker_evaluation(
    report_type: str,
    dataset_path: str,
    top_k: int = 6,
    version: str = "v1",
    output_dir: str = None
) -> Dict[str, Any]:
    """Rerankerë¥¼ ì‚¬ìš©í•œ í‰ê°€ ì‹¤í–‰ (evaluate_report_types.py ìŠ¤íƒ€ì¼)

    Args:
        report_type: ë³´ê³ ì„œ íƒ€ì… ('executive' ë˜ëŠ” 'weekly')
        dataset_path: í‰ê°€ ë°ì´í„°ì…‹ ê²½ë¡œ
        top_k: Top-K ê°’
        version: ë²„ì „ íƒœê·¸
        output_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬

    Returns:
        í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    import yaml

    # ì„¤ì • ë¡œë“œ
    config = get_report_config(report_type)

    # í”„ë¡¬í”„íŠ¸ ë¡œë“œ (common_utils ì‚¬ìš©)
    from utils.common_utils import load_prompt as load_prompt_util

    report_type_suffix = "weekly_report" if report_type == "weekly" else "executive_report"
    system_prompt = load_prompt_util(f"prompts/templates/service/{report_type_suffix}/system_prompt.txt")
    answer_generation_prompt = load_prompt_util(f"prompts/templates/service/{report_type_suffix}/answer_generation_prompt.txt")

    # í‰ê°€ ë°ì´í„°ì…‹ ë¡œë“œ
    eval_data = load_evaluation_dataset(dataset_path)

    # Langfuse í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    langfuse = get_langfuse_client()
    if not langfuse:
        print("âŒ Langfuse í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return {}

    report_type_display = "ìµœì¢…ë³´ê³ ì„œ" if report_type == "executive" else "ì£¼ê°„ë³´ê³ ì„œ"

    print("\n" + "=" * 80)
    print(f"ğŸ” {report_type_display} BGE-M3 + RRF + Qwen3-Reranker í‰ê°€ (Langfuse ì—°ë™)")
    print("=" * 80)
    print(f"ğŸ“‹ ë°ì´í„°ì…‹: {len(eval_data)} ê°œ ìƒ˜í”Œ")
    print(f"ğŸ“Š Top-K: {top_k}")
    print(f"ğŸ·ï¸  Version: {version}")
    print()

    stats = {
        "total_queries": len(eval_data),
        "total_time": 0,
        "evaluations": []
    }

    for idx, item in enumerate(eval_data, 1):
        try:
            eval_result = evaluate_single_query_with_reranker(
                question=item["question"],
                ground_truth=item["ground_truth"],
                context_page_id=item.get("context_page_id"),
                item_metadata=item.get("metadata", {}),
                report_type=report_type,
                top_k=top_k,
                qdrant_locks=config['qdrant_locks'],
                system_prompt=system_prompt,
                answer_generation_prompt=answer_generation_prompt,
                langfuse=langfuse,
                idx=idx,
                version_tag=version
            )

            stats["evaluations"].append(eval_result)
            stats["total_time"] += eval_result["time"]

            # ìºì‹œ ì €ì¥
            save_embedding_cache()

        except Exception as e:
            print(f"  âŒ [{idx}] í‰ê°€ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()

    stats["avg_time"] = stats["total_time"] / stats["total_queries"] if stats["total_queries"] > 0 else 0
    stats["avg_contexts"] = sum(e["num_contexts"] for e in stats["evaluations"]) / len(stats["evaluations"]) if stats["evaluations"] else 0

    # ê²°ê³¼ ì €ì¥
    if output_dir:
        output_path = Path(output_dir)
    else:
        output_path = Path("data/langfuse/evaluation_results") / f"{report_type}_report"

    output_path.mkdir(parents=True, exist_ok=True)

    output_file = output_path / f"bge-m3-rrf-reranker_k{top_k}_stats.json"
    save_result = {k: v for k, v in stats.items() if k != "evaluations"}
    save_result["num_evaluations"] = len(stats.get("evaluations", []))
    save_result["config"] = {
        "retriever_name": f"bge-m3-rrf-reranker-k{top_k}",
        "display_name": f"BGE-M3 + RRF + Qwen3-Reranker (Top {top_k})",
        "report_type": report_type,
        "embedding_preset": "bge-m3",
        "retriever_type": "rrf_ensemble_reranker",
        "reranker_model": "Qwen3-Reranker-4B",
        "top_k": top_k,
        "version": version,
    }

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(save_result, f, indent=2, ensure_ascii=False, default=str)

    print(f"\nâœ… í‰ê°€ ì™„ë£Œ!")
    print(f"   - í‰ê·  ì‹œê°„: {stats['avg_time']*1000:.2f}ms")
    print(f"   - í‰ê·  ì»¨í…ìŠ¤íŠ¸ ìˆ˜: {stats['avg_contexts']:.2f}")
    print(f"   - ê²°ê³¼ ì €ì¥: {output_file}")

    # Langfuse flush
    print("\nâ³ Langfuseì— ë°ì´í„° ì „ì†¡ ì¤‘...")
    langfuse.flush()

    # ì„ë² ë”© ìºì‹œ ì €ì¥
    print("\nğŸ’¾ ì„ë² ë”© ìºì‹œ ì €ì¥ ì¤‘...")
    save_embedding_cache()

    return stats


def run_reranker_test(
    questions: List[str],
    report_type: str,
    top_k_values: List[int] = [6, 8, 10],
    output_file: str = None,
    global_date_filter: tuple = None
):
    """BGE-M3 + RRF + Qwen3-Reranker í…ŒìŠ¤íŠ¸ (ì—¬ëŸ¬ k ê°’)

    Args:
        questions: ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
        report_type: ë³´ê³ ì„œ íƒ€ì… ('executive' ë˜ëŠ” 'weekly')
        top_k_values: í…ŒìŠ¤íŠ¸í•  k ê°’ ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸: [6, 8, 10])
        output_file: ê²°ê³¼ ì €ì¥ ê²½ë¡œ
        global_date_filter: ì „ì—­ ë‚ ì§œ í•„í„°
    """

    config = get_report_config(report_type)
    llm_configs = config['llm_configs']
    langfuse = get_langfuse_client()

    report_type_display = "ìµœì¢…ë³´ê³ ì„œìš©" if report_type == "executive" else "ì£¼ê°„ë³´ê³ ì„œìš©"

    print("\n" + "=" * 100)
    print(f"ğŸ§ª {report_type_display} BGE-M3 + RRF + Qwen3-Reranker í…ŒìŠ¤íŠ¸ (Langfuse ì—°ë™)")
    print("=" * 100)
    print(f"\nğŸ” ë¦¬íŠ¸ë¦¬ë²„: BGE-M3 + RRF Ensemble + Qwen3-Reranker-4B")
    print(f"ğŸ“Š í…ŒìŠ¤íŠ¸í•  k ê°’: {top_k_values}")
    if global_date_filter:
        print(f"ğŸ“… ì „ì—­ ë‚ ì§œ í•„í„°: {global_date_filter[0][:10]} ~ {global_date_filter[1][:10]}")
    print("\nğŸ’¬ í‰ê°€ ëŒ€ìƒ LLM:")
    for i, llm_config in enumerate(llm_configs, 1):
        print(f"  {i}. {llm_config['display_name']} - {llm_config['description']}")
    print()

    # ê° k ê°’ì— ëŒ€í•´ í…ŒìŠ¤íŠ¸ ìˆ˜í–‰
    for top_k in top_k_values:
        print(f"\n{'#' * 100}")
        print(f"ğŸ¯ Top-{top_k} í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print(f"{'#' * 100}\n")

        all_results = []
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        combination_base_dir = Path("data/results/reranker_combinations") / f"k{top_k}_{timestamp}"
        combination_base_dir.mkdir(parents=True, exist_ok=True)

        for q_idx, question in enumerate(questions, 1):
            print(f"\n{'=' * 100}")
            print(f"ğŸ“‹ ì§ˆë¬¸ {q_idx}/{len(questions)} (k={top_k})")
            print(f"{'=' * 100}")
            print(f"â“ {question}\n")

            # ë‚ ì§œ í•„í„° ê²°ì •
            if global_date_filter:
                date_filter = global_date_filter
            else:
                date_filter = extract_date_filter_from_question(question)
                if date_filter:
                    print(f"ğŸ“… ê°ì§€ëœ ë‚ ì§œ í•„í„°: {date_filter[0][:10]} ~ {date_filter[1][:10]}\n")

            # ë¬¸ì„œ ê²€ìƒ‰ ë° ì¬ìˆœìœ„í™”
            docs = retrieve_and_rerank_documents(
                question,
                config['qdrant_locks'],
                top_k=top_k,
                date_filter=date_filter,
                langfuse=langfuse,
                question_id=q_idx
            )

            question_result = {
                "question_id": q_idx,
                "question": question,
                "date_filter": f"{date_filter[0][:10]} ~ {date_filter[1][:10]}" if date_filter else None,
                "retriever": f"bge-m3-rrf-reranker-k{top_k}",
                "num_docs": len(docs),
                "doc_titles": [doc.metadata.get('page_title', 'Unknown') for doc in docs],
                "llms": []
            }

            # ê° LLMìœ¼ë¡œ ë³‘ë ¬ í…ŒìŠ¤íŠ¸
            print(f"\nğŸš€ {len(llm_configs)}ê°œ LLMìœ¼ë¡œ ë³‘ë ¬ ë‹µë³€ ìƒì„± ì‹œì‘...\n")

            with ThreadPoolExecutor(max_workers=7) as executor:
                future_to_llm = {
                    executor.submit(test_single_llm, llm_config, question, docs, report_type, langfuse, q_idx): (rank, llm_config)
                    for rank, llm_config in enumerate(llm_configs, 1)
                }

                for future in as_completed(future_to_llm):
                    rank, llm_config = future_to_llm[future]
                    try:
                        result = future.result()
                        result["rank"] = rank
                        question_result["llms"].append(result)

                        # ì¤‘ê°„ ì €ì¥
                        temp_results = all_results + [question_result]
                        try:
                            save_combination_results(temp_results, top_k, llm_configs, timestamp, combination_base_dir, questions, report_type)
                            print(f"ğŸ’¾ {llm_config['display_name']} ê²°ê³¼ ì €ì¥ ì™„ë£Œ")
                        except Exception as e:
                            print(f"âš ï¸ ì €ì¥ ì‹¤íŒ¨: {e}")

                    except Exception as e:
                        print(f"âŒ {llm_config['display_name']} ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
                        result = {
                            "success": False,
                            "llm_name": llm_config['name'],
                            "llm_display_name": llm_config['display_name'],
                            "error": str(e),
                            "rank": rank
                        }
                        question_result["llms"].append(result)

            question_result["llms"].sort(key=lambda x: x["rank"])
            all_results.append(question_result)

            # ì§ˆë¬¸ ì™„ë£Œ ì‹œ ìµœì¢… ì €ì¥
            print(f"\nğŸ’¾ ì§ˆë¬¸ {q_idx} ì™„ë£Œ - ìµœì¢… ê²°ê³¼ ì €ì¥ ì¤‘...")
            save_combination_results(all_results, top_k, llm_configs, timestamp, combination_base_dir, questions, report_type)

        # k ê°’ë³„ summary ì €ì¥
        base_output_dir = Path("data/results/reranker_summary") / f"k{top_k}_{timestamp}"

        if output_file:
            output_path = Path(output_file)
        else:
            output_path = base_output_dir / "summary.json"

        output_path.parent.mkdir(parents=True, exist_ok=True)

        final_result = {
            "test_date": datetime.now().isoformat(),
            "report_type": report_type,
            "top_k": top_k,
            "num_questions": len(questions),
            "retriever_config": {
                "name": f"bge-m3-rrf-reranker-k{top_k}",
                "display_name": f"BGE-M3 + RRF + Qwen3-Reranker (Top {top_k})",
                "description": f"BGE-M3 embedding with RRF ensemble and Qwen3-Reranker-4B reranking (final k={top_k})",
                "type": "rrf_ensemble_reranker",
                "embedding": "baai-bge-m3",
                "reranker": "Qwen3-Reranker-4B",
                "top_k": top_k
            },
            "llm_configs": llm_configs,
            "results": all_results
        }

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(final_result, f, ensure_ascii=False, indent=2)

        print(f"\n{'=' * 100}")
        print(f"âœ… Top-{top_k} í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print(f"ğŸ“ Summary ì €ì¥: {output_path}")
        print(f"ğŸ“ ì¡°í•©ë³„ ê²°ê³¼: {combination_base_dir}")
        print(f"{'=' * 100}\n")

    print(f"\nğŸ”— Langfuseì—ì„œ ìƒì„¸ ì¶”ì  ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”")
    langfuse.flush()


def main():
    import argparse

    parser = argparse.ArgumentParser(
        description="BGE-M3 + RRF + Qwen3-Reranker ë³´ê³ ì„œ ìƒì„± ë° ë¹„êµ (Langfuse ì—°ë™)"
    )
    parser.add_argument(
        "--report-type",
        type=str,
        choices=["executive", "weekly"],
        default="executive",
        help="ë³´ê³ ì„œ íƒ€ì…: executive(ìµœì¢…ë³´ê³ ì„œ), weekly(ì£¼ê°„ë³´ê³ ì„œ) (ê¸°ë³¸: executive)"
    )
    parser.add_argument(
        "--mode",
        type=str,
        choices=["single", "multi", "interactive", "evaluate"],
        default="interactive",
        help="í…ŒìŠ¤íŠ¸ ëª¨ë“œ: single(ë‹¨ì¼ ì§ˆë¬¸), multi(íŒŒì¼ì—ì„œ ì—¬ëŸ¬ ì§ˆë¬¸), interactive(ëŒ€í™”í˜• ì…ë ¥), evaluate(í‰ê°€ ë°ì´í„°ì…‹ ì‚¬ìš©)"
    )
    parser.add_argument(
        "--num-questions",
        type=int,
        default=5,
        help="multi ëª¨ë“œì—ì„œ í…ŒìŠ¤íŠ¸í•  ì§ˆë¬¸ ê°œìˆ˜ (ê¸°ë³¸: 5)"
    )
    parser.add_argument(
        "--question",
        type=str,
        default=None,
        help="single ëª¨ë“œì—ì„œ ì‚¬ìš©í•  ì§ˆë¬¸"
    )
    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="ê²°ê³¼ ì €ì¥ ê²½ë¡œ (ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ì €ì¥í•˜ì§€ ì•ŠìŒ)"
    )
    parser.add_argument(
        "--start-date",
        type=str,
        default=None,
        help="ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD í˜•ì‹)"
    )
    parser.add_argument(
        "--end-date",
        type=str,
        default=None,
        help="ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD í˜•ì‹)"
    )
    parser.add_argument(
        "--date-range",
        type=str,
        default=None,
        help="ìì—°ì–´ ë‚ ì§œ ë²”ìœ„ (ì˜ˆ: 'ì´ë²ˆ ì£¼', 'ì§€ë‚œì£¼', '12ì›” 2ì£¼ì°¨')"
    )
    parser.add_argument(
        "--top-k",
        type=str,
        default="6,8,10",
        help="í…ŒìŠ¤íŠ¸í•  k ê°’ (ì½¤ë§ˆë¡œ êµ¬ë¶„, ê¸°ë³¸: 6,8,10). evaluate ëª¨ë“œì—ì„œëŠ” ë‹¨ì¼ ê°’ë§Œ ì‚¬ìš©"
    )
    parser.add_argument(
        "--dataset",
        type=str,
        default="/home/work/rag/Project/rag-report-generator/data/evaluation/merged_qa_dataset.json",
        help="evaluate ëª¨ë“œì—ì„œ ì‚¬ìš©í•  í‰ê°€ ë°ì´í„°ì…‹ ê²½ë¡œ (ê¸°ë³¸: merged_qa_dataset.json)"
    )
    parser.add_argument(
        "--version",
        type=str,
        default="v1",
        help="í‰ê°€ ë²„ì „ íƒœê·¸ (ê¸°ë³¸: v1)"
    )

    args = parser.parse_args()

    # k ê°’ íŒŒì‹±
    top_k_values = [int(k.strip()) for k in args.top_k.split(',')]

    # ë‚ ì§œ í•„í„° íŒŒì‹±
    date_filter = parse_date_range(
        date_input=args.date_range,
        start_date=args.start_date,
        end_date=args.end_date
    )

    # ë³´ê³ ì„œ íƒ€ì…ì— ë§ëŠ” ì„¤ì • ë¡œë“œ
    config = get_report_config(args.report_type)

    if args.mode == "evaluate":
        # í‰ê°€ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•œ í‰ê°€ ëª¨ë“œ
        # evaluate ëª¨ë“œì—ì„œëŠ” top_k_valuesì˜ ì²« ë²ˆì§¸ ê°’ë§Œ ì‚¬ìš©
        top_k = top_k_values[0]

        print("\n" + "=" * 80)
        print(f"ğŸ“Š í‰ê°€ ëª¨ë“œ: í‰ê°€ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•œ ì„±ëŠ¥ í‰ê°€")
        print("=" * 80)
        print(f"ë³´ê³ ì„œ íƒ€ì…: {args.report_type}")
        print(f"ë°ì´í„°ì…‹: {args.dataset}")
        print(f"Top-K: {top_k}")
        print(f"ë²„ì „: {args.version}")
        print()

        run_reranker_evaluation(
            report_type=args.report_type,
            dataset_path=args.dataset,
            top_k=top_k,
            version=args.version,
            output_dir=args.output
        )

    elif args.mode == "interactive":
        print("\n" + "=" * 100)
        print(f"ğŸ¯ ëŒ€í™”í˜• ëª¨ë“œ: ì§ˆë¬¸ì„ ì§ì ‘ ì…ë ¥í•˜ì„¸ìš” (ë³´ê³ ì„œ íƒ€ì…: {args.report_type})")
        print("=" * 100)
        print("ğŸ’¡ íŒ:")
        print("   - ì—¬ëŸ¬ ì§ˆë¬¸ì„ ì…ë ¥í•˜ë ¤ë©´ ì„¸ë¯¸ì½œë¡ (;)ìœ¼ë¡œ êµ¬ë¶„í•˜ì„¸ìš”")
        print("   - ì…ë ¥ ì—†ì´ ì—”í„°ë¥¼ ëˆ„ë¥´ë©´ ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ 5ê°œê°€ ì‚¬ìš©ë©ë‹ˆë‹¤")
        print()

        user_input = input("ğŸ“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì—”í„°: ê¸°ë³¸ ì§ˆë¬¸ ì‚¬ìš©): ").strip()

        if not user_input:
            print("\nâœ… ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤:")
            questions = config['test_questions']
            for i, q in enumerate(questions, 1):
                print(f"  {i}. {q}")
        else:
            questions = [q.strip() for q in user_input.split(';') if q.strip()]
            print(f"\nâœ… ì´ {len(questions)}ê°œì˜ ì§ˆë¬¸ì´ ì…ë ¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            for i, q in enumerate(questions, 1):
                print(f"  {i}. {q}")

        run_reranker_test(questions, args.report_type, top_k_values, args.output, date_filter)

    elif args.mode == "single":
        if not args.question:
            print("âŒ --question ì¸ìê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return
        run_reranker_test([args.question], args.report_type, top_k_values, args.output, date_filter)

    else:
        # ì—¬ëŸ¬ ì§ˆë¬¸ ëª¨ë“œ (íŒŒì¼ì—ì„œ ë¡œë“œ)
        test_questions_data = load_test_questions(n=args.num_questions)
        questions = [q["question"] for q in test_questions_data]

        run_reranker_test(questions, args.report_type, top_k_values, args.output, date_filter)


if __name__ == "__main__":
    main()
