#!/usr/bin/env python3
"""ì„¤ì • íŒŒì¼ ê¸°ë°˜ í‰ê°€ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

evaluation_config.yamlì— ì •ì˜ëœ ë¦¬íŠ¸ë¦¬ë²„ ì¡°í•©ë³„ë¡œ í‰ê°€ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
"""

import sys
import os
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

import yaml
import json
from typing import Dict, Any, List
from datetime import datetime
import argparse


def load_evaluation_config(config_path: str = None) -> Dict[str, Any]:
    """í‰ê°€ ì„¤ì • íŒŒì¼ ë¡œë“œ

    Args:
        config_path: ì„¤ì • íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: config/evaluation_config.yaml)

    Returns:
        ì„¤ì • ë”•ì…”ë„ˆë¦¬
    """
    if config_path is None:
        config_path = Path(__file__).parent.parent / "config" / "evaluation_config.yaml"

    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)

    return config


def load_prompt_template(template_path: str) -> str:
    """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¡œë“œ

    Args:
        template_path: í…œí”Œë¦¿ íŒŒì¼ ê²½ë¡œ

    Returns:
        í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸
    """
    base_dir = Path(__file__).parent.parent
    full_path = base_dir / template_path

    with open(full_path, 'r', encoding='utf-8') as f:
        return f.read().strip()


def run_evaluation_for_retriever(
    retriever_config: Dict[str, Any],
    report_type: str,
    system_prompt: str,
    answer_generation_prompt: str,
    dataset_path: str,
    output_dir: str
) -> Dict[str, Any]:
    """ë‹¨ì¼ ë¦¬íŠ¸ë¦¬ë²„ì— ëŒ€í•œ í‰ê°€ ì‹¤í–‰

    Args:
        retriever_config: ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì •
        report_type: ë³´ê³ ì„œ íƒ€ì… (weekly_report / executive_report)
        system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        answer_generation_prompt: ë‹µë³€ ìƒì„± í”„ë¡¬í”„íŠ¸
        dataset_path: í‰ê°€ ë°ì´í„°ì…‹ ê²½ë¡œ
        output_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬

    Returns:
        í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    print("\n" + "=" * 80)
    print(f"ğŸš€ í‰ê°€ ì‹œì‘: {retriever_config['display_name']}")
    print("=" * 80)
    print(f"ğŸ“‹ ë³´ê³ ì„œ íƒ€ì…: {report_type}")
    print(f"ğŸ” ë¦¬íŠ¸ë¦¬ë²„: {retriever_config['retriever_type']}")
    print(f"ğŸ§¬ ì„ë² ë”© ëª¨ë¸: {retriever_config['embedding_preset']}")
    print(f"ğŸ“„ ì„¤ëª…: {retriever_config['description']}")
    print()

    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    os.environ['MODEL_PRESET'] = retriever_config['embedding_preset']

    # TODO: ì‹¤ì œ í‰ê°€ ë¡œì§ êµ¬í˜„
    # 1. ë¦¬íŠ¸ë¦¬ë²„ ì´ˆê¸°í™”
    # 2. ë°ì´í„°ì…‹ ë¡œë“œ
    # 3. ê° ì§ˆë¬¸ì— ëŒ€í•´:
    #    - ë¬¸ì„œ ê²€ìƒ‰
    #    - ë‹µë³€ ìƒì„± (system_prompt + answer_generation_prompt ì‚¬ìš©)
    #    - ë©”íŠ¸ë¦­ í‰ê°€ (Precision, Recall, Faithfulness)
    # 4. ê²°ê³¼ ì§‘ê³„ ë° ì €ì¥

    # ê²°ê³¼ ì €ì¥ ê²½ë¡œ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    result_filename = f"eval_{retriever_config['name']}_{report_type}_{timestamp}.json"
    result_path = Path(output_dir) / report_type / result_filename
    result_path.parent.mkdir(parents=True, exist_ok=True)

    # ì„ì‹œ ê²°ê³¼ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” í‰ê°€ ê²°ê³¼ë¡œ ëŒ€ì²´)
    result = {
        "retriever_name": retriever_config['name'],
        "display_name": retriever_config['display_name'],
        "report_type": report_type,
        "embedding_preset": retriever_config['embedding_preset'],
        "retriever_type": retriever_config['retriever_type'],
        "timestamp": timestamp,
        "expected_performance": retriever_config.get('expected_performance', {}),
        "actual_performance": {
            # TODO: ì‹¤ì œ í‰ê°€ ê²°ê³¼ë¡œ ì±„ìš°ê¸°
            "precision": 0.0,
            "recall": 0.0,
            "faithfulness": 0.0
        },
        "dataset_path": dataset_path,
        "result_path": str(result_path)
    }

    print(f"âœ… í‰ê°€ ì™„ë£Œ")
    print(f"ğŸ“Š ì˜ˆìƒ ì„±ëŠ¥:")
    for metric, value in retriever_config.get('expected_performance', {}).items():
        print(f"   - {metric}: {value:.2f}")
    print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {result_path}")

    return result


def run_weekly_report_evaluation(config: Dict[str, Any]) -> List[Dict[str, Any]]:
    """ì£¼ê°„ ë³´ê³ ì„œìš© í‰ê°€ ì‹¤í–‰

    Args:
        config: ì „ì²´ ì„¤ì • ë”•ì…”ë„ˆë¦¬

    Returns:
        í‰ê°€ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    weekly_config = config['weekly_report']

    print("\n" + "=" * 80)
    print(f"ğŸ“Š {weekly_config['name']} í‰ê°€ ì‹œì‘")
    print("=" * 80)
    print(f"ìš°ì„ ìˆœìœ„: {' > '.join(weekly_config['priority'])}")
    print(f"í‰ê°€ ëŒ€ìƒ ë¦¬íŠ¸ë¦¬ë²„: {len(weekly_config['retrievers'])}ê°œ")
    print()

    # í”„ë¡¬í”„íŠ¸ ë¡œë“œ
    system_prompt = load_prompt_template(weekly_config['system_prompt_path'])
    answer_generation_prompt = load_prompt_template(weekly_config['answer_generation_prompt_path'])

    results = []
    for retriever_config in weekly_config['retrievers']:
        result = run_evaluation_for_retriever(
            retriever_config=retriever_config,
            report_type="weekly_report",
            system_prompt=system_prompt,
            answer_generation_prompt=answer_generation_prompt,
            dataset_path=config['evaluation']['dataset_path'],
            output_dir=config['evaluation']['output_dir']
        )
        results.append(result)

    return results


def run_executive_report_evaluation(config: Dict[str, Any]) -> List[Dict[str, Any]]:
    """ì„ì› ë³´ê³ ì„œìš© í‰ê°€ ì‹¤í–‰

    Args:
        config: ì „ì²´ ì„¤ì • ë”•ì…”ë„ˆë¦¬

    Returns:
        í‰ê°€ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    exec_config = config['executive_report']

    print("\n" + "=" * 80)
    print(f"ğŸ“Š {exec_config['name']} í‰ê°€ ì‹œì‘")
    print("=" * 80)
    print(f"ìš°ì„ ìˆœìœ„: {' > '.join(exec_config['priority'])}")
    print(f"í‰ê°€ ëŒ€ìƒ ë¦¬íŠ¸ë¦¬ë²„: {len(exec_config['retrievers'])}ê°œ")
    print()

    # í”„ë¡¬í”„íŠ¸ ë¡œë“œ
    system_prompt = load_prompt_template(exec_config['system_prompt_path'])
    answer_generation_prompt = load_prompt_template(exec_config['answer_generation_prompt_path'])

    results = []
    for retriever_config in exec_config['retrievers']:
        result = run_evaluation_for_retriever(
            retriever_config=retriever_config,
            report_type="executive_report",
            system_prompt=system_prompt,
            answer_generation_prompt=answer_generation_prompt,
            dataset_path=config['evaluation']['dataset_path'],
            output_dir=config['evaluation']['output_dir']
        )
        results.append(result)

    return results


def compare_results(weekly_results: List[Dict[str, Any]], exec_results: List[Dict[str, Any]]):
    """í‰ê°€ ê²°ê³¼ ë¹„êµ ë° ì¶œë ¥

    Args:
        weekly_results: ì£¼ê°„ ë³´ê³ ì„œ í‰ê°€ ê²°ê³¼
        exec_results: ì„ì› ë³´ê³ ì„œ í‰ê°€ ê²°ê³¼
    """
    print("\n" + "=" * 80)
    print("ğŸ† í‰ê°€ ê²°ê³¼ ìš”ì•½")
    print("=" * 80)

    print("\nğŸ“‹ ì£¼ê°„ ë³´ê³ ì„œ (ìš´ì˜íŒ€)")
    print("-" * 80)
    for result in weekly_results:
        print(f"\n{result['display_name']}")
        print(f"   ì˜ˆìƒ ì„±ëŠ¥: ", end="")
        expected = result['expected_performance']
        print(f"Precision={expected.get('precision', 0):.2f}, ", end="")
        print(f"Recall={expected.get('recall', 0):.2f}, ", end="")
        print(f"Faithfulness={expected.get('faithfulness', 0):.2f}")

    print("\n\nğŸ“‹ ì„ì› ë³´ê³ ì„œ (ì˜ì‚¬ê²°ì •)")
    print("-" * 80)
    for result in exec_results:
        print(f"\n{result['display_name']}")
        print(f"   ì˜ˆìƒ ì„±ëŠ¥: ", end="")
        expected = result['expected_performance']
        print(f"Precision={expected.get('precision', 0):.2f}, ", end="")
        print(f"Recall={expected.get('recall', 0):.2f}, ", end="")
        print(f"Faithfulness={expected.get('faithfulness', 0):.2f}")

    print("\n\nğŸ’¡ ì¶”ì²œ")
    print("-" * 80)
    print("ì£¼ê°„ ë³´ê³ ì„œìš© (Recall ìš°ì„ ):")
    print("  â­â­â­ Upstage + RRF + MultiQuery + LC")
    print("       â†’ ì™„ë²½í•œ ê²€ìƒ‰ ì„±ëŠ¥ (Precision 1.00, Recall 1.00)")
    print()
    print("ì„ì› ë³´ê³ ì„œìš© (Faithfulness ìš°ì„ ):")
    print("  â­â­â­ OpenAI + RRF + MultiQuery")
    print("       â†’ ìµœê³  Faithfulness (0.96)")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="ë¦¬íŠ¸ë¦¬ë²„ ì¡°í•©ë³„ í‰ê°€ ì‹¤í–‰")
    parser.add_argument(
        "--config",
        type=str,
        default=None,
        help="í‰ê°€ ì„¤ì • íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: config/evaluation_config.yaml)"
    )
    parser.add_argument(
        "--report-type",
        type=str,
        choices=["weekly", "executive", "both"],
        default="both",
        help="í‰ê°€í•  ë³´ê³ ì„œ íƒ€ì… (weekly: ì£¼ê°„ ë³´ê³ ì„œ, executive: ì„ì› ë³´ê³ ì„œ, both: ë‘˜ ë‹¤)"
    )

    args = parser.parse_args()

    # ì„¤ì • ë¡œë“œ
    config = load_evaluation_config(args.config)

    print("=" * 80)
    print("ğŸ¯ ë¦¬íŠ¸ë¦¬ë²„ ì¡°í•©ë³„ í‰ê°€ ì‹œìŠ¤í…œ")
    print("=" * 80)
    print(f"ì„¤ì • íŒŒì¼: {args.config or 'config/evaluation_config.yaml'}")
    print(f"í‰ê°€ íƒ€ì…: {args.report_type}")
    print()

    # í‰ê°€ ì‹¤í–‰
    weekly_results = []
    exec_results = []

    if args.report_type in ["weekly", "both"]:
        weekly_results = run_weekly_report_evaluation(config)

    if args.report_type in ["executive", "both"]:
        exec_results = run_executive_report_evaluation(config)

    # ê²°ê³¼ ë¹„êµ
    if weekly_results or exec_results:
        compare_results(weekly_results, exec_results)

    print("\nâœ… ëª¨ë“  í‰ê°€ ì™„ë£Œ")


if __name__ == "__main__":
    main()
