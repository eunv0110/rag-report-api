# ë³´ê³ ì„œ íƒ€ì…ë³„ ë¦¬íŠ¸ë¦¬ë²„ í‰ê°€ ì‹œìŠ¤í…œ

ì£¼ê°„ ë³´ê³ ì„œ(ìš´ì˜íŒ€)ì™€ ì„ì› ë³´ê³ ì„œ(ì˜ì‚¬ê²°ì •) ê°ê°ì— ìµœì í™”ëœ ë¦¬íŠ¸ë¦¬ë²„ ì¡°í•©ì„ í‰ê°€í•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨

- [ê°œìš”](#ê°œìš”)
- [í‰ê°€ ì „ëµ](#í‰ê°€-ì „ëµ)
- [í”„ë¡¬í”„íŠ¸ êµ¬ì¡°](#í”„ë¡¬í”„íŠ¸-êµ¬ì¡°)
- [ì„¤ì • íŒŒì¼](#ì„¤ì •-íŒŒì¼)
- [ì‚¬ìš©ë²•](#ì‚¬ìš©ë²•)
- [ê²°ê³¼ ë¶„ì„](#ê²°ê³¼-ë¶„ì„)

## ê°œìš”

### ì£¼ê°„ ë³´ê³ ì„œ (ìš´ì˜íŒ€)
- **ìš°ì„ ìˆœìœ„**: Recall > Precision > Faithfulness
- **ëª©ì **: ë†“ì¹˜ëŠ” ì •ë³´ ì—†ì´ ëª¨ë“  ë‚´ìš© ì»¤ë²„
- **ì‚¬ìš©ì**: ìš´ì˜íŒ€ (ì¼ì¼ ì—…ë¬´ ìˆ˜í–‰)

### ì„ì› ë³´ê³ ì„œ (ì˜ì‚¬ê²°ì •)
- **ìš°ì„ ìˆœìœ„**: Faithfulness > Precision > Recall
- **ëª©ì **: í‹€ë¦° ì •ë³´ ì ˆëŒ€ ì•ˆë¨
- **ì‚¬ìš©ì**: ì„ì›ì§„ (ì¤‘ìš” ì˜ì‚¬ê²°ì •)

## í‰ê°€ ì „ëµ

### ì£¼ê°„ ë³´ê³ ì„œìš© ë¦¬íŠ¸ë¦¬ë²„

#### 1. Upstage + RRF + MultiQuery + LC â­â­â­ (ì¶”ì²œ)
- **ì„ë² ë”©**: Upstage Solar Embedding
- **ë¦¬íŠ¸ë¦¬ë²„**: RRF + MultiQuery + LongContext
- **ì˜ˆìƒ ì„±ëŠ¥**: Precision 1.00, Recall 1.00, Faithfulness 0.76
- **ì¥ì **: ì™„ë²½í•œ ê²€ìƒ‰ ì„±ëŠ¥ + ì¤€ìˆ˜í•œ Faithfulness

#### 2. Qwen + RRF Ensemble
- **ì„ë² ë”©**: Qwen 2.5 72B
- **ë¦¬íŠ¸ë¦¬ë²„**: RRF Ensemble
- **ì˜ˆìƒ ì„±ëŠ¥**: Precision 0.96, Recall 0.99, Faithfulness 0.76
- **ì¥ì **: ì•ˆì •ì  ë°±ì—… ì˜µì…˜

### ì„ì› ë³´ê³ ì„œìš© ë¦¬íŠ¸ë¦¬ë²„

#### 1. OpenAI + RRF + MultiQuery â­â­â­ (ì¶”ì²œ)
- **ì„ë² ë”©**: OpenAI text-embedding-3-large
- **ë¦¬íŠ¸ë¦¬ë²„**: RRF + MultiQuery
- **ì˜ˆìƒ ì„±ëŠ¥**: Precision 0.95, Recall 0.87, Faithfulness 0.96
- **ì¥ì **: ì „ì²´ ìµœê³  Faithfulness, ì„ì› ë³´ê³ ì„œì— ê°€ì¥ ì í•©

#### 2. BGE-M3 + RRF + LC + Time
- **ì„ë² ë”©**: BGE-M3 (Jina Embeddings v3)
- **ë¦¬íŠ¸ë¦¬ë²„**: RRF + LongContext + TimeWeighted
- **ì˜ˆìƒ ì„±ëŠ¥**: Precision 0.97, Recall 0.97, Faithfulness 0.80
- **ì¥ì **: ìµœì‹  ì •ë³´ ë°˜ì˜ + ë†’ì€ Faithfulness

## í”„ë¡¬í”„íŠ¸ êµ¬ì¡°

### ì£¼ê°„ ë³´ê³ ì„œ í”„ë¡¬í”„íŠ¸

**ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸** (`prompts/templates/evaluation/weekly_report/system_prompt.txt`):
- ì™„ì „ì„±, í¬ê´„ì„±, ì •í™•ì„± ê°•ì¡°
- ëª¨ë“  ì •ë³´ë¥¼ ë¹ ì§ì—†ì´ í¬í•¨í•˜ë„ë¡ ì§€ì‹œ
- ê²€ìƒ‰ ê²°ê³¼ì— ì—†ëŠ” ë‚´ìš© ì¶”ê°€ ê¸ˆì§€

**ë‹µë³€ ìƒì„± í”„ë¡¬í”„íŠ¸** (`prompts/templates/evaluation/weekly_report/answer_generation_prompt.txt`):
```
## ì£¼ê°„ ìš´ì˜ ë³´ê³ ì„œ
### 1. ì£¼ìš” ì§€í‘œ
### 2. ì£¼ìš” í™œë™ ë° ì§„í–‰ì‚¬í•­
### 3. ì´ìŠˆ ë° ë¬¸ì œì 
### 4. ë‹¤ìŒ ì£¼ ì˜ˆì •ì‚¬í•­
```

### ì„ì› ë³´ê³ ì„œ í”„ë¡¬í”„íŠ¸

**ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸** (`prompts/templates/evaluation/executive_report/system_prompt.txt`):
- ì •í™•ì„±, ì‹ ë¢°ì„±, ëª…í™•ì„± ìµœìš°ì„ 
- ì¶”ì¸¡, ì¶”ë¡ , ê°€ì • ì ˆëŒ€ ê¸ˆì§€
- ë¶ˆí™•ì‹¤í•œ ì •ë³´ëŠ” ì œì™¸

**ë‹µë³€ ìƒì„± í”„ë¡¬í”„íŠ¸** (`prompts/templates/evaluation/executive_report/answer_generation_prompt.txt`):
```
## ì„ì› ë³´ê³ ì„œ
### Executive Summary
### 1. ì£¼ìš” í˜„í™©
### 2. í•µì‹¬ ì´ìŠˆ ë° ë¦¬ìŠ¤í¬
### 3. ê¶Œê³ ì‚¬í•­ (ì„ íƒì )
### 4. ì¶”ê°€ í™•ì¸ í•„ìš” ì‚¬í•­
```

## ì„¤ì • íŒŒì¼

### `config/evaluation_config.yaml`

ì„¤ì • íŒŒì¼ì—ì„œ ë‹¤ìŒì„ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```yaml
weekly_report:
  retrievers:
    - name: "upstage_rrf_multiquery_lc"
      display_name: "Upstage + RRF + MultiQuery + LC"
      embedding_preset: "upstage"
      retriever_type: "rrf_multiquery_longcontext"
      k: 10  # Top-K ê°’ (ê°œë³„ ì„¤ì • ê°€ëŠ¥)
      expected_performance:
        precision: 1.00
        recall: 1.00
        faithfulness: 0.76
```

**ì£¼ìš” ì„¤ì • ì˜µì…˜**:
- `name`: ë¦¬íŠ¸ë¦¬ë²„ ê³ ìœ  ì´ë¦„
- `embedding_preset`: ì„ë² ë”© ëª¨ë¸ (upstage, qwen, openai, bge_m3)
- `retriever_type`: ë¦¬íŠ¸ë¦¬ë²„ íƒ€ì… (rrf_multiquery_longcontext, rrf_ensemble, ë“±)
- `k`: Top-K ê°’ (ì˜µì…˜, ë¯¸ì§€ì • ì‹œ ì»¤ë§¨ë“œë¼ì¸ `--top-k` ê°’ ì‚¬ìš©)

## ì‚¬ìš©ë²•

### 1. ì„¤ì • í™•ì¸

```bash
# í”„ë¡¬í”„íŠ¸ íŒŒì¼ê³¼ ì„¤ì •ì´ ì˜¬ë°”ë¥¸ì§€ í…ŒìŠ¤íŠ¸
python scripts/test_evaluation_setup.py
```

### 2. í‰ê°€ ì‹¤í–‰

#### ëª¨ë“  í‰ê°€ ì‹¤í–‰ (ì£¼ê°„ + ì„ì›)
```bash
python evaluators/evaluate_report_types.py --report-type both
```

#### ì£¼ê°„ ë³´ê³ ì„œë§Œ í‰ê°€
```bash
python evaluators/evaluate_report_types.py --report-type weekly
```

#### ì„ì› ë³´ê³ ì„œë§Œ í‰ê°€
```bash
python evaluators/evaluate_report_types.py --report-type executive
```

#### íŠ¹ì • ë¦¬íŠ¸ë¦¬ë²„ë§Œ í‰ê°€
```bash
python evaluators/evaluate_report_types.py \
  --report-type weekly \
  --retrievers upstage_rrf_multiquery_lc
```

#### Top-K ê°’ ë³€ê²½
```bash
python evaluators/evaluate_report_types.py \
  --report-type both \
  --top-k 15
```

**ì°¸ê³ **: ê°œë³„ ë¦¬íŠ¸ë¦¬ë²„ì˜ `k` ê°’ì€ ì„¤ì • íŒŒì¼ì—ì„œ ì§€ì • ê°€ëŠ¥í•˜ë©°, ì„¤ì •ëœ ê°’ì´ ì»¤ë§¨ë“œë¼ì¸ `--top-k`ë³´ë‹¤ ìš°ì„ ë©ë‹ˆë‹¤.

#### ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ ì‚¬ìš©
```bash
python evaluators/evaluate_report_types.py \
  --dataset data/evaluation/custom_qa_dataset.json \
  --report-type both
```

#### ë²„ì „ íƒœê·¸ ì§€ì •
```bash
python evaluators/evaluate_report_types.py \
  --report-type both \
  --version v2
```

### 3. ê²°ê³¼ í™•ì¸

#### Langfuse ëŒ€ì‹œë³´ë“œ
í‰ê°€ ê²°ê³¼ëŠ” Langfuseì— ìë™ìœ¼ë¡œ ê¸°ë¡ë©ë‹ˆë‹¤:
- URL: https://cloud.langfuse.com
- ê° í‰ê°€ëŠ” traceë¡œ ê¸°ë¡ë˜ë©° ë‹¤ìŒ íƒœê·¸ë¡œ í•„í„°ë§ ê°€ëŠ¥:
  - `weekly_report` / `executive_report`
  - ë¦¬íŠ¸ë¦¬ë²„ ì´ë¦„ (ì˜ˆ: `upstage_rrf_multiquery_lc_v1`)
  - ì„ë² ë”© í”„ë¦¬ì…‹ (ì˜ˆ: `upstage`, `openai`)

#### ë¡œì»¬ ê²°ê³¼ íŒŒì¼
```
data/langfuse/evaluation_results/
â”œâ”€â”€ weekly_report/
â”‚   â”œâ”€â”€ upstage_rrf_multiquery_lc_stats.json
â”‚   â””â”€â”€ qwen_rrf_ensemble_stats.json
â””â”€â”€ executive_report/
    â”œâ”€â”€ openai_rrf_multiquery_stats.json
    â””â”€â”€ bge_m3_rrf_lc_time_stats.json
```

ê° íŒŒì¼ì—ëŠ” ë‹¤ìŒ ì •ë³´ê°€ í¬í•¨ë©ë‹ˆë‹¤:
- ì´ ì¿¼ë¦¬ ìˆ˜
- í‰ê·  ì‘ë‹µ ì‹œê°„
- í‰ê·  ì»¨í…ìŠ¤íŠ¸ ìˆ˜
- ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì • ì •ë³´

## ê²°ê³¼ ë¶„ì„

### ë©”íŠ¸ë¦­ ì„¤ëª…

1. **Precision (ì •ë°€ë„)**
   - ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì‹¤ì œë¡œ ê´€ë ¨ ìˆëŠ”ì§€ ì¸¡ì •
   - ë†’ì„ìˆ˜ë¡ ë…¸ì´ì¦ˆê°€ ì ìŒ

2. **Recall (ì¬í˜„ìœ¨)**
   - ê´€ë ¨ëœ ëª¨ë“  ë¬¸ì„œë¥¼ ì°¾ì•˜ëŠ”ì§€ ì¸¡ì •
   - ë†’ì„ìˆ˜ë¡ ì •ë³´ ëˆ„ë½ì´ ì ìŒ

3. **Faithfulness (ì¶©ì‹¤ë„)**
   - ìƒì„±ëœ ë‹µë³€ì´ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì— ì¶©ì‹¤í•œì§€ ì¸¡ì •
   - ë†’ì„ìˆ˜ë¡ hallucinationì´ ì ìŒ

### í‰ê°€ ê¸°ì¤€

**ì£¼ê°„ ë³´ê³ ì„œ**:
- âœ… ëª¨ë“  ê´€ë ¨ ì •ë³´ê°€ í¬í•¨ë˜ì—ˆëŠ”ê°€? (Recall ìµœìš°ì„ )
- âœ… í¬í•¨ëœ ì •ë³´ê°€ ëª¨ë‘ ì •í™•í•œê°€? (Precision)
- âœ… ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¶©ì‹¤íˆ ë°˜ì˜í–ˆëŠ”ê°€? (Faithfulness)

**ì„ì› ë³´ê³ ì„œ**:
- âœ… í‹€ë¦° ì •ë³´ê°€ ì „í˜€ ì—†ëŠ”ê°€? (Faithfulness ìµœìš°ì„ )
- âœ… ë¶ˆí™•ì‹¤í•œ ì¶”ì¸¡ì´ í¬í•¨ë˜ì§€ ì•Šì•˜ëŠ”ê°€? (Precision)
- âœ… í•µì‹¬ ì •ë³´ê°€ ëˆ„ë½ë˜ì§€ ì•Šì•˜ëŠ”ê°€? (Recall)

## ì¶”ê°€ ì •ë³´

### í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

í‰ê°€ ì‹¤í–‰ ì „ `.env` íŒŒì¼ì— ë‹¤ìŒ ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤:

```bash
# Azure OpenAI (ë‹µë³€ ìƒì„±ìš©)
AZURE_AI_CREDENTIAL=your_credential
AZURE_AI_ENDPOINT=https://models.inference.ai.azure.com

# Langfuse (í‰ê°€ ì¶”ì ìš©)
LANGFUSE_PUBLIC_KEY=your_public_key
LANGFUSE_SECRET_KEY=your_secret_key
LANGFUSE_HOST=https://cloud.langfuse.com

# ì„ë² ë”© ëª¨ë¸ë³„ API í‚¤
UPSTAGE_API_KEY=your_key  # Upstage ì‚¬ìš© ì‹œ
OPENAI_API_KEY=your_key    # OpenAI ì‚¬ìš© ì‹œ
OPENROUTER_API_KEY=your_key  # Qwen ì‚¬ìš© ì‹œ
```

### ë¬¸ì œ í•´ê²°

**ì„ë² ë”© ìºì‹œ ì´ˆê¸°í™”**:
```bash
rm -rf data/embeddings_cache/
```

**Langfuse ì—°ê²° í™•ì¸**:
```bash
python -c "from utils.langfuse_utils import get_langfuse_client; print('âœ…' if get_langfuse_client() else 'âŒ')"
```

### ê´€ë ¨ íŒŒì¼

- **í‰ê°€ ìŠ¤í¬ë¦½íŠ¸**: `evaluators/evaluate_report_types.py`
- **ì„¤ì • íŒŒì¼**: `config/evaluation_config.yaml`
- **í”„ë¡¬í”„íŠ¸**: `prompts/templates/evaluation/`
- **ë¹„êµ ìŠ¤í¬ë¦½íŠ¸**: `evaluators/compare_evaluation_results.py`
- **í…ŒìŠ¤íŠ¸**: `scripts/test_evaluation_setup.py`
