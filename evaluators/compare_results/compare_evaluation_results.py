#!/usr/bin/env python3
"""í‰ê°€ ê²°ê³¼ ë¹„êµ (CSV íŒŒì¼ ê¸°ë°˜)

ë‘ ê°œì˜ Langfuse CSV í‰ê°€ ê²°ê³¼ íŒŒì¼ì„ ë¡œë“œí•˜ì—¬
ë©”íŠ¸ë¦­ ë³„ë¡œ ì„±ëŠ¥ì„ ë¹„êµí•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""

import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

import pandas as pd
import numpy as np
from typing import Dict, Any, List
from collections import defaultdict
import json

# ====================================================================
# ë°ì´í„° ê²½ë¡œ ì„¤ì • (ì—¬ê¸°ë¥¼ ìˆ˜ì •í•˜ì„¸ìš”)
# ====================================================================
# ì²˜ë¦¬í•  í´ë” ë¦¬ìŠ¤íŠ¸
FOLDER_PATHS = [
    "/home/work/rag/Project/rag-report-generator/data/final/bge_m3_rrf_ensemble",
    "/home/work/rag/Project/rag-report-generator/data/final/bge_m3_rrf_multiquery_lc",
    "/home/work/rag/Project/rag-report-generator/data/final/gemini_rrf_multiquery",
    "/home/work/rag/Project/rag-report-generator/data/final/openai_rrf_lc_time",
    "/home/work/rag/Project/rag-report-generator/data/final/openai_rrf_multiquery",
    "/home/work/rag/Project/rag-report-generator/data/final/openai_rrf_multiquery_lc",
    "/home/work/rag/Project/rag-report-generator/data/final/qwen_rrf_ensemble",
    "/home/work/rag/Project/rag-report-generator/data/final/qwen_rrf_multiquery_lc",
    "/home/work/rag/Project/rag-report-generator/data/final/upstage_rrf_ensemble",
    "/home/work/rag/Project/rag-report-generator/data/final/upstage_rrf_multiquery_lc",
]

OUTPUT_PATH = "/home/work/rag/Project/rag-report-generator/data/final/comparison_results/comparison.json"  # ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥


def load_csv_data(csv_path: str) -> pd.DataFrame:
    """
    CSV íŒŒì¼ ë¡œë“œ

    Args:
        csv_path: CSV íŒŒì¼ ê²½ë¡œ

    Returns:
        pandas DataFrame
    """
    print(f"ğŸ“‚ íŒŒì¼ ë¡œë”© ì¤‘: {csv_path}")

    # ì—¬ëŸ¬ ì¸ì½”ë”©ì„ ì‹œë„
    encodings = ['utf-8', 'cp949', 'euc-kr', 'latin1']

    for encoding in encodings:
        try:
            df = pd.read_csv(csv_path, encoding=encoding)
            print(f"   âœ… {len(df)} í–‰ ë¡œë“œë¨ (ì¸ì½”ë”©: {encoding})")
            return df
        except UnicodeDecodeError:
            continue

    # ëª¨ë“  ì¸ì½”ë”© ì‹¤íŒ¨ì‹œ ì—ëŸ¬
    raise ValueError(f"íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹œë„í•œ ì¸ì½”ë”©: {encodings}")



def extract_metrics_by_trace(df: pd.DataFrame) -> Dict[str, Dict[str, float]]:
    """
    Traceë³„ë¡œ ë©”íŠ¸ë¦­ ì¶”ì¶œ

    Args:
        df: Langfuse ë°ì´í„°í”„ë ˆì„

    Returns:
        {trace_id: {metric_name: value}} ë”•ì…”ë„ˆë¦¬
    """
    trace_metrics = defaultdict(dict)

    for _, row in df.iterrows():
        trace_id = row['traceId']
        metric_name = row['name']

        # valueê°€ ìˆ«ìí˜•ì¸ ê²½ìš°ë§Œ ì²˜ë¦¬
        if pd.notna(row['value']):
            try:
                metric_value = float(row['value'])
                trace_metrics[trace_id][metric_name] = metric_value
            except (ValueError, TypeError):
                pass

    return dict(trace_metrics)


def calculate_statistics(values: List[float]) -> Dict[str, float]:
    """
    í†µê³„ ê³„ì‚°

    Args:
        values: ê°’ ë¦¬ìŠ¤íŠ¸

    Returns:
        í†µê³„ ë”•ì…”ë„ˆë¦¬ (í‰ê· , ìµœì†Œ, ìµœëŒ€, ì¤‘ì•™ê°’, í‘œì¤€í¸ì°¨)
    """
    if not values:
        return {
            "count": 0,
            "avg": 0.0,
            "min": 0.0,
            "max": 0.0,
            "median": 0.0,
            "std": 0.0
        }

    return {
        "count": len(values),
        "avg": np.mean(values),
        "min": np.min(values),
        "max": np.max(values),
        "median": np.median(values),
        "std": np.std(values)
    }


def analyze_single_file(df: pd.DataFrame, file_name: str) -> Dict[str, Any]:
    """
    ë‹¨ì¼ CSV íŒŒì¼ ë¶„ì„

    Args:
        df: ë°ì´í„°í”„ë ˆì„
        file_name: íŒŒì¼ ì´ë¦„

    Returns:
        ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    print(f"\n{'=' * 60}")
    print(f"ğŸ“Š {file_name} ë¶„ì„ ì¤‘...")
    print(f"{'=' * 60}")

    # Traceë³„ ë©”íŠ¸ë¦­ ì¶”ì¶œ
    trace_metrics = extract_metrics_by_trace(df)

    # ë©”íŠ¸ë¦­ë³„ í†µê³„ ê³„ì‚°
    metrics_summary = defaultdict(list)

    for trace_id, metrics in trace_metrics.items():
        for metric_name, value in metrics.items():
            metrics_summary[metric_name].append(value)

    # í†µê³„ ê³„ì‚°
    stats = {}
    for metric_name, values in metrics_summary.items():
        stats[metric_name] = calculate_statistics(values)

    # ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ“ˆ ì´ Trace ìˆ˜: {len(trace_metrics)}")
    print(f"ğŸ“ˆ ì´ í‰ê°€ í•­ëª© ìˆ˜: {len(df)}")

    if stats:
        print(f"\në©”íŠ¸ë¦­ í†µê³„:")
        for metric_name, metric_stats in sorted(stats.items()):
            print(f"\n   {metric_name}:")
            print(f"      ê°œìˆ˜: {metric_stats['count']}")
            print(f"      í‰ê· : {metric_stats['avg']:.4f}")
            print(f"      ì¤‘ì•™ê°’: {metric_stats['median']:.4f}")
            print(f"      í‘œì¤€í¸ì°¨: {metric_stats['std']:.4f}")
            print(f"      ë²”ìœ„: {metric_stats['min']:.4f} ~ {metric_stats['max']:.4f}")

    return {
        "file_name": file_name,
        "total_traces": len(trace_metrics),
        "total_evaluations": len(df),
        "metrics": stats,
        "trace_metrics": trace_metrics
    }


def compare_four_files(
    result1: Dict[str, Any],
    result2: Dict[str, Any],
    result3: Dict[str, Any],
    result4: Dict[str, Any]
) -> Dict[str, Any]:
    """
    4ê°œ íŒŒì¼ì˜ ê²°ê³¼ë¥¼ í•œ ë²ˆì— ë¹„êµ ë° ì¶œë ¥

    Args:
        result1: ì²« ë²ˆì§¸ íŒŒì¼ ë¶„ì„ ê²°ê³¼
        result2: ë‘ ë²ˆì§¸ íŒŒì¼ ë¶„ì„ ê²°ê³¼
        result3: ì„¸ ë²ˆì§¸ íŒŒì¼ ë¶„ì„ ê²°ê³¼
        result4: ë„¤ ë²ˆì§¸ íŒŒì¼ ë¶„ì„ ê²°ê³¼

    Returns:
        ë¹„êµ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    results = [result1, result2, result3, result4]
    comparison_data = {}

    print("\n" + "=" * 150)
    print("ğŸ† 4ê°œ íŒŒì¼ ë¹„êµ ê²°ê³¼")
    print("=" * 150)

    # ê¸°ë³¸ í†µê³„ ë¹„êµ
    print(f"\n{'í•­ëª©':<30} {result1['file_name']:<28} {result2['file_name']:<28} {result3['file_name']:<28} {result4['file_name']:<28}")
    print("-" * 150)
    print(f"{'ì´ Trace ìˆ˜':<30} {result1['total_traces']:<28} {result2['total_traces']:<28} {result3['total_traces']:<28} {result4['total_traces']:<28}")
    print(f"{'ì´ í‰ê°€ í•­ëª© ìˆ˜':<30} {result1['total_evaluations']:<28} {result2['total_evaluations']:<28} {result3['total_evaluations']:<28} {result4['total_evaluations']:<28}")

    # ê¸°ë³¸ í†µê³„ ì €ì¥
    comparison_data["basic_stats"] = {
        "files": [r["file_name"] for r in results],
        "total_traces": [r["total_traces"] for r in results],
        "total_evaluations": [r["total_evaluations"] for r in results]
    }

    # ë©”íŠ¸ë¦­ë³„ ë¹„êµ
    all_metrics = set()
    for result in results:
        all_metrics.update(result["metrics"].keys())

    metrics_comparison = {}
    if all_metrics:
        print("\n" + "=" * 150)
        print("ğŸ“Š ë©”íŠ¸ë¦­ë³„ ë¹„êµ")
        print("=" * 150)

        for metric_name in sorted(all_metrics):
            print(f"\n[{metric_name}]")

            # í—¤ë” ì¶œë ¥
            print(f"{'í†µê³„':<20} {result1['file_name']:<28} {result2['file_name']:<28} {result3['file_name']:<28} {result4['file_name']:<28}")
            print("-" * 150)

            # ê° í†µê³„ í•­ëª©ë³„ë¡œ 4ê°œ íŒŒì¼ ë¹„êµ
            stats_list = [result["metrics"].get(metric_name, {}) for result in results]

            # ê°œìˆ˜
            counts = [stats.get('count', 0) for stats in stats_list]
            print(f"{'ê°œìˆ˜':<20} {counts[0]:<28} {counts[1]:<28} {counts[2]:<28} {counts[3]:<28}")

            # í‰ê· 
            avgs = [stats.get('avg', 0) for stats in stats_list]
            avg_str = [f"{avg:.4f}" for avg in avgs]
            print(f"{'í‰ê· ':<20} {avg_str[0]:<28} {avg_str[1]:<28} {avg_str[2]:<28} {avg_str[3]:<28}")

            # ìµœê³  í‰ê·  ì°¾ê¸°
            best_file = None
            if any(stats_list):
                max_avg = max(avgs)
                max_idx = avgs.index(max_avg)
                best_file = results[max_idx]['file_name']
                print(f"{'  â†’ ìµœê³  í‰ê· ':<20} {best_file} ({max_avg:.4f})")

            # ì¤‘ì•™ê°’
            medians = [stats.get('median', 0) for stats in stats_list]
            med_str = [f"{med:.4f}" for med in medians]
            print(f"{'ì¤‘ì•™ê°’':<20} {med_str[0]:<28} {med_str[1]:<28} {med_str[2]:<28} {med_str[3]:<28}")

            # í‘œì¤€í¸ì°¨
            stds = [stats.get('std', 0) for stats in stats_list]
            std_str = [f"{std:.4f}" for std in stds]
            print(f"{'í‘œì¤€í¸ì°¨':<20} {std_str[0]:<28} {std_str[1]:<28} {std_str[2]:<28} {std_str[3]:<28}")

            # ìµœì†Œê°’
            mins = [stats.get('min', 0) for stats in stats_list]
            min_str = [f"{m:.4f}" for m in mins]
            print(f"{'ìµœì†Œê°’':<20} {min_str[0]:<28} {min_str[1]:<28} {min_str[2]:<28} {min_str[3]:<28}")

            # ìµœëŒ€ê°’
            maxs = [stats.get('max', 0) for stats in stats_list]
            max_str = [f"{m:.4f}" for m in maxs]
            print(f"{'ìµœëŒ€ê°’':<20} {max_str[0]:<28} {max_str[1]:<28} {max_str[2]:<28} {max_str[3]:<28}")

            # ë©”íŠ¸ë¦­ ë¹„êµ ë°ì´í„° ì €ì¥
            metrics_comparison[metric_name] = {
                "count": counts,
                "avg": avgs,
                "median": medians,
                "std": stds,
                "min": mins,
                "max": maxs,
                "best_file": best_file
            }

    comparison_data["metrics_comparison"] = metrics_comparison

    # Trace ë¹„êµ
    all_traces = [set(result["trace_metrics"].keys()) for result in results]
    common_traces = all_traces[0].intersection(*all_traces[1:])

    print("\n" + "=" * 150)
    print("ğŸ” Trace ë¹„êµ")
    print("=" * 150)
    print(f"ì „ì²´ ê³µí†µ Trace ìˆ˜: {len(common_traces)}")
    for i, result in enumerate(results, 1):
        print(f"{result['file_name']}: {len(all_traces[i-1])} Traces")

    # Trace ë¹„êµ ë°ì´í„° ì €ì¥
    comparison_data["trace_comparison"] = {
        "common_traces_count": len(common_traces),
        "traces_per_file": [len(traces) for traces in all_traces]
    }

    # ê³µí†µ Traceì— ëŒ€í•œ ë©”íŠ¸ë¦­ ìˆœìœ„ ë¶„ì„
    ranking = {}
    if common_traces and all_metrics:
        print("\n" + "=" * 150)
        print("ğŸ“ˆ ê³µí†µ Trace ë©”íŠ¸ë¦­ ìˆœìœ„ (í‰ê·  ê¸°ì¤€)")
        print("=" * 150)

        for metric_name in sorted(all_metrics):
            metric_avgs = []
            for result in results:
                if metric_name in result["metrics"]:
                    metric_avgs.append({
                        "file_name": result["file_name"],
                        "avg": result["metrics"][metric_name].get("avg", 0)
                    })

            # í‰ê· ê°’ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
            metric_avgs.sort(key=lambda x: x["avg"], reverse=True)
            ranking[metric_name] = metric_avgs

        for metric_name, ranks in ranking.items():
            print(f"\n{metric_name}:")
            for i, rank_data in enumerate(ranks, 1):
                print(f"   {i}ìœ„: {rank_data['file_name']:<30} (í‰ê· : {rank_data['avg']:.4f})")

    comparison_data["ranking"] = ranking

    return comparison_data


def compare_two_files(
    result1: Dict[str, Any],
    result2: Dict[str, Any]
) -> None:
    """
    ë‘ íŒŒì¼ì˜ ê²°ê³¼ ë¹„êµ ë° ì¶œë ¥

    Args:
        result1: ì²« ë²ˆì§¸ íŒŒì¼ ë¶„ì„ ê²°ê³¼
        result2: ë‘ ë²ˆì§¸ íŒŒì¼ ë¶„ì„ ê²°ê³¼
    """
    print("\n" + "=" * 80)
    print("ğŸ† ë‘ íŒŒì¼ ë¹„êµ ê²°ê³¼")
    print("=" * 80)

    # ê¸°ë³¸ í†µê³„ ë¹„êµ
    print(f"\n{'í•­ëª©':<30} {result1['file_name']:<25} {result2['file_name']:<25}")
    print("-" * 80)
    print(f"{'ì´ Trace ìˆ˜':<30} {result1['total_traces']:<25} {result2['total_traces']:<25}")
    print(f"{'ì´ í‰ê°€ í•­ëª© ìˆ˜':<30} {result1['total_evaluations']:<25} {result2['total_evaluations']:<25}")

    # ë©”íŠ¸ë¦­ë³„ ë¹„êµ
    all_metrics = set(result1["metrics"].keys()) | set(result2["metrics"].keys())

    if all_metrics:
        print("\n" + "=" * 80)
        print("ğŸ“Š ë©”íŠ¸ë¦­ë³„ ë¹„êµ")
        print("=" * 80)

        for metric_name in sorted(all_metrics):
            print(f"\n[{metric_name}]")
            print(f"{'í†µê³„':<20} {result1['file_name']:<25} {result2['file_name']:<25} {'ì°¨ì´':<15}")
            print("-" * 85)

            stats1 = result1["metrics"].get(metric_name, {})
            stats2 = result2["metrics"].get(metric_name, {})

            if stats1 and stats2:
                # ê°œìˆ˜
                print(f"{'ê°œìˆ˜':<20} {stats1.get('count', 0):<25} {stats2.get('count', 0):<25} {stats2.get('count', 0) - stats1.get('count', 0):<15}")

                # í‰ê· 
                avg1 = stats1.get('avg', 0)
                avg2 = stats2.get('avg', 0)
                diff = avg2 - avg1
                diff_pct = (diff / avg1 * 100) if avg1 != 0 else 0
                print(f"{'í‰ê· ':<20} {avg1:<25.4f} {avg2:<25.4f} {diff:+.4f} ({diff_pct:+.2f}%)")

                # ì¤‘ì•™ê°’
                med1 = stats1.get('median', 0)
                med2 = stats2.get('median', 0)
                diff = med2 - med1
                print(f"{'ì¤‘ì•™ê°’':<20} {med1:<25.4f} {med2:<25.4f} {diff:+.4f}")

                # í‘œì¤€í¸ì°¨
                std1 = stats1.get('std', 0)
                std2 = stats2.get('std', 0)
                diff = std2 - std1
                print(f"{'í‘œì¤€í¸ì°¨':<20} {std1:<25.4f} {std2:<25.4f} {diff:+.4f}")

                # ìµœì†Œ/ìµœëŒ€
                print(f"{'ìµœì†Œê°’':<20} {stats1.get('min', 0):<25.4f} {stats2.get('min', 0):<25.4f}")
                print(f"{'ìµœëŒ€ê°’':<20} {stats1.get('max', 0):<25.4f} {stats2.get('max', 0):<25.4f}")
            else:
                if stats1:
                    print(f"   âš ï¸  {result2['file_name']}ì— '{metric_name}' ë©”íŠ¸ë¦­ ì—†ìŒ")
                else:
                    print(f"   âš ï¸  {result1['file_name']}ì— '{metric_name}' ë©”íŠ¸ë¦­ ì—†ìŒ")

    # ê³µí†µ Trace ë¶„ì„
    traces1 = set(result1["trace_metrics"].keys())
    traces2 = set(result2["trace_metrics"].keys())

    common_traces = traces1 & traces2
    only_in_1 = traces1 - traces2
    only_in_2 = traces2 - traces1

    print("\n" + "=" * 80)
    print("ğŸ” Trace ë¹„êµ")
    print("=" * 80)
    print(f"ê³µí†µ Trace ìˆ˜: {len(common_traces)}")
    print(f"{result1['file_name']}ì—ë§Œ ìˆëŠ” Trace: {len(only_in_1)}")
    print(f"{result2['file_name']}ì—ë§Œ ìˆëŠ” Trace: {len(only_in_2)}")

    # ê³µí†µ Traceì— ëŒ€í•œ ë©”íŠ¸ë¦­ ì°¨ì´ ë¶„ì„
    if common_traces and all_metrics:
        print("\n" + "=" * 80)
        print("ğŸ“ˆ ê³µí†µ Trace ë©”íŠ¸ë¦­ ê°œì„ /ì €í•˜ ë¶„ì„")
        print("=" * 80)

        for metric_name in sorted(all_metrics):
            improvements = 0
            degradations = 0
            unchanged = 0

            for trace_id in common_traces:
                val1 = result1["trace_metrics"][trace_id].get(metric_name)
                val2 = result2["trace_metrics"][trace_id].get(metric_name)

                if val1 is not None and val2 is not None:
                    if val2 > val1:
                        improvements += 1
                    elif val2 < val1:
                        degradations += 1
                    else:
                        unchanged += 1

            if improvements + degradations + unchanged > 0:
                print(f"\n{metric_name}:")
                print(f"   ê°œì„ : {improvements} ({improvements/len(common_traces)*100:.1f}%)")
                print(f"   ì €í•˜: {degradations} ({degradations/len(common_traces)*100:.1f}%)")
                print(f"   ë™ì¼: {unchanged} ({unchanged/len(common_traces)*100:.1f}%)")


def get_csv_files_from_folder(folder_path: str) -> List[str]:
    """
    í´ë”ì—ì„œ ëª¨ë“  CSV íŒŒì¼ ì°¾ê¸°

    Args:
        folder_path: í´ë” ê²½ë¡œ

    Returns:
        CSV íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ (top6, top8, top10, top12 ìˆœì„œë¡œ ì •ë ¬)
    """
    folder = Path(folder_path)
    if not folder.exists():
        print(f"âš ï¸  í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {folder_path}")
        return []

    csv_files = sorted(folder.glob("*.csv"))

    # top6, top8, top10, top12 ìˆœì„œë¡œ ì •ë ¬
    def sort_key(file_path):
        name = file_path.stem
        if 'top6' in name:
            return 0
        elif 'top8' in name:
            return 1
        elif 'top10' in name:
            return 2
        elif 'top12' in name:
            return 3
        else:
            return 4

    csv_files = sorted(csv_files, key=sort_key)

    return [str(f) for f in csv_files]


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    output_path = OUTPUT_PATH
    all_folder_results = {}  # ëª¨ë“  í´ë”ì˜ ë¹„êµ ê²°ê³¼ë¥¼ ì €ì¥

    # ê° í´ë” ì²˜ë¦¬
    for folder_path in FOLDER_PATHS:
        folder_name = Path(folder_path).name
        print("\n" + "=" * 150)
        print(f"ğŸ—‚ï¸  í´ë” ì²˜ë¦¬ ì¤‘: {folder_name}")
        print("=" * 150)

        # í´ë”ì—ì„œ CSV íŒŒì¼ ì°¾ê¸°
        csv_files = get_csv_files_from_folder(folder_path)

        if len(csv_files) != 4:
            print(f"âš ï¸  {len(csv_files)}ê°œì˜ CSV íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤. 4ê°œê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            print(f"   ì°¾ì€ íŒŒì¼ë“¤: {[Path(f).name for f in csv_files]}")
            continue

        # CSV íŒŒì¼ ë¡œë“œ
        dfs = []
        for csv_file in csv_files:
            try:
                df = load_csv_data(csv_file)
                dfs.append(df)
            except Exception as e:
                print(f"âŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {csv_file}")
                print(f"   ì—ëŸ¬: {e}")
                break

        if len(dfs) != 4:
            print(f"âš ï¸  íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            continue

        # ê° íŒŒì¼ ë¶„ì„
        results = []
        for i, (df, csv_file) in enumerate(zip(dfs, csv_files)):
            result = analyze_single_file(df, Path(csv_file).name)
            results.append(result)

        # 4ê°œ íŒŒì¼ ë¹„êµ (ë¹„êµ ê²°ê³¼ ë°˜í™˜ë°›ìŒ)
        comparison_result = compare_four_files(results[0], results[1], results[2], results[3])

        # í´ë”ë³„ ê²°ê³¼ ì €ì¥
        all_folder_results[folder_name] = comparison_result

    # ëª¨ë“  í´ë”ì˜ ë¹„êµ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ JSON íŒŒì¼ë¡œ ì €ì¥
    if output_path and all_folder_results:
        output_path_obj = Path(output_path)
        output_path_obj.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path_obj, "w", encoding="utf-8") as f:
            json.dump(all_folder_results, f, indent=2, ensure_ascii=False, default=str)

        print(f"\nğŸ’¾ ì „ì²´ ë¹„êµ ê²°ê³¼ ì €ì¥: {output_path_obj}")
        print(f"   ì´ {len(all_folder_results)}ê°œ í´ë”ì˜ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
