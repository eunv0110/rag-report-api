#!/usr/bin/env python3
"""ì£¼ê°„ ë³´ê³ ì„œ vs ì„ì› ë³´ê³ ì„œ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

ê° ë³´ê³ ì„œ íƒ€ì…ì— ìµœì í™”ëœ ë¦¬íŠ¸ë¦¬ë²„ ì¡°í•©ì„ í‰ê°€í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    # ëª¨ë“  í‰ê°€ ì‹¤í–‰ (ì£¼ê°„ + ì„ì›)
    python evaluators/evaluate_report_types.py --report-type both

    # ì£¼ê°„ ë³´ê³ ì„œë§Œ í‰ê°€
    python evaluators/evaluate_report_types.py --report-type weekly

    # ì„ì› ë³´ê³ ì„œë§Œ í‰ê°€
    python evaluators/evaluate_report_types.py --report-type executive

    # íŠ¹ì • ë¦¬íŠ¸ë¦¬ë²„ ì¡°í•©ë§Œ í‰ê°€
    python evaluators/evaluate_report_types.py --report-type weekly --retrievers upstage_rrf_multiquery_lc
"""

import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

from dotenv import load_dotenv
load_dotenv()

import json
import time
import os
import yaml
from datetime import datetime
from typing import List, Dict, Any, Optional
from langchain.chat_models import init_chat_model

from config.settings import (
    AZURE_AI_CREDENTIAL,
    AZURE_AI_ENDPOINT,
)
from retrievers.ensemble_retriever import get_ensemble_retriever
from retrievers.multiquery_retriever import get_multiquery_retriever
from retrievers.longcontext_retriever import get_longcontext_retriever
from retrievers.timeweighted_retriever import get_time_weighted_retriever
from utils.langfuse_utils import get_langfuse_client
from models.embeddings.factory import get_embedder

# ìƒìˆ˜ ì •ì˜
DEFAULT_DATASET_PATH = "/home/work/rag/Project/rag-report-generator/data/evaluation/merged_qa_dataset.json"
DEFAULT_NUM_CONTEXTS_FOR_ANSWER = 5
DEFAULT_TEMPERATURE = 0.1
DEFAULT_MAX_TOKENS = 1000
DEFAULT_TOP_K = 10


def load_evaluation_config() -> Dict[str, Any]:
    """í‰ê°€ ì„¤ì • íŒŒì¼ ë¡œë“œ"""
    config_path = Path(__file__).parent.parent / "config" / "evaluation_config.yaml"
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def load_prompt(prompt_file: str) -> str:
    """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¡œë“œ"""
    prompt_path = Path(__file__).parent.parent / prompt_file
    if not prompt_path.exists():
        raise FileNotFoundError(f"í”„ë¡¬í”„íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {prompt_path}")

    with open(prompt_path, "r", encoding="utf-8") as f:
        return f.read().strip()


def load_evaluation_dataset(file_path: str) -> List[Dict[str, Any]]:
    """í‰ê°€ìš© ë°ì´í„°ì…‹ ë¡œë“œ"""
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def generate_llm_answer(
    question: str,
    contexts: List[str],
    system_prompt: str,
    answer_generation_prompt: str
) -> str:
    """LLM APIë¥¼ í˜¸ì¶œí•˜ì—¬ ë‹µë³€ ìƒì„±"""
    if not AZURE_AI_CREDENTIAL or not AZURE_AI_ENDPOINT:
        return "Azure OpenAI ì„¤ì •ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”."

    os.environ['AZURE_AI_CREDENTIAL'] = AZURE_AI_CREDENTIAL
    os.environ['AZURE_AI_ENDPOINT'] = AZURE_AI_ENDPOINT

    context_text = "\n\n".join(contexts[:DEFAULT_NUM_CONTEXTS_FOR_ANSWER]) if contexts else "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    prompt = answer_generation_prompt.replace("{context}", context_text).replace("{question}", question)

    try:
        model = init_chat_model(
            "azure_ai:gpt-4.1",
            temperature=DEFAULT_TEMPERATURE,
            max_completion_tokens=DEFAULT_MAX_TOKENS
        )

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": prompt}
        ]

        response = model.invoke(messages)
        return response.content
    except Exception as e:
        error_msg = f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {str(e)}"
        print(f"  âš ï¸ LLM API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return error_msg


def create_trace_and_generation(
    langfuse,
    retriever_config: Dict[str, Any],
    report_type: str,
    question: str,
    contexts: List[str],
    answer: str,
    ground_truth: str,
    context_metadata: List[Dict],
    item_metadata: Dict,
    total_time: float,
    idx: int,
    top_k: int,
    context_page_id: Optional[str] = None,
    version_tag: str = "v1"
) -> str:
    """Langfuse Traceì™€ Generation ìƒì„±"""
    context_text = "\n\n---\n\n".join(contexts) if contexts else ""

    retriever_name = retriever_config['name']
    display_name = retriever_config['display_name']

    all_tags = [
        f"{retriever_name}_{version_tag}",
        f"{report_type}",
        f"top_k_{top_k}",
        version_tag,
        "evaluation",
        retriever_config['embedding_preset'],
        retriever_config['retriever_type']
    ]

    with langfuse.start_as_current_observation(
        as_type='generation',
        name=f"generation_{retriever_name}_{report_type}_{version_tag}",
        model="gpt-4.1",
        input={
            "question": question,
            "context": context_text
        },
        output={
            "answer": answer
        },
        metadata={
            "ground_truth": ground_truth,
            "contexts": contexts,
            "context_metadata": context_metadata,
            "retriever_name": retriever_name,
            "report_type": report_type,
            "version": version_tag,
            "top_k": top_k,
            "embedding_preset": retriever_config['embedding_preset'],
            "retriever_type": retriever_config['retriever_type']
        }
    ) as generation:
        trace_id = generation.trace_id

        langfuse.update_current_trace(
            name=f"eval_{retriever_name}_{report_type}_{version_tag}_q{idx}",
            tags=all_tags,
            input={
                "question": question,
                "context": context_text
            },
            output={
                "answer": answer
            },
            metadata={
                "retriever_name": retriever_name,
                "display_name": display_name,
                "report_type": report_type,
                "version": version_tag,
                "top_k": top_k,
                "total_time_ms": total_time * 1000,
                "num_retrieved_contexts": len(contexts),
                "context_page_id": context_page_id,
                "question_id": idx,
                "category": item_metadata.get("category", "unknown"),
                "difficulty": item_metadata.get("difficulty", "unknown"),
                "embedding_preset": retriever_config['embedding_preset'],
                "retriever_type": retriever_config['retriever_type']
            }
        )

    print(f"\n[DEBUG] Trace {idx}:")
    print(f"  - ID: {trace_id}")
    print(f"  - Question: {question[:50]}...")
    print(f"  - Context length: {len(context_text)} chars")
    print(f"  - Answer length: {len(answer)} chars")

    return trace_id


def add_retrieval_quality_score(langfuse, trace_id: str, context_metadata: List[Dict]):
    """ê²€ìƒ‰ í’ˆì§ˆ ìŠ¤ì½”ì–´ ì¶”ê°€"""
    if not context_metadata:
        return

    scores = [m.get("score") for m in context_metadata if m.get("score") is not None]
    if scores:
        avg_score = sum(scores) / len(scores)
        langfuse.create_score(
            trace_id=trace_id,
            name="retrieval_quality",
            value=avg_score,
            comment=f"Average retrieval score from {len(scores)} contexts"
        )


def create_retriever_from_config(retriever_config: Dict[str, Any], top_k: int = 10):
    """ì„¤ì •ì—ì„œ ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±

    Args:
        retriever_config: ë¦¬íŠ¸ë¦¬ë²„ ì„¤ì • ë”•ì…”ë„ˆë¦¬
        top_k: Top-K ê°’ (retriever_configì— 'k' ê°’ì´ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ìš°ì„  ì‚¬ìš©)

    Returns:
        (retriever, retriever_tags)
    """
    retriever_type = retriever_config['retriever_type']
    embedding_preset = retriever_config['embedding_preset']

    # retriever_configì—ì„œ k ê°’ ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ top_k ì‚¬ìš©)
    k = retriever_config.get('k', top_k)

    print(f"   ğŸ“Š Top-K ì„¤ì •: {k}")

    # í™˜ê²½ ë³€ìˆ˜ë¡œ ì„ë² ë”© í”„ë¦¬ì…‹ ì„¤ì •
    os.environ['MODEL_PRESET'] = embedding_preset

    if retriever_type == "rrf_multiquery_longcontext":
        # RRF + MultiQuery + LongContext
        base_retriever = get_ensemble_retriever(k=k)
        multiquery_retriever = get_multiquery_retriever(
            base_retriever=base_retriever,
            num_queries=3,
            k=k
        )
        retriever = get_longcontext_retriever(
            base_retriever=multiquery_retriever,
            k=k
        )
        retriever_tags = ["longcontext", "multiquery", "ensemble", "rrf"]

    elif retriever_type == "rrf_ensemble":
        # RRF Ensemble
        retriever = get_ensemble_retriever(k=k)
        retriever_tags = ["ensemble", "rrf", "bm25", "dense"]

    elif retriever_type == "rrf_multiquery":
        # RRF + MultiQuery
        base_retriever = get_ensemble_retriever(k=k)
        retriever = get_multiquery_retriever(
            base_retriever=base_retriever,
            num_queries=3,
            k=k
        )
        retriever_tags = ["multiquery", "ensemble", "rrf"]

    elif retriever_type == "rrf_longcontext_timeweighted":
        # RRF + LongContext + TimeWeighted
        time_weighted = get_time_weighted_retriever(
            decay_rate=0.01,
            k=k
        )
        retriever = get_longcontext_retriever(
            base_retriever=time_weighted,
            k=k
        )
        retriever_tags = ["longcontext", "time_weighted", "decay_0.01"]

    else:
        raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ë¦¬íŠ¸ë¦¬ë²„ íƒ€ì…: {retriever_type}")

    return retriever, retriever_tags


def evaluate_single_query(
    retriever,
    retriever_config: Dict[str, Any],
    report_type: str,
    item: Dict[str, Any],
    langfuse,
    idx: int,
    top_k: int,
    system_prompt: str,
    answer_generation_prompt: str,
    version_tag: str = "v1"
) -> Dict[str, Any]:
    """ë‹¨ì¼ ì¿¼ë¦¬ í‰ê°€"""
    question = item["question"]
    ground_truth = item["ground_truth"]
    context_page_id = item.get("context_page_id")
    item_metadata = item.get("metadata", {})

    start_time = time.time()

    # ê²€ìƒ‰ ìˆ˜í–‰
    search_results = retriever.invoke(question)

    # LangChain Documentë¥¼ contextsë¡œ ë³€í™˜
    contexts = []
    context_metadata = []

    for result in search_results[:top_k]:
        contexts.append(result.page_content)
        context_metadata.append({
            "page_title": result.metadata.get('page_title', 'Unknown'),
            "section_title": result.metadata.get('section_title', 'N/A'),
            "chunk_id": result.metadata.get('chunk_id', 'unknown'),
            "score": result.metadata.get('_combined_score') or result.metadata.get('_similarity_score')
        })

    if not contexts:
        print(f"  âš ï¸ [{idx}] No contexts found for question!")
        contexts = ["ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."]

    # LLM ë‹µë³€ ìƒì„±
    answer = generate_llm_answer(question, contexts, system_prompt, answer_generation_prompt)

    if not answer or answer.startswith("ë‹µë³€ ìƒì„± ì‹¤íŒ¨") or answer.startswith("Azure OpenAI ì„¤ì •"):
        print(f"  âš ï¸ [{idx}] LLM answer generation failed!")
        if not answer:
            answer = "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    total_time = time.time() - start_time

    # Langfuse Trace & Generation
    trace_id = create_trace_and_generation(
        langfuse=langfuse,
        retriever_config=retriever_config,
        report_type=report_type,
        question=question,
        contexts=contexts,
        answer=answer,
        ground_truth=ground_truth,
        context_metadata=context_metadata,
        item_metadata=item_metadata,
        total_time=total_time,
        idx=idx,
        top_k=top_k,
        context_page_id=context_page_id,
        version_tag=version_tag
    )

    # ê²€ìƒ‰ í’ˆì§ˆ ìŠ¤ì½”ì–´ ì¶”ê°€
    add_retrieval_quality_score(langfuse, trace_id, context_metadata)

    print(f"  [{idx}] {question[:50]}... ({len(contexts)}ê°œ ë¬¸ì„œ, {total_time*1000:.0f}ms)")

    return {
        "question": question,
        "answer": answer,
        "ground_truth": ground_truth,
        "num_contexts": len(contexts),
        "time": total_time,
        "trace_id": trace_id
    }


def evaluate_retriever(
    retriever,
    retriever_config: Dict[str, Any],
    report_type: str,
    eval_data: List[Dict[str, Any]],
    langfuse,
    top_k: int,
    system_prompt: str,
    answer_generation_prompt: str,
    version_tag: str = "v1"
) -> Dict[str, Any]:
    """ë¦¬íŠ¸ë¦¬ë²„ í‰ê°€"""
    print(f"\n{'=' * 80}")
    print(f"ğŸ” {retriever_config['display_name']} - {report_type} í‰ê°€ ì¤‘...")
    print(f"{'=' * 80}")

    stats = {
        "total_queries": len(eval_data),
        "total_time": 0,
        "evaluations": []
    }

    for idx, item in enumerate(eval_data, 1):
        eval_result = evaluate_single_query(
            retriever=retriever,
            retriever_config=retriever_config,
            report_type=report_type,
            item=item,
            langfuse=langfuse,
            idx=idx,
            top_k=top_k,
            system_prompt=system_prompt,
            answer_generation_prompt=answer_generation_prompt,
            version_tag=version_tag
        )

        stats["evaluations"].append(eval_result)
        stats["total_time"] += eval_result["time"]

        # ìºì‹œ ì €ì¥
        try:
            embedder = get_embedder()
            if hasattr(embedder, 'use_cache') and embedder.use_cache and embedder.cache:
                embedder.cache.save()
        except:
            pass

    stats["avg_time"] = stats["total_time"] / stats["total_queries"]
    stats["avg_contexts"] = sum(e["num_contexts"] for e in stats["evaluations"]) / stats["total_queries"]

    return stats


def run_report_evaluation(
    report_type: str,
    config: Dict[str, Any],
    dataset_path: str,
    top_k: int,
    version: str,
    langfuse,
    selected_retrievers: List[str] = None
) -> Dict[str, Any]:
    """íŠ¹ì • ë³´ê³ ì„œ íƒ€ì…ì— ëŒ€í•œ í‰ê°€ ì‹¤í–‰

    Args:
        report_type: 'weekly_report' or 'executive_report'
        config: ì „ì²´ ì„¤ì • ë”•ì…”ë„ˆë¦¬
        dataset_path: í‰ê°€ ë°ì´í„°ì…‹ ê²½ë¡œ
        top_k: Top-K ê°’
        version: ë²„ì „ íƒœê·¸
        langfuse: Langfuse í´ë¼ì´ì–¸íŠ¸
        selected_retrievers: í‰ê°€í•  ë¦¬íŠ¸ë¦¬ë²„ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ëª¨ë‘)

    Returns:
        í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    report_config = config[report_type]

    print("\n" + "=" * 80)
    print(f"ğŸ“Š {report_config['name']} í‰ê°€ ì‹œì‘")
    print("=" * 80)
    print(f"ìš°ì„ ìˆœìœ„: {' > '.join(report_config['priority'])}")

    # í”„ë¡¬í”„íŠ¸ ë¡œë“œ
    system_prompt = load_prompt(report_config['system_prompt_path'])
    answer_generation_prompt = load_prompt(report_config['answer_generation_prompt_path'])

    # ë°ì´í„°ì…‹ ë¡œë“œ
    eval_data = load_evaluation_dataset(dataset_path)
    print(f"ğŸ“‹ ë°ì´í„°ì…‹: {len(eval_data)} ê°œ ìƒ˜í”Œ")

    # í‰ê°€í•  ë¦¬íŠ¸ë¦¬ë²„ í•„í„°ë§
    retrievers_to_eval = report_config['retrievers']
    if selected_retrievers:
        retrievers_to_eval = [
            r for r in retrievers_to_eval
            if r['name'] in selected_retrievers
        ]

    print(f"ğŸ” í‰ê°€ ëŒ€ìƒ: {len(retrievers_to_eval)}ê°œ ë¦¬íŠ¸ë¦¬ë²„\n")

    results = {}

    for retriever_config in retrievers_to_eval:
        # Top-K ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ ì»¤ë§¨ë“œë¼ì¸ top_k ì‚¬ìš©)
        top_k_list = retriever_config.get('top_k_list', [top_k])

        for current_top_k in top_k_list:
            print(f"\n{'=' * 80}")
            print(f"ğŸš€ {retriever_config['display_name']}")
            print(f"   ì„ë² ë”©: {retriever_config['embedding_preset']}")
            print(f"   ë¦¬íŠ¸ë¦¬ë²„: {retriever_config['retriever_type']}")
            print(f"   Top-K: {current_top_k}")
            print(f"   {retriever_config['description']}")
            print(f"{'=' * 80}")

            try:
                # ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±
                retriever, retriever_tags = create_retriever_from_config(retriever_config, current_top_k)

                # í‰ê°€ ìˆ˜í–‰
                stats = evaluate_retriever(
                    retriever=retriever,
                    retriever_config=retriever_config,
                    report_type=report_type,
                    eval_data=eval_data,
                    langfuse=langfuse,
                    top_k=current_top_k,
                    system_prompt=system_prompt,
                    answer_generation_prompt=answer_generation_prompt,
                    version_tag=version
                )

                # ê²°ê³¼ ì €ì¥
                output_dir = Path(config['evaluation']['output_dir']) / report_type
                output_dir.mkdir(parents=True, exist_ok=True)

                # íŒŒì¼ëª…ì— top-k í¬í•¨
                output_file = output_dir / f"{retriever_config['name']}_k{current_top_k}_stats.json"
                save_result = {k: v for k, v in stats.items() if k != "evaluations"}
                save_result["num_evaluations"] = len(stats.get("evaluations", []))
                save_result["config"] = {
                    "retriever_name": retriever_config['name'],
                    "display_name": retriever_config['display_name'],
                    "report_type": report_type,
                    "embedding_preset": retriever_config['embedding_preset'],
                    "retriever_type": retriever_config['retriever_type'],
                    "top_k": current_top_k,
                    "version": version,
                }

                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(save_result, f, indent=2, ensure_ascii=False, default=str)

                print(f"\nâœ… í‰ê°€ ì™„ë£Œ (Top-K={current_top_k})")
                print(f"   - í‰ê·  ì‹œê°„: {stats['avg_time']*1000:.2f}ms")
                print(f"   - í‰ê·  ì»¨í…ìŠ¤íŠ¸ ìˆ˜: {stats['avg_contexts']:.2f}")
                print(f"   - ê²°ê³¼ ì €ì¥: {output_file}")

                result_key = f"{retriever_config['name']}_k{current_top_k}"
                results[result_key] = {
                    "success": True,
                    "stats": stats,
                    "output_file": str(output_file),
                    "top_k": current_top_k
                }

            except Exception as e:
                print(f"âŒ í‰ê°€ ì‹¤íŒ¨ (Top-K={current_top_k}): {e}")
                import traceback
                traceback.print_exc()

                result_key = f"{retriever_config['name']}_k{current_top_k}"
                results[result_key] = {
                    "success": False,
                    "error": str(e),
                    "top_k": current_top_k
                }

    return results


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse

    parser = argparse.ArgumentParser(
        description="ì£¼ê°„ ë³´ê³ ì„œ vs ì„ì› ë³´ê³ ì„œ í‰ê°€",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )

    parser.add_argument(
        "--report-type",
        type=str,
        choices=["weekly", "executive", "both"],
        default="both",
        help="í‰ê°€í•  ë³´ê³ ì„œ íƒ€ì… (ê¸°ë³¸ê°’: both)"
    )

    parser.add_argument(
        "--retrievers",
        type=str,
        nargs="+",
        help="í‰ê°€í•  ë¦¬íŠ¸ë¦¬ë²„ ì´ë¦„ (ë¯¸ì§€ì • ì‹œ ëª¨ë‘ í‰ê°€)"
    )

    parser.add_argument(
        "--dataset",
        type=str,
        default=DEFAULT_DATASET_PATH,
        help="í‰ê°€ ë°ì´í„°ì…‹ ê²½ë¡œ"
    )

    parser.add_argument(
        "--top-k",
        type=int,
        default=DEFAULT_TOP_K,
        help="Top-K ê°’ (ê¸°ë³¸ê°’: 10)"
    )

    parser.add_argument(
        "--version",
        type=str,
        default="v1",
        help="ë²„ì „ íƒœê·¸ (ê¸°ë³¸ê°’: v1)"
    )

    args = parser.parse_args()

    # ì„¤ì • ë¡œë“œ
    config = load_evaluation_config()

    print("=" * 80)
    print("ğŸ¯ ì£¼ê°„ ë³´ê³ ì„œ vs ì„ì› ë³´ê³ ì„œ í‰ê°€ ì‹œìŠ¤í…œ")
    print("=" * 80)
    print(f"\nğŸ“Š í‰ê°€ ì„¤ì •:")
    print(f"   - ë³´ê³ ì„œ íƒ€ì…: {args.report_type}")
    print(f"   - Dataset: {args.dataset}")
    print(f"   - Top-K: {args.top_k}")
    print(f"   - Version: {args.version}")
    if args.retrievers:
        print(f"   - ì„ íƒëœ ë¦¬íŠ¸ë¦¬ë²„: {', '.join(args.retrievers)}")

    # Langfuse í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    langfuse = get_langfuse_client()
    if not langfuse:
        print("âŒ Langfuse í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # í‰ê°€ ì‹¤í–‰
    all_results = {}

    if args.report_type in ["weekly", "both"]:
        weekly_results = run_report_evaluation(
            report_type="weekly_report",
            config=config,
            dataset_path=args.dataset,
            top_k=args.top_k,
            version=args.version,
            langfuse=langfuse,
            selected_retrievers=args.retrievers
        )
        all_results["weekly_report"] = weekly_results

    if args.report_type in ["executive", "both"]:
        exec_results = run_report_evaluation(
            report_type="executive_report",
            config=config,
            dataset_path=args.dataset,
            top_k=args.top_k,
            version=args.version,
            langfuse=langfuse,
            selected_retrievers=args.retrievers
        )
        all_results["executive_report"] = exec_results

    # Langfuse flush
    print("\nâ³ Langfuseì— ë°ì´í„° ì „ì†¡ ì¤‘...")
    langfuse.flush()

    # ì„ë² ë”© ìºì‹œ ì €ì¥
    print("\nğŸ’¾ ì„ë² ë”© ìºì‹œ ì €ì¥ ì¤‘...")
    try:
        embedder = get_embedder()
        if hasattr(embedder, 'save_cache'):
            embedder.save_cache()
        else:
            print("  â„¹ï¸  ìºì‹œ ì €ì¥ ë©”ì„œë“œ ì—†ìŒ")
    except Exception as e:
        print(f"  âš ï¸  ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 80)
    print("ğŸ“Š í‰ê°€ ê²°ê³¼ ìš”ì•½")
    print("=" * 80)

    for report_type, results in all_results.items():
        report_name = "ì£¼ê°„ ë³´ê³ ì„œ (ìš´ì˜íŒ€)" if report_type == "weekly_report" else "ì„ì› ë³´ê³ ì„œ (ì˜ì‚¬ê²°ì •)"
        print(f"\n[{report_name}]")

        for retriever_name, result in results.items():
            status = "âœ… ì„±ê³µ" if result['success'] else "âŒ ì‹¤íŒ¨"
            print(f"  {retriever_name}: {status}")

    print("\n" + "=" * 80)
    print("âœ… ëª¨ë“  í‰ê°€ ì™„ë£Œ!")
    print("=" * 80)

    print("\nğŸ“Š ë‹¤ìŒ ë‹¨ê³„:")
    print("   1. Langfuse ëŒ€ì‹œë³´ë“œì—ì„œ ê²°ê³¼ í™•ì¸: https://cloud.langfuse.com")
    print("   2. ê²°ê³¼ íŒŒì¼ í™•ì¸:")
    print(f"      - {config['evaluation']['output_dir']}/weekly_report/")
    print(f"      - {config['evaluation']['output_dir']}/executive_report/")
    print()


if __name__ == "__main__":
    main()
