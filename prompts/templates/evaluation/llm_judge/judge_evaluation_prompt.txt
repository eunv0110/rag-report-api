다음은 하나의 질문에 대해 7개의 서로 다른 LLM이 생성한 답변입니다.
각 답변의 품질을 평가하고 순위를 매겨주세요.

## 질문
{question}

## 검색된 문서
{context}

## 평가 대상 답변들

{answers}

## 평가 기준
1. **정확성(Accuracy)**: 주어진 문서를 바탕으로 정확한 정보를 제공하는가? (30점)
2. **완전성(Completeness)**: 질문에 대한 답변이 충분하고 포괄적인가? (25점)
3. **관련성(Relevance)**: 질문과 직접적으로 관련된 내용만 포함하는가? (20점)
4. **구조화(Structure)**: 답변이 논리적으로 잘 구성되어 있는가? (15점)
5. **명확성(Clarity)**: 이해하기 쉽고 명확하게 작성되었는가? (10점)

## 요구사항
각 답변에 대해:
1. 5가지 평가 기준별 점수와 근거를 제시하세요.
2. 총점(100점 만점)을 계산하세요.
3. 최종 순위(1위~7위)를 매기세요.

## 출력 형식 (JSON)
반드시 다음 JSON 형식으로만 답변하세요:

```json
{{
  "evaluations": [
    {{
      "llm_name": "모델명",
      "scores": {{
        "accuracy": 점수,
        "completeness": 점수,
        "relevance": 점수,
        "structure": 점수,
        "clarity": 점수
      }},
      "reasoning": {{
        "accuracy": "평가 근거",
        "completeness": "평가 근거",
        "relevance": "평가 근거",
        "structure": "평가 근거",
        "clarity": "평가 근거"
      }},
      "total_score": 총점,
      "rank": 순위
    }}
  ],
  "summary": "전체 평가 요약 (2-3문장)"
}}
```
