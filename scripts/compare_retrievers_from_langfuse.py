#!/usr/bin/env python3
"""Langfuse REST APIë¡œ í‰ê°€ ê²°ê³¼ë¥¼ ê°€ì ¸ì™€ ë¦¬íŠ¸ë¦¬ë²„ ë¹„êµ

ì‚¬ìš©ë²•:
    # ëª¨ë“  ë¦¬íŠ¸ë¦¬ë²„ ë¹„êµ
    python scripts/compare_retrievers_from_langfuse.py

    # íŠ¹ì • ë³´ê³ ì„œ íƒ€ì…ë§Œ
    python scripts/compare_retrievers_from_langfuse.py --report-type weekly_report

    # íŠ¹ì • ë©”íŠ¸ë¦­ìœ¼ë¡œ ì •ë ¬
    python scripts/compare_retrievers_from_langfuse.py --sort-by context_recall
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

from dotenv import load_dotenv
load_dotenv()

import argparse
import pandas as pd
import requests
from typing import List, Dict, Any, Optional
from collections import defaultdict
import base64

from config.settings import LANGFUSE_PUBLIC_KEY, LANGFUSE_SECRET_KEY, LANGFUSE_HOST


def get_auth_header() -> Dict[str, str]:
    """Langfuse API ì¸ì¦ í—¤ë” ìƒì„±"""
    if not LANGFUSE_PUBLIC_KEY or not LANGFUSE_SECRET_KEY:
        raise ValueError("Langfuse í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")

    # Basic Auth ìƒì„±
    credentials = f"{LANGFUSE_PUBLIC_KEY}:{LANGFUSE_SECRET_KEY}"
    encoded = base64.b64encode(credentials.encode()).decode()

    return {
        "Authorization": f"Basic {encoded}",
        "Content-Type": "application/json"
    }


def fetch_trace_by_id(trace_id: str) -> Optional[Dict[str, Any]]:
    """íŠ¹ì • trace IDë¡œ trace ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
    base_url = LANGFUSE_HOST.rstrip('/')
    url = f"{base_url}/api/public/traces/{trace_id}"

    headers = get_auth_header()

    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        return response.json()
    except:
        return None


def fetch_all_scores(max_pages: int = 50) -> List[Dict[str, Any]]:
    """ëª¨ë“  scores ê°€ì ¸ì˜¤ê¸°"""
    base_url = LANGFUSE_HOST.rstrip('/')
    url = f"{base_url}/api/public/scores"
    headers = get_auth_header()

    all_scores = []

    print("\n" + "=" * 80)
    print("ğŸ” Langfuse Scores ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
    print("=" * 80)

    for page in range(1, max_pages + 1):
        params = {
            "page": page,
            "limit": 100
        }

        try:
            response = requests.get(url, headers=headers, params=params)
            response.raise_for_status()

            data = response.json()
            scores = data.get("data", [])

            if not scores:
                print(f"   Page {page}: ë°ì´í„° ì—†ìŒ - ì¤‘ë‹¨")
                break

            all_scores.extend(scores)
            print(f"   Page {page}: {len(scores)} scores ë°œê²¬ (ëˆ„ì : {len(all_scores)})")

        except requests.exceptions.HTTPError as e:
            print(f"  âŒ HTTP ì—ëŸ¬: {e}")
            print(f"     Response: {e.response.text}")
            break
        except Exception as e:
            print(f"  âŒ API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            break

    print(f"âœ… ì´ {len(all_scores)} scores ë°œê²¬")
    return all_scores


def aggregate_scores_by_retriever(
    all_scores: List[Dict[str, Any]],
    report_type_filter: Optional[str] = None
) -> Dict[str, Dict[str, List[float]]]:
    """Scoresë¥¼ retrieverë³„ë¡œ ì§‘ê³„"""

    print("\nğŸ“Š Scores ì§‘ê³„ ì¤‘...")

    retriever_scores = defaultdict(lambda: defaultdict(list))
    trace_cache = {}  # trace ì •ë³´ ìºì‹±

    processed = 0
    skipped = 0

    for score in all_scores:
        trace_id = score.get("traceId")
        score_name = score.get("name", "").lower()
        score_value = score.get("value")

        # ê´€ì‹¬ìˆëŠ” ë©”íŠ¸ë¦­ë§Œ
        if score_name not in ["faithfulness", "context_precision", "context_recall"]:
            continue

        if score_value is None:
            continue

        # Trace ì •ë³´ ê°€ì ¸ì˜¤ê¸° (ìºì‹±)
        if trace_id not in trace_cache:
            trace = fetch_trace_by_id(trace_id)
            if trace:
                trace_cache[trace_id] = trace
            else:
                trace_cache[trace_id] = None

        trace = trace_cache[trace_id]
        if not trace:
            skipped += 1
            continue

        # Metadataì—ì„œ retriever ì •ë³´ ì¶”ì¶œ
        metadata = trace.get("metadata", {})
        retriever_name = metadata.get("retriever_name")
        trace_report_type = metadata.get("report_type")
        top_k = metadata.get("top_k", "unknown")

        # Report type í•„í„°ë§
        if report_type_filter and trace_report_type != report_type_filter:
            skipped += 1
            continue

        if not retriever_name:
            skipped += 1
            continue

        # Key ìƒì„±: retriever_name + top_k
        key = f"{retriever_name}_k{top_k}"
        retriever_scores[key][score_name].append(score_value)
        processed += 1

        if processed % 100 == 0:
            print(f"   âœ“ {processed} scores ì²˜ë¦¬ë¨ (ìŠ¤í‚µ: {skipped})...")

    print(f"âœ… ì´ {processed} scores ì²˜ë¦¬ ì™„ë£Œ (ìŠ¤í‚µ: {skipped})")
    print(f"âœ… {len(retriever_scores)} ê°œ ë¦¬íŠ¸ë¦¬ë²„ ë°œê²¬")

    return dict(retriever_scores)


def calculate_statistics(scores: List[float]) -> Dict[str, float]:
    """ì ìˆ˜ í†µê³„ ê³„ì‚°"""
    if not scores:
        return {"mean": 0.0, "min": 0.0, "max": 0.0, "count": 0}

    return {
        "mean": sum(scores) / len(scores),
        "min": min(scores),
        "max": max(scores),
        "count": len(scores)
    }


def create_comparison_dataframe(retriever_scores: Dict[str, Dict[str, List[float]]]) -> pd.DataFrame:
    """ë¹„êµ í…Œì´ë¸” ìƒì„±"""
    rows = []

    for retriever_name, scores_dict in retriever_scores.items():
        row = {"retriever": retriever_name}

        # ê° ë©”íŠ¸ë¦­ì˜ í‰ê·  ê³„ì‚°
        for metric in ["faithfulness", "context_precision", "context_recall"]:
            if metric in scores_dict and scores_dict[metric]:
                stats = calculate_statistics(scores_dict[metric])
                row[f"{metric}_mean"] = stats["mean"]
                row[f"{metric}_count"] = stats["count"]
            else:
                row[f"{metric}_mean"] = None
                row[f"{metric}_count"] = 0

        rows.append(row)

    return pd.DataFrame(rows)


def print_comparison_table(df: pd.DataFrame, sort_by: str = "faithfulness_mean"):
    """ë¹„êµ í…Œì´ë¸” ì¶œë ¥"""
    if df.empty:
        print("âŒ ë¹„êµí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ì •ë ¬
    if sort_by + "_mean" in df.columns:
        df = df.sort_values(sort_by + "_mean", ascending=False, na_position='last')

    print("\n" + "=" * 120)
    print("ğŸ“Š ë¦¬íŠ¸ë¦¬ë²„ ì„±ëŠ¥ ë¹„êµ")
    print("=" * 120)

    # ì¶œë ¥ í¬ë§· ì„¤ì •
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    pd.set_option('display.max_colwidth', 50)

    # ì£¼ìš” ë©”íŠ¸ë¦­ë§Œ ì¶œë ¥
    display_cols = ["retriever"]
    metric_cols = []

    for metric in ["faithfulness", "context_precision", "context_recall"]:
        mean_col = f"{metric}_mean"
        count_col = f"{metric}_count"
        if mean_col in df.columns and df[mean_col].notna().any():
            display_cols.append(mean_col)
            display_cols.append(count_col)
            metric_cols.append(mean_col)

    # í…Œì´ë¸” ì¶œë ¥
    display_df = df[display_cols].copy()

    # ìˆ«ì í¬ë§·íŒ…
    for col in metric_cols:
        display_df[col] = display_df[col].apply(lambda x: f"{x:.3f}" if pd.notna(x) else "N/A")

    print(display_df.to_string(index=False))
    print("=" * 120)

    # ìµœê³  ì„±ëŠ¥ ë¦¬íŠ¸ë¦¬ë²„
    print("\nğŸ† ìµœê³  ì„±ëŠ¥ ë¦¬íŠ¸ë¦¬ë²„:")
    for metric in ["faithfulness", "context_precision", "context_recall"]:
        mean_col = f"{metric}_mean"
        if mean_col in df.columns and df[mean_col].notna().any():
            # ì›ë³¸ ë°ì´í„°í”„ë ˆì„ì—ì„œ ìµœëŒ“ê°’ ì°¾ê¸°
            numeric_df = df[df[mean_col].notna()].copy()
            if not numeric_df.empty:
                best_idx = numeric_df[mean_col].idxmax()
                best_retriever = numeric_df.loc[best_idx, "retriever"]
                best_score = numeric_df.loc[best_idx, mean_col]
                count = numeric_df.loc[best_idx, f"{metric}_count"]
                print(f"  â€¢ {metric.capitalize()}: {best_retriever} ({best_score:.3f}, n={int(count)})")


def export_to_csv(df: pd.DataFrame, output_path: str):
    """ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥"""
    df.to_csv(output_path, index=False)
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}")


def main():
    parser = argparse.ArgumentParser(description="Langfuseì—ì„œ ë¦¬íŠ¸ë¦¬ë²„ í‰ê°€ ê²°ê³¼ ë¹„êµ")
    parser.add_argument(
        "--report-type",
        type=str,
        choices=["weekly_report", "executive_report"],
        help="ë³´ê³ ì„œ íƒ€ì… í•„í„°"
    )
    parser.add_argument(
        "--sort-by",
        type=str,
        default="faithfulness",
        choices=["faithfulness", "context_precision", "context_recall"],
        help="ì •ë ¬ ê¸°ì¤€ ë©”íŠ¸ë¦­"
    )
    parser.add_argument(
        "--max-pages",
        type=int,
        default=10,
        help="ê°€ì ¸ì˜¬ ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (í˜ì´ì§€ë‹¹ 100ê°œ scores)"
    )
    parser.add_argument(
        "--export",
        type=str,
        help="ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥í•  ê²½ë¡œ"
    )

    args = parser.parse_args()

    # Scores ê°€ì ¸ì˜¤ê¸°
    try:
        all_scores = fetch_all_scores(max_pages=args.max_pages)
    except ValueError as e:
        print(f"âŒ {e}")
        return

    if not all_scores:
        print("\nâŒ Scoresë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ íŒíŠ¸:")
        print("   1. Langfuseì— í‰ê°€ ê²°ê³¼ê°€ ì—…ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸")
        print("   2. .env íŒŒì¼ì— LANGFUSE_PUBLIC_KEY, LANGFUSE_SECRET_KEYê°€ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸")
        return

    # Retrieverë³„ë¡œ ì§‘ê³„
    retriever_scores = aggregate_scores_by_retriever(
        all_scores,
        report_type_filter=args.report_type
    )

    if not retriever_scores:
        print("\nâŒ ìœ íš¨í•œ í‰ê°€ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ íŒíŠ¸:")
        print("   1. traceì˜ metadataì— 'retriever_name'ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸")
        print("   2. report_type í•„í„°ê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸")
        return

    # ë¹„êµ í…Œì´ë¸” ìƒì„±
    df = create_comparison_dataframe(retriever_scores)

    # ê²°ê³¼ ì¶œë ¥
    print_comparison_table(df, sort_by=args.sort_by)

    # CSV ì €ì¥
    if args.export:
        export_to_csv(df, args.export)


if __name__ == "__main__":
    main()
