#!/usr/bin/env python3
"""ìœ¡ì•ˆ í‰ê°€(Human Evaluation) í…œí”Œë¦¿ ë° ë°ì´í„° ê´€ë¦¬

ìœ¡ì•ˆ í‰ê°€ ê²°ê³¼ë¥¼ êµ¬ì¡°í™”í•˜ì—¬ ì €ì¥í•˜ê³ , Judge í‰ê°€ì™€ ë¹„êµ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ê´€ë¦¬í•©ë‹ˆë‹¤.
"""

import json
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any


class HumanEvaluationManager:
    """ìœ¡ì•ˆ í‰ê°€ ê´€ë¦¬ í´ë˜ìŠ¤"""

    # ì£¼ê°„ ë³´ê³ ì„œ í‰ê°€ ê¸°ì¤€
    WEEKLY_CRITERIA = {
        "conciseness": {
            "name": "ê°„ê²°ì„±",
            "description": "ë¶ˆí•„ìš”í•œ ë°˜ë³µ ì—†ì´ í•µì‹¬ë§Œ ê°„ê²°í•˜ê²Œ ì „ë‹¬í•˜ëŠ”ê°€?",
            "weight": 0.35,
            "max_score": 35
        },
        "structure": {
            "name": "êµ¬ì¡°",
            "description": "ì£¼ê°„ ë³´ê³ ì„œ í˜•ì‹ì— ì í•©í•˜ê²Œ ë…¼ë¦¬ì ìœ¼ë¡œ êµ¬ì¡°í™”ë˜ì—ˆëŠ”ê°€?",
            "weight": 0.25,
            "max_score": 25
        },
        "practicality": {
            "name": "ì‹¤ë¬´ì„±",
            "description": "ì‹¤ë¬´ì—ì„œ ë°”ë¡œ í™œìš© ê°€ëŠ¥í•œ í˜•íƒœì¸ê°€?",
            "weight": 0.25,
            "max_score": 25
        },
        "accuracy": {
            "name": "ì •í™•ì„±",
            "description": "ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ë‚´ìš©ì„ ì •í™•í•˜ê²Œ ë°˜ì˜í•˜ê³  ìˆëŠ”ê°€?",
            "weight": 0.15,
            "max_score": 15
        }
    }

    # ìµœì¢… ë³´ê³ ì„œ í‰ê°€ ê¸°ì¤€
    EXECUTIVE_CRITERIA = {
        "completeness": {
            "name": "ì™„ì„±ë„",
            "description": "ê²½ì˜ì§„ ë³´ê³ ì„œë¡œì„œ êµ¬ì¡°ì™€ ë‚´ìš©ì´ ì™„ì„±ë„ê°€ ë†’ì€ê°€?",
            "weight": 0.25,
            "max_score": 25
        },
        "accuracy": {
            "name": "ì •í™•ì„±",
            "description": "ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ë‚´ìš©ì„ ì •í™•í•˜ê²Œ ë°˜ì˜í•˜ê³  ìˆëŠ”ê°€?",
            "weight": 0.25,
            "max_score": 25
        },
        "practicality": {
            "name": "ì‹¤ìš©ì„±",
            "description": "ê²½ì˜ì§„ì´ ì˜ì‚¬ê²°ì •ì— ì‹¤ì œë¡œ í™œìš©í•  ìˆ˜ ìˆëŠ” ë‚´ìš©ì¸ê°€?",
            "weight": 0.25,
            "max_score": 25
        },
        "conciseness": {
            "name": "ê°„ê²°ì„±",
            "description": "í•µì‹¬ë§Œ ê°„ê²°í•˜ê²Œ ìš”ì•½ë˜ì—ˆëŠ”ê°€?",
            "weight": 0.25,
            "max_score": 25
        }
    }

    def __init__(self):
        self.evaluations = {}

    def create_evaluation_template(
        self,
        report_type: str,
        llm_names: List[str]
    ) -> Dict[str, Any]:
        """í‰ê°€ í…œí”Œë¦¿ ìƒì„±

        Args:
            report_type: 'weekly' or 'executive'
            llm_names: í‰ê°€í•  LLM ì´ë¦„ ë¦¬ìŠ¤íŠ¸

        Returns:
            í‰ê°€ í…œí”Œë¦¿ ë”•ì…”ë„ˆë¦¬
        """
        criteria = self.WEEKLY_CRITERIA if report_type == 'weekly' else self.EXECUTIVE_CRITERIA

        template = {
            "report_type": report_type,
            "evaluation_date": datetime.now().isoformat(),
            "criteria": criteria,
            "evaluations": {}
        }

        for llm_name in llm_names:
            template["evaluations"][llm_name] = {
                "criteria_scores": {
                    criterion_key: {
                        "score": 0,
                        "max_score": criterion_info["max_score"],
                        "comment": ""
                    }
                    for criterion_key, criterion_info in criteria.items()
                },
                "total_score": 0,
                "rank": 0,
                "overall_comment": "",
                "strengths": [],
                "weaknesses": []
            }

        return template

    def load_evaluation_from_data(
        self,
        report_type: str,
        evaluation_data: Dict[str, Any]
    ):
        """ê·€í•˜ì˜ í‰ê°€ ê²°ê³¼ë¥¼ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ë³€í™˜

        Args:
            report_type: 'weekly' or 'executive'
            evaluation_data: {llm_name: {total_score, criteria_scores, comments}}
        """
        criteria = self.WEEKLY_CRITERIA if report_type == 'weekly' else self.EXECUTIVE_CRITERIA

        structured_eval = {
            "report_type": report_type,
            "evaluation_date": datetime.now().isoformat(),
            "criteria": criteria,
            "evaluations": {}
        }

        for llm_name, data in evaluation_data.items():
            structured_eval["evaluations"][llm_name] = {
                "criteria_scores": data.get("criteria_scores", {}),
                "total_score": data.get("total_score", 0),
                "rank": data.get("rank", 0),
                "overall_comment": data.get("overall_comment", ""),
                "strengths": data.get("strengths", []),
                "weaknesses": data.get("weaknesses", [])
            }

        self.evaluations[report_type] = structured_eval
        return structured_eval

    def save_evaluation(
        self,
        report_type: str,
        output_path: str
    ):
        """í‰ê°€ ê²°ê³¼ ì €ì¥

        Args:
            report_type: 'weekly' or 'executive'
            output_path: ì €ì¥ ê²½ë¡œ
        """
        if report_type not in self.evaluations:
            raise ValueError(f"{report_type} í‰ê°€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.evaluations[report_type], f, indent=2, ensure_ascii=False)

        print(f"âœ… í‰ê°€ ê²°ê³¼ ì €ì¥: {output_file}")

    def export_for_judge_comparison(
        self,
        report_type: str,
        output_path: str
    ):
        """Judge í‰ê°€ì™€ ë¹„êµ ê°€ëŠ¥í•œ í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°

        Args:
            report_type: 'weekly' or 'executive'
            output_path: ì €ì¥ ê²½ë¡œ
        """
        if report_type not in self.evaluations:
            raise ValueError(f"{report_type} í‰ê°€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        eval_data = self.evaluations[report_type]

        # Judge í‰ê°€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        judge_format = {
            "report_type": report_type,
            "evaluation_date": eval_data["evaluation_date"],
            "evaluator": "Human",
            "results": {}
        }

        for llm_name, llm_eval in eval_data["evaluations"].items():
            judge_format["results"][llm_name] = {
                "final_score": llm_eval["total_score"] / 10,  # 100ì ì„ 10ì  ë§Œì ìœ¼ë¡œ ë³€í™˜
                "criterion_results": []
            }

            for criterion_key, criterion_score in llm_eval["criteria_scores"].items():
                criterion_info = eval_data["criteria"][criterion_key]
                judge_format["results"][llm_name]["criterion_results"].append({
                    "criterion": criterion_key,
                    "criterion_name": criterion_info["name"],
                    "weight": criterion_info["weight"],
                    "score": criterion_score["score"] / criterion_score["max_score"] * 10,  # 10ì  ë§Œì ìœ¼ë¡œ
                    "weighted_score": criterion_score["score"] / 100,
                    "reasoning": criterion_score.get("comment", ""),
                    "strengths": llm_eval.get("strengths", []),
                    "weaknesses": llm_eval.get("weaknesses", [])
                })

        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(judge_format, f, indent=2, ensure_ascii=False)

        print(f"âœ… Judge ë¹„êµìš© í˜•ì‹ìœ¼ë¡œ ì €ì¥: {output_file}")
        return judge_format


def create_your_evaluation_data():
    """ê·€í•˜ì˜ ìœ¡ì•ˆ í‰ê°€ ê²°ê³¼ë¥¼ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë³€í™˜"""

    # ì£¼ê°„ ë³´ê³ ì„œ í‰ê°€ ê²°ê³¼
    weekly_evaluation = {
        "OpenAI GPT-4.1": {
            "total_score": 91,
            "rank": 1,
            "criteria_scores": {
                "conciseness": {"score": 30, "max_score": 35, "comment": "í‰ê·  1,000ì ë‚´ì™¸ë¡œ ê°€ì¥ ì ì ˆí•œ ê¸¸ì´"},
                "structure": {"score": 23, "max_score": 25, "comment": "ì£¼ìš” ì§€í‘œ â†’ í™œë™ â†’ ì´ìŠˆ â†’ ë‹¤ìŒì£¼ ê³„íš íë¦„ì´ ìì—°ìŠ¤ëŸ¬ì›€"},
                "practicality": {"score": 24, "max_score": 25, "comment": "ì‹¤ë¬´ì—ì„œ ë°”ë¡œ í™œìš© ê°€ëŠ¥í•œ í˜•íƒœ"},
                "accuracy": {"score": 14, "max_score": 15, "comment": "ë¶ˆí•„ìš”í•œ ë°˜ë³µì´ë‚˜ ì¥í™©í•œ ì„¤ëª… ì—†ìŒ"}
            },
            "strengths": [
                "í‰ê·  1,000ì ë‚´ì™¸ë¡œ ê°€ì¥ ì ì ˆí•œ ê¸¸ì´",
                "ì£¼ìš” ì§€í‘œ â†’ í™œë™ â†’ ì´ìŠˆ â†’ ë‹¤ìŒì£¼ ê³„íš íë¦„ì´ ìì—°ìŠ¤ëŸ¬ì›€",
                "ì‹¤ë¬´ì—ì„œ ë°”ë¡œ í™œìš© ê°€ëŠ¥í•œ í˜•íƒœ",
                "ë¶ˆí•„ìš”í•œ ë°˜ë³µì´ë‚˜ ì¥í™©í•œ ì„¤ëª… ì—†ìŒ"
            ],
            "weaknesses": [],
            "overall_comment": "1ìœ„: ì‹¤ë¬´ì—ì„œ ë°”ë¡œ í™œìš© ê°€ëŠ¥í•œ ìµœì ì˜ ì£¼ê°„ ë³´ê³ ì„œ"
        },
        "DeepSeek-V3.1": {
            "total_score": 90,
            "rank": 2,
            "criteria_scores": {
                "conciseness": {"score": 32, "max_score": 35, "comment": "ê°€ì¥ ê°„ê²°í•¨ (í‰ê·  800ì)"},
                "structure": {"score": 22, "max_score": 25, "comment": "ëª…í™•í•œ êµ¬ì¡°"},
                "practicality": {"score": 22, "max_score": 25, "comment": "í•µì‹¬ ìˆ˜ì¹˜ì™€ ì•¡ì…˜ë§Œ ì •ë¦¬"},
                "accuracy": {"score": 14, "max_score": 15, "comment": "ë¶ˆí•„ìš”í•œ ì„¤ëª… ìµœì†Œí™”"}
            },
            "strengths": [
                "ê°€ì¥ ê°„ê²°í•¨ (í‰ê·  800ì)",
                "ë¶ˆí•„ìš”í•œ ì„¤ëª… ìµœì†Œí™”",
                "í•µì‹¬ ìˆ˜ì¹˜ì™€ ì•¡ì…˜ë§Œ ì •ë¦¬"
            ],
            "weaknesses": [
                "ê°€ë” ë„ˆë¬´ ê°„ê²°í•´ì„œ ë§¥ë½ ë¶€ì¡±"
            ],
            "overall_comment": "2ìœ„: ë¹ ë¥¸ íŒŒì•…ìš© ê°„ê²°í•œ ë³´ê³ ì„œ"
        },
        "Claude 4.5 Sonnet": {
            "total_score": 88,
            "rank": 3,
            "criteria_scores": {
                "conciseness": {"score": 31, "max_score": 35, "comment": "ì ì ˆí•œ ê¸¸ì´"},
                "structure": {"score": 22, "max_score": 25, "comment": "êµ¬ì¡°ê°€ ëª…í™•í•˜ê³  ì¼ê´€ì„± ìˆìŒ"},
                "practicality": {"score": 22, "max_score": 25, "comment": "í…Œì´ë¸” í™œìš©ìœ¼ë¡œ ê°€ë…ì„± ì¢‹ìŒ"},
                "accuracy": {"score": 13, "max_score": 15, "comment": "ê°€ë” 'ë¬¸ì„œì— ëª…ì‹œë˜ì§€ ì•ŠìŒ' ê³¼ë‹¤"}
            },
            "strengths": [
                "êµ¬ì¡°ê°€ ëª…í™•í•˜ê³  ì¼ê´€ì„± ìˆìŒ",
                "í…Œì´ë¸” í™œìš©ìœ¼ë¡œ ê°€ë…ì„± ì¢‹ìŒ"
            ],
            "weaknesses": [
                "ë•Œë•Œë¡œ 'ë¬¸ì„œì— ëª…ì‹œë˜ì§€ ì•ŠìŒ' ê³¼ë‹¤"
            ],
            "overall_comment": "3ìœ„: êµ¬ì¡°ì ìœ¼ë¡œ ìš°ìˆ˜"
        },
        "Claude 4.5 Opus": {
            "total_score": 86,
            "rank": 4,
            "criteria_scores": {
                "conciseness": {"score": 30, "max_score": 35},
                "structure": {"score": 22, "max_score": 25},
                "practicality": {"score": 21, "max_score": 25},
                "accuracy": {"score": 13, "max_score": 15}
            },
            "strengths": [],
            "weaknesses": [],
            "overall_comment": "4ìœ„"
        },
        "Phi-4": {
            "total_score": 82,
            "rank": 5,
            "criteria_scores": {
                "conciseness": {"score": 28, "max_score": 35},
                "structure": {"score": 21, "max_score": 25},
                "practicality": {"score": 20, "max_score": 25},
                "accuracy": {"score": 13, "max_score": 15}
            },
            "strengths": [],
            "weaknesses": [],
            "overall_comment": "5ìœ„"
        },
        "OpenAI GPT-5.1": {
            "total_score": 81,
            "rank": 6,
            "criteria_scores": {
                "conciseness": {"score": 18, "max_score": 35, "comment": "ë„ˆë¬´ ì¥í™©í•¨"},
                "structure": {"score": 24, "max_score": 25},
                "practicality": {"score": 24, "max_score": 25},
                "accuracy": {"score": 15, "max_score": 15}
            },
            "strengths": [],
            "weaknesses": ["ë„ˆë¬´ ì¥í™©í•¨"],
            "overall_comment": "6ìœ„: ë‚´ìš©ì€ ì¢‹ìœ¼ë‚˜ ë„ˆë¬´ ê¸¸ì–´ì„œ ì£¼ê°„ ë³´ê³ ì„œë¡œëŠ” ë¶€ì í•©"
        },
        "Llama-3.3-70B-Instruct": {
            "total_score": 72,
            "rank": 7,
            "criteria_scores": {
                "conciseness": {"score": 25, "max_score": 35},
                "structure": {"score": 18, "max_score": 25},
                "practicality": {"score": 18, "max_score": 25},
                "accuracy": {"score": 11, "max_score": 15}
            },
            "strengths": [],
            "weaknesses": [],
            "overall_comment": "7ìœ„"
        }
    }

    # ìµœì¢… ë³´ê³ ì„œ í‰ê°€ ê²°ê³¼
    executive_evaluation = {
        "OpenAI GPT-4.1": {
            "total_score": 87.5,
            "rank": 1,
            "criteria_scores": {
                "completeness": {"score": 22.5, "max_score": 25, "comment": "ì„ì›/ì‹¤ë¬´ì§„ì´ ì½ê¸° ê°€ì¥ ì¢‹ì€ í˜•íƒœ"},
                "accuracy": {"score": 22.5, "max_score": 25, "comment": "ë†’ì€ ì™„ì„±ë„"},
                "practicality": {"score": 22.5, "max_score": 25, "comment": "í•µì‹¬ íŒŒì•… ìš©ì´"},
                "conciseness": {"score": 20.0, "max_score": 25, "comment": "ì ì ˆí•œ ê¸¸ì´"}
            },
            "strengths": [
                "ì„ì›/ì‹¤ë¬´ì§„ì´ ì½ê¸° ê°€ì¥ ì¢‹ì€ í˜•íƒœ",
                "í•µì‹¬ íŒŒì•… ìš©ì´",
                "ì ì ˆí•œ ê¸¸ì´",
                "ë†’ì€ ì™„ì„±ë„"
            ],
            "weaknesses": [],
            "overall_comment": "1ìœ„: ì„ì›/ì‹¤ë¬´ì§„ì´ ì½ê¸° ê°€ì¥ ì¢‹ì€ í˜•íƒœ"
        },
        "OpenAI GPT-5.1": {
            "total_score": 85.5,
            "rank": 2,
            "criteria_scores": {
                "completeness": {"score": 24.0, "max_score": 25, "comment": "ì™„ë²½í•œ êµ¬ì¡°"},
                "accuracy": {"score": 24.0, "max_score": 25, "comment": "ì™„ë²½í•œ ì •í™•ì„±"},
                "practicality": {"score": 24.0, "max_score": 25, "comment": "ë¹ ì§ì—†ëŠ” ì •ë³´"},
                "conciseness": {"score": 13.5, "max_score": 25, "comment": "ë„ˆë¬´ ê¸¸ì–´ì„œ í•µì‹¬ íŒŒì•…ì´ ì–´ë ¤ì›€"}
            },
            "strengths": [
                "ì™„ë²½í•œ êµ¬ì¡°",
                "ì™„ë²½í•œ ì •í™•ì„±",
                "ë¹ ì§ì—†ëŠ” ì •ë³´"
            ],
            "weaknesses": [
                "ë„ˆë¬´ ê¸¸ì–´ì„œ í•µì‹¬ íŒŒì•…ì´ ì–´ë ¤ì›€"
            ],
            "overall_comment": "2ìœ„: ì™¸ë¶€ ì œì¶œìš© ìƒì„¸ ê¸°ìˆ  ë¬¸ì„œì— ìµœì "
        },
        "DeepSeek-V3.1": {
            "total_score": 79.0,
            "rank": 3,
            "criteria_scores": {
                "completeness": {"score": 19.5, "max_score": 25},
                "accuracy": {"score": 19.5, "max_score": 25},
                "practicality": {"score": 17.5, "max_score": 25, "comment": "ì„¸ë¶€ ë‚´ìš© ë¶€ì¡±í•  ìˆ˜ ìˆìŒ"},
                "conciseness": {"score": 22.5, "max_score": 25, "comment": "íš¨ìœ¨ì , í•µì‹¬ë§Œ ì¶”ì¶œ"}
            },
            "strengths": [
                "íš¨ìœ¨ì ",
                "í•µì‹¬ë§Œ ì¶”ì¶œ"
            ],
            "weaknesses": [
                "ì„¸ë¶€ ë‚´ìš© ë¶€ì¡±í•  ìˆ˜ ìˆìŒ"
            ],
            "overall_comment": "3ìœ„: ë¹ ë¥¸ ìƒí™© íŒŒì•…ìš© ìš”ì•½ ë³´ê³ ì„œ"
        },
        "Claude 4.5 Opus": {
            "total_score": 64.0,
            "rank": 4,
            "criteria_scores": {
                "completeness": {"score": 16.0, "max_score": 25},
                "accuracy": {"score": 16.0, "max_score": 25},
                "practicality": {"score": 13.5, "max_score": 25},
                "conciseness": {"score": 18.5, "max_score": 25}
            },
            "strengths": [],
            "weaknesses": [],
            "overall_comment": "4ìœ„"
        },
        "Claude 4.5 Sonnet": {
            "total_score": 63.5,
            "rank": 5,
            "criteria_scores": {
                "completeness": {"score": 15.5, "max_score": 25},
                "accuracy": {"score": 15.5, "max_score": 25},
                "practicality": {"score": 13.0, "max_score": 25},
                "conciseness": {"score": 18.5, "max_score": 25}
            },
            "strengths": [],
            "weaknesses": [],
            "overall_comment": "5ìœ„"
        },
        "Phi-4": {
            "total_score": 54.5,
            "rank": 6,
            "criteria_scores": {
                "completeness": {"score": 14.0, "max_score": 25},
                "accuracy": {"score": 13.0, "max_score": 25},
                "practicality": {"score": 12.0, "max_score": 25},
                "conciseness": {"score": 16.0, "max_score": 25}
            },
            "strengths": [],
            "weaknesses": [],
            "overall_comment": "6ìœ„"
        },
        "Llama-3.3-70B-Instruct": {
            "total_score": 28.0,
            "rank": 7,
            "criteria_scores": {
                "completeness": {"score": 6.5, "max_score": 25},
                "accuracy": {"score": 5.5, "max_score": 25},
                "practicality": {"score": 5.5, "max_score": 25},
                "conciseness": {"score": 10.5, "max_score": 25}
            },
            "strengths": [],
            "weaknesses": [],
            "overall_comment": "7ìœ„"
        }
    }

    return weekly_evaluation, executive_evaluation


def main():
    """ë©”ì¸ í•¨ìˆ˜ - ìœ¡ì•ˆ í‰ê°€ ë°ì´í„° ì €ì¥"""
    manager = HumanEvaluationManager()

    # ìœ¡ì•ˆ í‰ê°€ ë°ì´í„° ë¡œë“œ
    weekly_eval, executive_eval = create_your_evaluation_data()

    # êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ë³€í™˜
    manager.load_evaluation_from_data('weekly', weekly_eval)
    manager.load_evaluation_from_data('executive', executive_eval)

    # ì €ì¥ ë””ë ‰í† ë¦¬
    output_dir = Path("data/results/multi_llm_test/human_evaluation")

    # í‰ê°€ ê²°ê³¼ ì €ì¥
    manager.save_evaluation('weekly', output_dir / "weekly_human_evaluation.json")
    manager.save_evaluation('executive', output_dir / "executive_human_evaluation.json")

    # Judge ë¹„êµìš© í˜•ì‹ìœ¼ë¡œ ì €ì¥
    manager.export_for_judge_comparison('weekly', output_dir / "weekly_human_evaluation_judge_format.json")
    manager.export_for_judge_comparison('executive', output_dir / "executive_human_evaluation_judge_format.json")

    print("\nâœ… ìœ¡ì•ˆ í‰ê°€ ë°ì´í„° ì €ì¥ ì™„ë£Œ!")
    print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {output_dir}")


if __name__ == "__main__":
    main()
