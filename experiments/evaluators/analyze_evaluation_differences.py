#!/usr/bin/env python3
"""ìœ¡ì•ˆ í‰ê°€ì™€ LLM Judge í‰ê°€ì˜ ì°¨ì´ ë¶„ì„

ìœ¡ì•ˆ í‰ê°€ ê²°ê³¼ì™€ LLM as Judge í‰ê°€ ê²°ê³¼ë¥¼ ë¹„êµí•˜ì—¬:
1. ìˆœìœ„ ì°¨ì´ ë¶„ì„
2. í‰ê°€ ê¸°ì¤€ë³„ ì ìˆ˜ ì°¨ì´
3. í‰ê°€ ë°©ì‹ì˜ íŠ¹ì§•ê³¼ í¸í–¥
4. ê°œì„  ë°©ì•ˆ ì œì•ˆ
"""

import json
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, List, Any
from scipy.stats import spearmanr, kendalltau
from datetime import datetime


def load_human_evaluation(file_path: str) -> Dict[str, Any]:
    """ìœ¡ì•ˆ í‰ê°€ ê²°ê³¼ ë¡œë“œ"""
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def load_judge_evaluation(file_path: str) -> Dict[str, Any]:
    """Judge í‰ê°€ ê²°ê³¼ ë¡œë“œ (ìˆë‹¤ë©´)"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        return None


def analyze_human_evaluation_patterns(
    human_eval: Dict[str, Any],
    report_type: str
) -> Dict[str, Any]:
    """ìœ¡ì•ˆ í‰ê°€ì˜ íŒ¨í„´ ë¶„ì„"""

    print(f"\n{'='*80}")
    print(f"{report_type.upper()} - ìœ¡ì•ˆ í‰ê°€ ë¶„ì„")
    print(f"{'='*80}")

    results = human_eval['results']

    # ìˆœìœ„ ì •ë³´
    rankings = []
    for llm_name, data in results.items():
        final_score = data['final_score']
        rankings.append({
            'llm': llm_name,
            'final_score': final_score,
            'criterion_scores': {
                cr['criterion']: cr['score']
                for cr in data['criterion_results']
            }
        })

    rankings.sort(key=lambda x: x['final_score'], reverse=True)

    # ì ìˆ˜ ë¶„í¬
    scores = [r['final_score'] for r in rankings]
    score_range = max(scores) - min(scores)

    print(f"\nğŸ“Š ì ìˆ˜ ë¶„í¬:")
    print(f"  ìµœê³ ì : {max(scores):.2f}")
    print(f"  ìµœì €ì : {min(scores):.2f}")
    print(f"  í‰ê· : {np.mean(scores):.2f}")
    print(f"  ì¤‘ì•™ê°’: {np.median(scores):.2f}")
    print(f"  ì ìˆ˜ ë²”ìœ„: {score_range:.2f}")
    print(f"  í‘œì¤€í¸ì°¨: {np.std(scores):.2f}")

    # ìˆœìœ„ë³„ ì ìˆ˜
    print(f"\nğŸ† ìˆœìœ„ë³„ ì ìˆ˜:")
    for i, r in enumerate(rankings, 1):
        print(f"  {i}ìœ„: {r['llm']:<25} - {r['final_score']:.2f}ì ")

    # í‰ê°€ ê¸°ì¤€ë³„ í‰ê·  ì ìˆ˜
    criteria_names = list(rankings[0]['criterion_scores'].keys())

    print(f"\nğŸ“‹ í‰ê°€ ê¸°ì¤€ë³„ í‰ê·  ì ìˆ˜ (10ì  ë§Œì ):")
    for criterion in criteria_names:
        criterion_scores = [r['criterion_scores'][criterion] for r in rankings]
        print(f"  {criterion}: {np.mean(criterion_scores):.2f}")

    # í‰ê°€ ê¸°ì¤€ë³„ ë³€ë³„ë ¥ (í‘œì¤€í¸ì°¨)
    print(f"\nğŸ” í‰ê°€ ê¸°ì¤€ë³„ ë³€ë³„ë ¥ (í‘œì¤€í¸ì°¨):")
    for criterion in criteria_names:
        criterion_scores = [r['criterion_scores'][criterion] for r in rankings]
        std = np.std(criterion_scores)
        print(f"  {criterion}: {std:.2f} (ë³€ë³„ë ¥: {'ë†’ìŒ' if std > 1.0 else 'ë³´í†µ' if std > 0.5 else 'ë‚®ìŒ'})")

    # LLMë³„ ê°•ì /ì•½ì  ë¶„ì„
    print(f"\nğŸ’¡ LLMë³„ íŠ¹ì§•:")
    for r in rankings[:3]:  # ìƒìœ„ 3ê°œë§Œ
        llm_name = r['llm']
        llm_data = results[llm_name]

        print(f"\n  {llm_name}:")
        if llm_data.get('strengths'):
            print(f"    ê°•ì :")
            for strength in llm_data['strengths']:
                print(f"      - {strength}")
        if llm_data.get('weaknesses'):
            print(f"    ì•½ì :")
            for weakness in llm_data['weaknesses']:
                print(f"      - {weakness}")

    return {
        'rankings': rankings,
        'score_stats': {
            'max': max(scores),
            'min': min(scores),
            'mean': np.mean(scores),
            'median': np.median(scores),
            'range': score_range,
            'std': np.std(scores)
        },
        'criteria_stats': {
            criterion: {
                'mean': np.mean([r['criterion_scores'][criterion] for r in rankings]),
                'std': np.std([r['criterion_scores'][criterion] for r in rankings])
            }
            for criterion in criteria_names
        }
    }


def compare_with_judge(
    human_eval: Dict[str, Any],
    judge_eval: Dict[str, Any],
    report_type: str
) -> Dict[str, Any]:
    """ìœ¡ì•ˆ í‰ê°€ì™€ Judge í‰ê°€ ë¹„êµ"""

    if judge_eval is None:
        print(f"\nâš ï¸  Judge í‰ê°€ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None

    print(f"\n{'='*80}")
    print(f"{report_type.upper()} - ìœ¡ì•ˆ í‰ê°€ vs Judge í‰ê°€ ë¹„êµ")
    print(f"{'='*80}")

    human_results = human_eval['results']
    judge_results = judge_eval['results']

    # ê³µí†µ LLM
    common_llms = set(human_results.keys()) & set(judge_results.keys())

    if not common_llms:
        print("âš ï¸  ê³µí†µ LLMì´ ì—†ìŠµë‹ˆë‹¤.")
        return None

    print(f"ê³µí†µ LLM ìˆ˜: {len(common_llms)}")

    # ë¹„êµ ë°ì´í„°
    comparison = []
    for llm in common_llms:
        human_score = human_results[llm]['final_score']
        judge_score = judge_results[llm]['final_score']

        comparison.append({
            'llm': llm,
            'human_score': human_score,
            'judge_score': judge_score,
            'score_diff': human_score - judge_score
        })

    # DataFrame ìƒì„±
    df = pd.DataFrame(comparison)

    # ìˆœìœ„ ê³„ì‚°
    df['human_rank'] = df['human_score'].rank(ascending=False, method='min').astype(int)
    df['judge_rank'] = df['judge_score'].rank(ascending=False, method='min').astype(int)
    df['rank_diff'] = abs(df['human_rank'] - df['judge_rank'])

    df = df.sort_values('human_rank')

    # ìƒê´€ê´€ê³„
    spearman_corr, spearman_p = spearmanr(df['human_score'], df['judge_score'])
    kendall_corr, kendall_p = kendalltau(df['human_score'], df['judge_score'])
    pearson_corr = np.corrcoef(df['human_score'], df['judge_score'])[0, 1]

    print(f"\nğŸ“Š ìƒê´€ê´€ê³„:")
    print(f"  Pearson: {pearson_corr:.4f}")
    print(f"  Spearman: {spearman_corr:.4f} (p={spearman_p:.4f})")
    print(f"  Kendall Tau: {kendall_corr:.4f} (p={kendall_p:.4f})")

    # ìˆœìœ„ ì°¨ì´
    print(f"\nğŸ“ˆ ìˆœìœ„ ì°¨ì´:")
    print(f"  í‰ê· : {df['rank_diff'].mean():.2f}")
    print(f"  ìµœëŒ€: {df['rank_diff'].max()}")

    # ë¹„êµí‘œ
    print(f"\nğŸ“‹ ìƒì„¸ ë¹„êµ:")
    print(df.to_string(index=False))

    # í° ì°¨ì´
    big_diff = df[df['rank_diff'] >= 2]
    if len(big_diff) > 0:
        print(f"\nâš ï¸  í° ìˆœìœ„ ì°¨ì´ (>= 2):")
        for _, row in big_diff.iterrows():
            print(f"  {row['llm']}:")
            print(f"    ìœ¡ì•ˆ: {row['human_rank']}ìœ„ ({row['human_score']:.2f}ì )")
            print(f"    Judge: {row['judge_rank']}ìœ„ ({row['judge_score']:.2f}ì )")
            print(f"    ì°¨ì´: {row['rank_diff']}")

    return {
        'comparison_df': df,
        'correlations': {
            'pearson': pearson_corr,
            'spearman': spearman_corr,
            'kendall': kendall_corr
        },
        'rank_diff_stats': {
            'mean': df['rank_diff'].mean(),
            'max': df['rank_diff'].max(),
            'std': df['rank_diff'].std()
        }
    }


def generate_insights_and_recommendations(
    weekly_analysis: Dict[str, Any],
    executive_analysis: Dict[str, Any],
    output_dir: Path
):
    """ì¸ì‚¬ì´íŠ¸ ë° ê°œì„  ë°©ì•ˆ ë¦¬í¬íŠ¸ ìƒì„±"""

    report_path = output_dir / "evaluation_insights_and_recommendations.md"

    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# ìœ¡ì•ˆ í‰ê°€ vs LLM Judge í‰ê°€ - ì¸ì‚¬ì´íŠ¸ ë° ê°œì„  ë°©ì•ˆ\n\n")
        f.write(f"**ë¶„ì„ ì¼ì‹œ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write("---\n\n")

        # ì£¼ê°„ ë³´ê³ ì„œ ì¸ì‚¬ì´íŠ¸
        f.write("## ğŸ“Š ì£¼ê°„ ë³´ê³ ì„œ í‰ê°€ ì¸ì‚¬ì´íŠ¸\n\n")

        if weekly_analysis:
            rankings = weekly_analysis['rankings']
            stats = weekly_analysis['score_stats']

            f.write(f"### ì ìˆ˜ ë¶„í¬\n\n")
            f.write(f"- ìµœê³ ì : {stats['max']:.2f}\n")
            f.write(f"- ìµœì €ì : {stats['min']:.2f}\n")
            f.write(f"- ì ìˆ˜ ë²”ìœ„: {stats['range']:.2f}\n")
            f.write(f"- í‘œì¤€í¸ì°¨: {stats['std']:.2f}\n\n")

            f.write(f"### ìˆœìœ„\n\n")
            for i, r in enumerate(rankings, 1):
                f.write(f"{i}. **{r['llm']}**: {r['final_score']:.2f}ì \n")
            f.write("\n")

            f.write(f"### í‰ê°€ ê¸°ì¤€ë³„ ë³€ë³„ë ¥\n\n")
            for criterion, crit_stats in weekly_analysis['criteria_stats'].items():
                discriminability = "ë†’ìŒ" if crit_stats['std'] > 1.0 else "ë³´í†µ" if crit_stats['std'] > 0.5 else "ë‚®ìŒ"
                f.write(f"- **{criterion}**: í‰ê·  {crit_stats['mean']:.2f}, í‘œì¤€í¸ì°¨ {crit_stats['std']:.2f} (ë³€ë³„ë ¥: {discriminability})\n")
            f.write("\n")

        # ìµœì¢… ë³´ê³ ì„œ ì¸ì‚¬ì´íŠ¸
        f.write("## ğŸ“‹ ìµœì¢… ë³´ê³ ì„œ í‰ê°€ ì¸ì‚¬ì´íŠ¸\n\n")

        if executive_analysis:
            rankings = executive_analysis['rankings']
            stats = executive_analysis['score_stats']

            f.write(f"### ì ìˆ˜ ë¶„í¬\n\n")
            f.write(f"- ìµœê³ ì : {stats['max']:.2f}\n")
            f.write(f"- ìµœì €ì : {stats['min']:.2f}\n")
            f.write(f"- ì ìˆ˜ ë²”ìœ„: {stats['range']:.2f}\n")
            f.write(f"- í‘œì¤€í¸ì°¨: {stats['std']:.2f}\n\n")

            f.write(f"### ìˆœìœ„\n\n")
            for i, r in enumerate(rankings, 1):
                f.write(f"{i}. **{r['llm']}**: {r['final_score']:.2f}ì \n")
            f.write("\n")

            f.write(f"### í‰ê°€ ê¸°ì¤€ë³„ ë³€ë³„ë ¥\n\n")
            for criterion, crit_stats in executive_analysis['criteria_stats'].items():
                discriminability = "ë†’ìŒ" if crit_stats['std'] > 1.0 else "ë³´í†µ" if crit_stats['std'] > 0.5 else "ë‚®ìŒ"
                f.write(f"- **{criterion}**: í‰ê·  {crit_stats['mean']:.2f}, í‘œì¤€í¸ì°¨ {crit_stats['std']:.2f} (ë³€ë³„ë ¥: {discriminability})\n")
            f.write("\n")

        # ì£¼ìš” ë°œê²¬ì‚¬í•­
        f.write("## ğŸ’¡ ì£¼ìš” ë°œê²¬ì‚¬í•­\n\n")

        f.write("### 1. ë³´ê³ ì„œ íƒ€ì…ì— ë”°ë¥¸ LLM ì„±ëŠ¥ ì°¨ì´\n\n")
        f.write("- **ì£¼ê°„ ë³´ê³ ì„œ**: GPT-4.1 (91ì ), DeepSeek-V3.1 (90ì ), Claude Sonnet 4.5 (88ì )\n")
        f.write("  - ê°„ê²°ì„±ê³¼ ì‹¤ë¬´ì„±ì´ ì¤‘ìš”\n")
        f.write("  - GPT-4.1ì´ ê°€ì¥ ê· í˜•ì¡íŒ ì„±ëŠ¥\n")
        f.write("  - DeepSeek-V3.1ì€ ê°„ê²°ì„±ì—ì„œ ìµœê³ ì \n\n")

        f.write("- **ìµœì¢… ë³´ê³ ì„œ**: GPT-4.1 (87.5ì ), GPT-5.1 (85.5ì ), DeepSeek-V3.1 (79ì )\n")
        f.write("  - ì™„ì„±ë„ì™€ ì •í™•ì„±ì´ ì¤‘ìš”\n")
        f.write("  - GPT-5.1ì€ ì™„ë²½í•˜ì§€ë§Œ ë„ˆë¬´ ê¸¸ì–´ì„œ 2ìœ„\n")
        f.write("  - ì£¼ê°„ë³´ê³ ì„œì™€ ë‹¤ë¥¸ ìˆœìœ„ ë³€ë™ (ì˜ˆ: Claude ëª¨ë¸ë“¤ì´ í•˜ìœ„ê¶Œ)\n\n")

        f.write("### 2. í‰ê°€ ê¸°ì¤€ë³„ íŠ¹ì§•\n\n")

        f.write("**ì£¼ê°„ ë³´ê³ ì„œ:**\n")
        f.write("- **ê°„ê²°ì„±**ì´ ê°€ì¥ ì¤‘ìš”í•œ ë³€ë³„ë ¥ (ê°€ì¤‘ì¹˜ 35%)\n")
        f.write("- GPT-5.1ì€ ê°„ê²°ì„± ë¶€ì¡±ìœ¼ë¡œ ìˆœìœ„ í•˜ë½\n")
        f.write("- DeepSeek-V3.1ì€ ê°„ê²°ì„±ì—ì„œ ìµœê³  ì ìˆ˜\n\n")

        f.write("**ìµœì¢… ë³´ê³ ì„œ:**\n")
        f.write("- **ê°„ê²°ì„±**ì˜ ì¤‘ìš”ë„ê°€ ìƒëŒ€ì ìœ¼ë¡œ ë‚®ì•„ì§ (ê°€ì¤‘ì¹˜ 25%)\n")
        f.write("- **ì™„ì„±ë„**, **ì •í™•ì„±**, **ì‹¤ìš©ì„±**ì´ ê· ë“±í•˜ê²Œ ì¤‘ìš”\n")
        f.write("- GPT-5.1ì€ ì™„ì„±ë„ì—ì„œ ìµœê³ ì ì´ì§€ë§Œ ê°„ê²°ì„± ë¶€ì¡±ìœ¼ë¡œ ì¢…í•© 2ìœ„\n\n")

        f.write("### 3. LLMë³„ íŠ¹ì§•\n\n")

        f.write("**GPT-4.1:**\n")
        f.write("- ë‘ ë³´ê³ ì„œ íƒ€ì… ëª¨ë‘ì—ì„œ 1ìœ„\n")
        f.write("- ê· í˜•ì¡íŒ ì„±ëŠ¥ (ê°„ê²°ì„±, êµ¬ì¡°, ì •í™•ì„± ëª¨ë‘ ìš°ìˆ˜)\n")
        f.write("- ì‹¤ë¬´ í™œìš©ë„ê°€ ê°€ì¥ ë†’ìŒ\n\n")

        f.write("**GPT-5.1:**\n")
        f.write("- ì™„ì„±ë„ì™€ ì •í™•ì„±ì€ ìµœê³  ìˆ˜ì¤€\n")
        f.write("- ì£¼ê°„ë³´ê³ ì„œì—ì„œëŠ” 6ìœ„ë¡œ ì €ì¡° (ë„ˆë¬´ ì¥í™©í•¨)\n")
        f.write("- ìµœì¢…ë³´ê³ ì„œì—ì„œëŠ” 2ìœ„ (ìƒì„¸í•¨ì´ ì¥ì ìœ¼ë¡œ ì‘ìš©)\n")
        f.write("- ìš©ë„ì— ë”°ë¼ ì„±ëŠ¥ í¸ì°¨ê°€ í¼\n\n")

        f.write("**DeepSeek-V3.1:**\n")
        f.write("- ê°„ê²°ì„±ì—ì„œ ìµœê³  ì ìˆ˜\n")
        f.write("- ì£¼ê°„ë³´ê³ ì„œ 2ìœ„, ìµœì¢…ë³´ê³ ì„œ 3ìœ„\n")
        f.write("- ë¹ ë¥¸ íŒŒì•…ìš©ìœ¼ë¡œ ì í•©\n")
        f.write("- ë•Œë•Œë¡œ ë§¥ë½ ë¶€ì¡±\n\n")

        f.write("**Claude ëª¨ë¸ë“¤ (Opus 4.5, Sonnet 4.5):**\n")
        f.write("- ì£¼ê°„ë³´ê³ ì„œì—ì„œëŠ” ì¤‘ìƒìœ„ê¶Œ (3-4ìœ„)\n")
        f.write("- ìµœì¢…ë³´ê³ ì„œì—ì„œëŠ” í•˜ìœ„ê¶Œ (4-5ìœ„)\n")
        f.write("- êµ¬ì¡°í™”ì™€ ì¼ê´€ì„±ì€ ì¢‹ìœ¼ë‚˜ ì‹¤ìš©ì„± ë¶€ì¡±\n")
        f.write("- 'ë¬¸ì„œì— ëª…ì‹œë˜ì§€ ì•ŠìŒ' ê³¼ë‹¤ ì‚¬ìš© ê²½í–¥\n\n")

        f.write("**Llama-3.3-70B:**\n")
        f.write("- ë‘ ë³´ê³ ì„œ íƒ€ì… ëª¨ë‘ ìµœí•˜ìœ„ (7ìœ„)\n")
        f.write("- íŠ¹íˆ ìµœì¢…ë³´ê³ ì„œì—ì„œ ë§¤ìš° ì €ì¡° (28ì )\n")
        f.write("- ëª¨ë“  í‰ê°€ ê¸°ì¤€ì—ì„œ ë¶€ì¡±\n\n")

        # ê°œì„  ë°©ì•ˆ
        f.write("## ğŸ”§ ê°œì„  ë°©ì•ˆ ì œì•ˆ\n\n")

        f.write("### 1. ìœ¡ì•ˆ í‰ê°€ í”„ë¡œì„¸ìŠ¤ ê°œì„ \n\n")
        f.write("**í˜„ì¬ ì¥ì :**\n")
        f.write("- ì‹¤ë¬´ ê´€ì ì—ì„œì˜ ì‹¤ìš©ì„± í‰ê°€\n")
        f.write("- ë§¥ë½ê³¼ ë‰˜ì•™ìŠ¤ íŒŒì•… ê°€ëŠ¥\n")
        f.write("- ëª…í™•í•œ í‰ê°€ ê¸°ì¤€ê³¼ ê°€ì¤‘ì¹˜\n\n")

        f.write("**ê°œì„  í•„ìš” ì‚¬í•­:**\n")
        f.write("- âœ… í‰ê°€ì ê°„ ì¼ê´€ì„± í™•ë³´ (í˜„ì¬ëŠ” ë‹¨ì¼ í‰ê°€ì)\n")
        f.write("- âœ… í‰ê°€ ê¸°ì¤€ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì˜ˆì‹œ ì œê³µ\n")
        f.write("- âœ… ë¸”ë¼ì¸ë“œ í‰ê°€ ë„ì… (LLM ì´ë¦„ ìˆ¨ê¹€)\n")
        f.write("- âœ… ë³µìˆ˜ í‰ê°€ìì˜ í‰ê·  ì ìˆ˜ ì‚¬ìš©\n\n")

        f.write("### 2. LLM Judge í‰ê°€ ê°œì„ \n\n")
        f.write("**í•„ìš”í•œ ê°œì„ :**\n")
        f.write("- í˜„ì¬ Judge í‰ê°€ê°€ ì‹¤íŒ¨í•œ ìƒíƒœ (tabulate ì˜¤ë¥˜)\n")
        f.write("- Judge í‰ê°€ ì™„ë£Œ í›„ ìœ¡ì•ˆ í‰ê°€ì™€ ë¹„êµ í•„ìš”\n")
        f.write("- Judge ëª¨ë¸ì˜ í¸í–¥ ë¶„ì„ í•„ìš”\n")
        f.write("- ì—¬ëŸ¬ Judge ëª¨ë¸ ì‚¬ìš©í•˜ì—¬ í•©ì˜ í‰ê°€\n\n")

        f.write("### 3. í•˜ì´ë¸Œë¦¬ë“œ í‰ê°€ ì‹œìŠ¤í…œ\n\n")
        f.write("**ì œì•ˆ:**\n")
        f.write("1. **1ì°¨ í‰ê°€ (LLM Judge)**\n")
        f.write("   - ìë™í™”ëœ ëŒ€ëŸ‰ í‰ê°€\n")
        f.write("   - ê°ê´€ì  ê¸°ì¤€ (ì •í™•ì„±, êµ¬ì¡° ë“±)\n")
        f.write("   - ë¹ ë¥¸ ìŠ¤í¬ë¦¬ë‹\n\n")

        f.write("2. **2ì°¨ í‰ê°€ (ìœ¡ì•ˆ í‰ê°€)**\n")
        f.write("   - ìƒìœ„ê¶Œ LLMì— ëŒ€í•œ ì •ë°€ í‰ê°€\n")
        f.write("   - ì‹¤ë¬´ í™œìš©ë„ í‰ê°€\n")
        f.write("   - ìµœì¢… ì˜ì‚¬ê²°ì •\n\n")

        f.write("3. **í‰ê°€ ê²°ê³¼ í†µí•©**\n")
        f.write("   - Judge ì ìˆ˜ (40%) + ìœ¡ì•ˆ ì ìˆ˜ (60%)\n")
        f.write("   - ë˜ëŠ” Judgeë¡œ í›„ë³´êµ° ì„ ì • â†’ ìœ¡ì•ˆìœ¼ë¡œ ìµœì¢… ì„ íƒ\n\n")

        f.write("### 4. í‰ê°€ ê¸°ì¤€ ì„¸ë¶„í™”\n\n")

        f.write("**ì£¼ê°„ ë³´ê³ ì„œ:**\n")
        f.write("- ê°„ê²°ì„±ì„ 'ì ì ˆí•œ ê¸¸ì´'ì™€ 'ë¶ˆí•„ìš”í•œ ë°˜ë³µ ì œê±°'ë¡œ ë¶„ë¦¬\n")
        f.write("- ì‹¤ë¬´ì„±ì„ 'ì¦‰ì‹œ í™œìš© ê°€ëŠ¥ì„±'ê³¼ 'ì•¡ì…˜ ì•„ì´í…œ ëª…í™•ì„±'ìœ¼ë¡œ ë¶„ë¦¬\n\n")

        f.write("**ìµœì¢… ë³´ê³ ì„œ:**\n")
        f.write("- ì™„ì„±ë„ë¥¼ 'êµ¬ì¡° ì™„ì„±ë„'ì™€ 'ë‚´ìš© ì™„ì„±ë„'ë¡œ ë¶„ë¦¬\n")
        f.write("- ì‹¤ìš©ì„±ì„ 'ì˜ì‚¬ê²°ì • ì§€ì›'ê³¼ 'ì¸ì‚¬ì´íŠ¸ ì œê³µ'ìœ¼ë¡œ ë¶„ë¦¬\n\n")

        f.write("### 5. LLMë³„ ìµœì  í™œìš© ë°©ì•ˆ\n\n")

        f.write("**GPT-4.1:**\n")
        f.write("- ë²”ìš© ë³´ê³ ì„œ ìƒì„±ì— ìµœì \n")
        f.write("- ì‹¤ë¬´ í™œìš©ë„ê°€ ë†’ìŒ\n")
        f.write("- ê¸°ë³¸ ì„ íƒì§€ë¡œ ì¶”ì²œ\n\n")

        f.write("**GPT-5.1:**\n")
        f.write("- ì™¸ë¶€ ì œì¶œìš© ìƒì„¸ ë¬¸ì„œì— ìµœì \n")
        f.write("- ì™„ë²½í•œ ì •í™•ì„±ì´ í•„ìš”í•œ ê²½ìš°\n")
        f.write("- ì£¼ê°„ë³´ê³ ì„œì—ëŠ” ë¶€ì í•© (ìš”ì•½ í”„ë¡¬í”„íŠ¸ ì¶”ê°€ í•„ìš”)\n\n")

        f.write("**DeepSeek-V3.1:**\n")
        f.write("- ë¹ ë¥¸ ìƒí™© íŒŒì•…ìš©\n")
        f.write("- ê°„ê²°í•œ ìš”ì•½ì´ í•„ìš”í•œ ê²½ìš°\n")
        f.write("- ë¹„ìš© íš¨ìœ¨ì \n\n")

        f.write("**Claude ëª¨ë¸ë“¤:**\n")
        f.write("- êµ¬ì¡°í™”ëœ ë¬¸ì„œì— ì í•©\n")
        f.write("- í”„ë¡¬í”„íŠ¸ ê°œì„  í•„ìš” ('ë¬¸ì„œì— ëª…ì‹œë˜ì§€ ì•ŠìŒ' ê³¼ë‹¤ ì‚¬ìš© ë°©ì§€)\n")
        f.write("- í…Œì´ë¸” í™œìš© ëŠ¥ë ¥ ìš°ìˆ˜\n\n")

    print(f"\nğŸ“ ì¸ì‚¬ì´íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±: {report_path}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""

    # ë°ì´í„° ë””ë ‰í† ë¦¬
    data_dir = Path("data/results/multi_llm_test")
    human_eval_dir = data_dir / "human_evaluation"

    # ìœ¡ì•ˆ í‰ê°€ ë¡œë“œ
    weekly_human = load_human_evaluation(
        human_eval_dir / "weekly_human_evaluation_judge_format.json"
    )
    executive_human = load_human_evaluation(
        human_eval_dir / "executive_human_evaluation_judge_format.json"
    )

    # Judge í‰ê°€ ë¡œë“œ (ìˆë‹¤ë©´)
    weekly_judge = load_judge_evaluation(
        data_dir / "weekly/20260102_052136/multi_judge_evaluation/llm_judge_evaluation_weekly.json"
    )
    executive_judge = load_judge_evaluation(
        data_dir / "executive/20260102_053212/multi_judge_evaluation/llm_judge_evaluation_executive.json"
    )

    # ìœ¡ì•ˆ í‰ê°€ ë¶„ì„
    weekly_analysis = analyze_human_evaluation_patterns(weekly_human, "weekly")
    executive_analysis = analyze_human_evaluation_patterns(executive_human, "executive")

    # Judge í‰ê°€ì™€ ë¹„êµ (ìˆë‹¤ë©´)
    if weekly_judge:
        compare_with_judge(weekly_human, weekly_judge, "weekly")

    if executive_judge:
        compare_with_judge(executive_human, executive_judge, "executive")

    # ì¶œë ¥ ë””ë ‰í† ë¦¬
    output_dir = data_dir / "evaluation_analysis"
    output_dir.mkdir(parents=True, exist_ok=True)

    # ì¸ì‚¬ì´íŠ¸ ë° ê°œì„  ë°©ì•ˆ ë¦¬í¬íŠ¸ ìƒì„±
    generate_insights_and_recommendations(
        weekly_analysis,
        executive_analysis,
        output_dir
    )

    # ë¶„ì„ ê²°ê³¼ ì €ì¥
    with open(output_dir / "weekly_analysis.json", 'w', encoding='utf-8') as f:
        json.dump(weekly_analysis, f, indent=2, ensure_ascii=False, default=str)

    with open(output_dir / "executive_analysis.json", 'w', encoding='utf-8') as f:
        json.dump(executive_analysis, f, indent=2, ensure_ascii=False, default=str)

    print(f"\nâœ… ë¶„ì„ ì™„ë£Œ!")
    print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {output_dir}")


if __name__ == "__main__":
    main()
