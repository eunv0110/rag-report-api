#!/usr/bin/env python3
"""ì£¼ê°„/ìµœì¢…ë³´ê³ ì„œ ìµœì  ì¡°í•© ì¬í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

ì£¼ê°„ë³´ê³ ì„œ: Qwen3-Reranker-4B + BGE-M3 + RRF Ensemble (Top 6)
ìµœì¢…ë³´ê³ ì„œ: OpenAI + RRF MultiQuery (Top 8)

ë‹µë³€ ìƒì„±: GPT-4 Turbo (1106-preview)
ì¶”ì : Langfuse

ì‚¬ìš©ë²•:
    # ì£¼ê°„ë³´ê³ ì„œ í‰ê°€
    python re_evaluate_optimal.py --report-type weekly
    
    # ìµœì¢…ë³´ê³ ì„œ í‰ê°€
    python re_evaluate_optimal.py --report-type executive
    
    # ë‘˜ ë‹¤ í‰ê°€
    python re_evaluate_optimal.py --report-type both
"""

import sys
from pathlib import Path
# í”„ë¡œì íŠ¸ ë£¨íŠ¸ì™€ app, experiments ë””ë ‰í† ë¦¬ë¥¼ pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "app"))
sys.path.insert(0, str(project_root / "experiments"))

from dotenv import load_dotenv
load_dotenv()

import json
import time
from datetime import datetime
from typing import List, Dict, Any
from concurrent.futures import ThreadPoolExecutor, as_completed

from utils.langfuse import get_langfuse_client
from utils.common import (
    load_prompt,
    load_evaluation_dataset,
    create_trace_and_generation,
    add_retrieval_quality_score,
    save_embedding_cache
)
from utils.retriever_factory import create_retriever_from_config

# Reranker ê´€ë ¨ ì„í¬íŠ¸
import torch
from sentence_transformers import CrossEncoder

# Reranker ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ (ì „ì—­ ë³€ìˆ˜ë¡œ í•œ ë²ˆë§Œ ë¡œë“œ)
QWEN3_RERANKER = None


def format_query(query: str, instruction: str = None) -> str:
    """Qwen3-Rerankerë¥¼ ìœ„í•œ ì¿¼ë¦¬ í¬ë§·íŒ…"""
    prefix = '<|im_start|>system\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be "yes" or "no".<|im_end|>\n<|im_start|>user\n'
    if instruction is None:
        instruction = "Given a query, retrieve relevant passages that answer the query"
    return f"{prefix}<Instruct>: {instruction}\n<Query>: {query}\n"


def format_document(document: str) -> str:
    """Qwen3-Rerankerë¥¼ ìœ„í•œ ë¬¸ì„œ í¬ë§·íŒ…"""
    suffix = "<|im_end|>\n<|im_start|>assistant\n<think>\n\n</think>\n\n"
    return f"<Document>: {document}{suffix}"


def get_optimal_batch_size() -> int:
    """GPU ë©”ëª¨ë¦¬ì— ë”°ë¥¸ ìµœì  ë°°ì¹˜ í¬ê¸° ê³„ì‚°"""
    # VLLMê³¼ ê³µì¡´í•˜ê¸° ìœ„í•´ ë°°ì¹˜ í¬ê¸°ë¥¼ 8ìœ¼ë¡œ ê³ ì •
    return 8


def get_qwen3_reranker() -> CrossEncoder:
    """Qwen3-Reranker-4B ëª¨ë¸ ë¡œë“œ (ìºì‹±)"""
    global QWEN3_RERANKER

    # ì´ë¯¸ ë¡œë“œëœ ëª¨ë¸ì´ ìˆìœ¼ë©´ ì¬ì‚¬ìš©
    if QWEN3_RERANKER is not None:
        return QWEN3_RERANKER

    # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    device = 'cuda' if torch.cuda.is_available() else 'cpu'

    print(f"ğŸ”„ Qwen3-Reranker-4B ëª¨ë¸ ë¡œë”© ì¤‘... (device: {device})")
    QWEN3_RERANKER = CrossEncoder(
        "tomaarsen/Qwen3-Reranker-4B-seq-cls",
        max_length=8192,
        device=device,
        trust_remote_code=True
    )
    print("âœ… Qwen3-Reranker-4B ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

    # ìµœì  ë°°ì¹˜ í¬ê¸° ì¶œë ¥
    optimal_bs = get_optimal_batch_size()
    print(f"ğŸ’¡ ê¶Œì¥ ë°°ì¹˜ í¬ê¸°: {optimal_bs}")

    return QWEN3_RERANKER


def rerank_documents(
    query: str,
    docs: list,
    top_k: int = 6,
    batch_size: int = None,
    initial_k: int = None
) -> list:
    """Qwen3-Reranker-4B ëª¨ë¸ë¡œ ë¬¸ì„œ ì¬ìˆœìœ„í™”"""
    # Reranker ëª¨ë¸ ë¡œë“œ
    reranker = get_qwen3_reranker()

    # ë°°ì¹˜ í¬ê¸° ìë™ ì„¤ì •
    if batch_size is None:
        batch_size = get_optimal_batch_size()

    # ì´ˆê¸° ë¬¸ì„œ ìˆ˜ ì„¤ì •
    if initial_k is None:
        initial_k = len(docs)

    # Qwen3-Reranker í¬ë§·ìœ¼ë¡œ query-document ìŒ ìƒì„±
    formatted_query = format_query(query)
    pairs = [
        [formatted_query, format_document(doc.page_content)]
        for doc in docs
    ]

    # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì¬ìˆœìœ„í™” ì ìˆ˜ ê³„ì‚° (ë©”ëª¨ë¦¬ ì ˆì•½)
    all_scores = []
    for i in range(0, len(pairs), batch_size):
        batch_pairs = pairs[i:i + batch_size]
        batch_scores = reranker.predict(batch_pairs)
        all_scores.extend(batch_scores)

        # ë°°ì¹˜ ì²˜ë¦¬ í›„ GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    # ì ìˆ˜ì™€ ë¬¸ì„œë¥¼ í•¨ê»˜ ì •ë ¬ (ë‚´ë¦¼ì°¨ìˆœ)
    doc_score_pairs = list(zip(docs, all_scores))
    doc_score_pairs.sort(key=lambda x: x[1], reverse=True)

    # ìƒìœ„ kê°œ ë¬¸ì„œë§Œ ë°˜í™˜
    reranked_docs = [doc for doc, score in doc_score_pairs[:top_k]]

    print(f"\nğŸ”„ Qwen3 Reranking ì™„ë£Œ: {initial_k}ê°œ â†’ {len(reranked_docs)}ê°œ (ë°°ì¹˜ í¬ê¸°: {batch_size})")
    print("Top 3 Reranked Scores:")
    for i, (doc, score) in enumerate(doc_score_pairs[:3], 1):
        print(f"  {i}. {doc.metadata.get('page_title', 'Unknown')}: {score:.4f}")

    return reranked_docs


def generate_answer(
    question: str,
    contexts: List[str],
    system_prompt: str,
    answer_generation_prompt: str,
    langfuse=None,
    trace_id: str = None
) -> str:
    """GPT-4 Turbo (1106-preview)ë¡œ ë‹µë³€ ìƒì„±"""
    from langchain.chat_models import init_chat_model
    from langchain_core.messages import SystemMessage, HumanMessage

    # Context êµ¬ì„±
    context_parts = []
    for i, ctx in enumerate(contexts, 1):
        context_parts.append(f"[ë¬¸ì„œ {i}]\n{ctx}\n")

    context_text = "\n".join(context_parts)

    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    user_prompt = answer_generation_prompt.replace("{context}", context_text).replace("{question}", question)

    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_prompt)
    ]

    # init_chat_modelë¡œ ëª¨ë¸ ì´ˆê¸°í™”
    model = init_chat_model("azure_ai:gpt-4.1")

    # Langfuse ì¶”ì 
    if langfuse and trace_id:
        with langfuse.start_as_current_observation(
            as_type='generation',
            name="final_answer_generation",
            model="gpt-4-1106-preview",
            input={"question": question, "num_contexts": len(contexts)},
            metadata={"model": "gpt-4.1"}
        ) as generation:
            response = model.invoke(messages)
            answer = response.content

            # í† í° ì‚¬ìš©ëŸ‰ ê¸°ë¡
            usage_dict = None
            if hasattr(response, 'usage_metadata') and response.usage_metadata:
                usage_dict = {
                    "input": response.usage_metadata.get('input_tokens', 0),
                    "output": response.usage_metadata.get('output_tokens', 0),
                    "total": response.usage_metadata.get('total_tokens', 0)
                }

            generation.update(
                output={"answer": answer},
                usage=usage_dict
            )
    else:
        response = model.invoke(messages)
        answer = response.content

    return answer


def evaluate_weekly_report(
    item: Dict[str, Any],
    langfuse,
    idx: int,
    system_prompt: str,
    answer_generation_prompt: str,
    version_tag: str = "optimal_v1"
) -> Dict[str, Any]:
    """ì£¼ê°„ë³´ê³ ì„œ í‰ê°€: BGE-M3 + RRF + Qwen3-Reranker (Top 6)"""
    question = item["question"]
    ground_truth = item["ground_truth"]
    context_page_id = item.get("context_page_id")
    item_metadata = item.get("metadata", {})
    
    start_time = time.time()
    
    # 1. BGE-M3 + RRF Ensembleë¡œ ì´ˆê¸° ê²€ìƒ‰ (20ê°œ)
    retriever_config = {
        "name": "bge-m3-rrf-ensemble",
        "display_name": "BGE-M3 + RRF Ensemble",
        "description": "BGE-M3 embedding with RRF ensemble",
        "embedding_preset": "bge-m3",
        "retriever_type": "rrf_ensemble",
        "k": 20,
        "bm25_weight": 0.5,
        "dense_weight": 0.5,
        "use_mmr": True,
        "lambda_mult": 0.5
    }

    retriever, _ = create_retriever_from_config(retriever_config)
    
    print(f"  [{idx}] ğŸ” BGE-M3 + RRF ê²€ìƒ‰ ì¤‘...")
    initial_docs = retriever.invoke(question)
    print(f"  [{idx}] ğŸ“„ ì´ˆê¸° ê²€ìƒ‰: {len(initial_docs)}ê°œ ë¬¸ì„œ")
    
    # 2. Qwen3-Rerankerë¡œ ì¬ìˆœìœ„í™” (Top 6)
    print(f"  [{idx}] ğŸ”„ Qwen3 Reranking ì¤‘...")
    reranked_docs = rerank_documents(question, initial_docs, top_k=6, batch_size=16)
    
    # ì»¨í…ìŠ¤íŠ¸ ë³€í™˜
    contexts = []
    context_metadata = []
    
    for result in reranked_docs:
        contexts.append(result.page_content)
        context_metadata.append({
            "page_title": result.metadata.get('page_title', 'Unknown'),
            "section_title": result.metadata.get('section_title', 'N/A'),
            "chunk_id": result.metadata.get('chunk_id', 'unknown'),
            "score": result.metadata.get('_combined_score') or result.metadata.get('_similarity_score')
        })
    
    if not contexts:
        print(f"  âš ï¸ [{idx}] No contexts found!")
        contexts = ["ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."]
    
    # 3. GPT-4ë¡œ ë‹µë³€ ìƒì„±
    print(f"  [{idx}] ğŸ’¬ GPT-4ë¡œ ë‹µë³€ ìƒì„± ì¤‘...")
    
    # Langfuse Trace ìƒì„±
    retriever_name = "bge-m3-rrf-qwen3-reranker-k6"
    retriever_tags = ["bge-m3", "rrf_ensemble", "qwen3-reranker", "top_k_6", "weekly_report"]
    
    additional_metadata = {
        "context_page_id": context_page_id,
        "retriever_name": retriever_name,
        "display_name": "BGE-M3 + RRF + Qwen3-Reranker (Top 6)",
        "report_type": "weekly_report",
        "top_k": 6,
        "embedding_preset": "bge-m3",
        "retriever_type": "rrf_ensemble_reranker",
        "reranker_model": "Qwen3-Reranker-4B",
        "llm_model": "gpt-4-1106-preview"
    }
    
    trace_id = create_trace_and_generation(
        langfuse=langfuse,
        retriever_name=retriever_name,
        question=question,
        contexts=contexts,
        answer="",  # ì„ì‹œ
        ground_truth=ground_truth,
        context_metadata=context_metadata,
        item_metadata=item_metadata,
        total_time=0,  # ì„ì‹œ
        idx=idx,
        version_tag=version_tag,
        retriever_tags=retriever_tags,
        additional_metadata=additional_metadata
    )
    
    # GPT-4 Turbo ë‹µë³€ ìƒì„±
    answer = generate_answer(
        question=question,
        contexts=contexts,
        system_prompt=system_prompt,
        answer_generation_prompt=answer_generation_prompt,
        langfuse=langfuse,
        trace_id=trace_id
    )
    
    total_time = time.time() - start_time
    
    # Trace ì—…ë°ì´íŠ¸
    if langfuse and trace_id:
        langfuse.update_current_trace(
            output={"answer": answer},
            metadata={**additional_metadata, "total_time": total_time}
        )
    
    # ê²€ìƒ‰ í’ˆì§ˆ ìŠ¤ì½”ì–´ ì¶”ê°€
    add_retrieval_quality_score(langfuse, trace_id, context_metadata)
    
    print(f"  [{idx}] âœ… ì™„ë£Œ ({len(contexts)}ê°œ ë¬¸ì„œ, {total_time*1000:.0f}ms)")
    
    return {
        "question": question,
        "answer": answer,
        "ground_truth": ground_truth,
        "num_contexts": len(contexts),
        "time": total_time,
        "trace_id": trace_id,
        "context_metadata": context_metadata
    }


def evaluate_executive_report(
    item: Dict[str, Any],
    langfuse,
    idx: int,
    system_prompt: str,
    answer_generation_prompt: str,
    version_tag: str = "optimal_v1"
) -> Dict[str, Any]:
    """ìµœì¢…ë³´ê³ ì„œ í‰ê°€: OpenAI + RRF MultiQuery (Top 8)"""
    question = item["question"]
    ground_truth = item["ground_truth"]
    context_page_id = item.get("context_page_id")
    item_metadata = item.get("metadata", {})
    
    start_time = time.time()
    
    # OpenAI + RRF MultiQuery ì„¤ì •
    retriever_config = {
        "name": "openai-rrf-multiquery",
        "display_name": "OpenAI + RRF MultiQuery",
        "description": "OpenAI embedding with RRF multiquery",
        "embedding_preset": "openai",
        "retriever_type": "rrf_multiquery",
        "k": 8,
        "bm25_weight": 0.5,
        "dense_weight": 0.5,
        "use_mmr": True,
        "lambda_mult": 0.5
    }

    retriever, _ = create_retriever_from_config(retriever_config)
    
    print(f"  [{idx}] ğŸ” OpenAI + RRF MultiQuery ê²€ìƒ‰ ì¤‘...")
    search_results = retriever.invoke(question)
    
    # ì»¨í…ìŠ¤íŠ¸ ë³€í™˜
    contexts = []
    context_metadata = []
    
    for result in search_results[:8]:
        contexts.append(result.page_content)
        context_metadata.append({
            "page_title": result.metadata.get('page_title', 'Unknown'),
            "section_title": result.metadata.get('section_title', 'N/A'),
            "chunk_id": result.metadata.get('chunk_id', 'unknown'),
            "score": result.metadata.get('_combined_score') or result.metadata.get('_similarity_score')
        })
    
    if not contexts:
        print(f"  âš ï¸ [{idx}] No contexts found!")
        contexts = ["ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."]
    
    print(f"  [{idx}] ğŸ“„ ê²€ìƒ‰ ì™„ë£Œ: {len(contexts)}ê°œ ë¬¸ì„œ")
    
    # GPT-4 Turboë¡œ ë‹µë³€ ìƒì„±
    print(f"  [{idx}] ğŸ’¬ GPT-4 Turbo ë‹µë³€ ìƒì„± ì¤‘...")
    
    # Langfuse Trace ìƒì„±
    retriever_name = "openai-rrf-multiquery-k8"
    retriever_tags = ["openai", "rrf_multiquery", "top_k_8", "executive_report"]
    
    additional_metadata = {
        "context_page_id": context_page_id,
        "retriever_name": retriever_name,
        "display_name": "OpenAI + RRF MultiQuery (Top 8)",
        "report_type": "executive_report",
        "top_k": 8,
        "embedding_preset": "openai",
        "retriever_type": "rrf_multiquery",
        "llm_model": "gpt-4-1106-preview"
    }
    
    trace_id = create_trace_and_generation(
        langfuse=langfuse,
        retriever_name=retriever_name,
        question=question,
        contexts=contexts,
        answer="",  # ì„ì‹œ
        ground_truth=ground_truth,
        context_metadata=context_metadata,
        item_metadata=item_metadata,
        total_time=0,  # ì„ì‹œ
        idx=idx,
        version_tag=version_tag,
        retriever_tags=retriever_tags,
        additional_metadata=additional_metadata
    )
    
    # GPT-4 Turbo ë‹µë³€ ìƒì„±
    answer = generate_answer(
        question=question,
        contexts=contexts,
        system_prompt=system_prompt,
        answer_generation_prompt=answer_generation_prompt,
        langfuse=langfuse,
        trace_id=trace_id
    )
    
    total_time = time.time() - start_time
    
    # Trace ì—…ë°ì´íŠ¸
    if langfuse and trace_id:
        langfuse.update_current_trace(
            output={"answer": answer},
            metadata={**additional_metadata, "total_time": total_time}
        )
    
    # ê²€ìƒ‰ í’ˆì§ˆ ìŠ¤ì½”ì–´ ì¶”ê°€
    add_retrieval_quality_score(langfuse, trace_id, context_metadata)
    
    print(f"  [{idx}] âœ… ì™„ë£Œ ({len(contexts)}ê°œ ë¬¸ì„œ, {total_time*1000:.0f}ms)")
    
    return {
        "question": question,
        "answer": answer,
        "ground_truth": ground_truth,
        "num_contexts": len(contexts),
        "time": total_time,
        "trace_id": trace_id,
        "context_metadata": context_metadata
    }


def run_evaluation(
    report_type: str,
    dataset_path: str,
    version: str = "optimal_v1",
    max_workers: int = 3
):
    """í‰ê°€ ì‹¤í–‰"""
    
    # Langfuse í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    langfuse = get_langfuse_client()
    if not langfuse:
        print("âŒ Langfuse í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # í”„ë¡¬í”„íŠ¸ ë¡œë“œ
    report_suffix = "weekly_report" if report_type == "weekly" else "executive_report_v3"
    system_prompt = load_prompt(f"prompts/templates/service/{report_suffix}/system_prompt.txt")
    answer_generation_prompt = load_prompt(f"prompts/templates/service/{report_suffix}/answer_generation_prompt.txt")
    
    # ë°ì´í„°ì…‹ ë¡œë“œ
    eval_data = load_evaluation_dataset(dataset_path)
    
    report_display = "ì£¼ê°„ë³´ê³ ì„œ" if report_type == "weekly" else "ìµœì¢…ë³´ê³ ì„œ"
    
    if report_type == "weekly":
        config_display = "BGE-M3 + RRF + Qwen3-Reranker (Top 6)"
    else:
        config_display = "OpenAI + RRF MultiQuery (Top 8)"
    
    print("\n" + "=" * 80)
    print(f"ğŸ¯ {report_display} ìµœì  ì¡°í•© ì¬í‰ê°€")
    print("=" * 80)
    print(f"ğŸ“Š ì„¤ì •: {config_display}")
    print(f"ğŸ’¬ LLM: GPT-4 Turbo (1106-preview)")
    print(f"ğŸ“‹ ë°ì´í„°ì…‹: {len(eval_data)} ê°œ ìƒ˜í”Œ")
    print(f"ğŸ·ï¸  Version: {version}")
    print(f"âš¡ ë³‘ë ¬ ì›Œì»¤: {max_workers}")
    print()
    
    stats = {
        "total_queries": len(eval_data),
        "total_time": 0,
        "evaluations": []
    }

    # ë³‘ë ¬ ì²˜ë¦¬ ì „ì— Reranker ëª¨ë¸ ë¯¸ë¦¬ ë¡œë“œ (race condition ë°©ì§€)
    if report_type == "weekly":
        print("ğŸ”„ Reranker ëª¨ë¸ ì‚¬ì „ ë¡œë“œ ì¤‘...")
        get_qwen3_reranker()
        print("âœ… Reranker ëª¨ë¸ ì‚¬ì „ ë¡œë“œ ì™„ë£Œ!\n")

    # ë³‘ë ¬ ì²˜ë¦¬ë¡œ í‰ê°€ ì‹¤í–‰
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        if report_type == "weekly":
            future_to_item = {
                executor.submit(
                    evaluate_weekly_report,
                    item,
                    langfuse,
                    idx,
                    system_prompt,
                    answer_generation_prompt,
                    version
                ): (idx, item)
                for idx, item in enumerate(eval_data, 1)
            }
        else:
            future_to_item = {
                executor.submit(
                    evaluate_executive_report,
                    item,
                    langfuse,
                    idx,
                    system_prompt,
                    answer_generation_prompt,
                    version
                ): (idx, item)
                for idx, item in enumerate(eval_data, 1)
            }
        
        for future in as_completed(future_to_item):
            idx, item = future_to_item[future]
            try:
                eval_result = future.result()
                stats["evaluations"].append(eval_result)
                stats["total_time"] += eval_result["time"]
                
                # ìºì‹œ ì €ì¥ (ì£¼ê¸°ì ìœ¼ë¡œ)
                if idx % 5 == 0:
                    save_embedding_cache()
                    
            except Exception as e:
                print(f"  âŒ [{idx}] í‰ê°€ ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()
    
    stats["avg_time"] = stats["total_time"] / stats["total_queries"] if stats["total_queries"] > 0 else 0
    stats["avg_contexts"] = sum(e["num_contexts"] for e in stats["evaluations"]) / len(stats["evaluations"]) if stats["evaluations"] else 0
    
    # ê²°ê³¼ ì €ì¥
    output_dir = Path("data/langfuse/evaluation_results") / f"{report_type}_report_optimal"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = output_dir / f"optimal_{timestamp}_stats.json"
    
    save_result = {k: v for k, v in stats.items() if k != "evaluations"}
    save_result["num_evaluations"] = len(stats.get("evaluations", []))
    save_result["config"] = {
        "report_type": report_type,
        "retriever": config_display,
        "llm": "GPT-4 Turbo (1106-preview)",
        "version": version,
        "timestamp": timestamp
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(save_result, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"\nâœ… í‰ê°€ ì™„ë£Œ!")
    print(f"   - í‰ê·  ì‹œê°„: {stats['avg_time']*1000:.2f}ms")
    print(f"   - í‰ê·  ì»¨í…ìŠ¤íŠ¸ ìˆ˜: {stats['avg_contexts']:.2f}")
    print(f"   - ê²°ê³¼ ì €ì¥: {output_file}")
    
    # Langfuse flush
    print("\nâ³ Langfuseì— ë°ì´í„° ì „ì†¡ ì¤‘...")
    langfuse.flush()
    
    # ì„ë² ë”© ìºì‹œ ì €ì¥
    print("\nğŸ’¾ ì„ë² ë”© ìºì‹œ ì €ì¥ ì¤‘...")
    save_embedding_cache()
    
    return stats


def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description="ì£¼ê°„/ìµœì¢…ë³´ê³ ì„œ ìµœì  ì¡°í•© ì¬í‰ê°€",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )
    
    parser.add_argument(
        "--report-type",
        type=str,
        choices=["weekly", "executive", "both"],
        default="both",
        help="í‰ê°€í•  ë³´ê³ ì„œ íƒ€ì…"
    )
    
    parser.add_argument(
        "--dataset",
        type=str,
        default="/home/work/rag/Project/rag-report-generator/data/evaluation/merged_qa_dataset.json",
        help="í‰ê°€ ë°ì´í„°ì…‹ ê²½ë¡œ"
    )
    
    parser.add_argument(
        "--version",
        type=str,
        default="optimal_v1",
        help="ë²„ì „ íƒœê·¸"
    )
    
    parser.add_argument(
        "--max-workers",
        type=int,
        default=3,
        help="ë³‘ë ¬ ì²˜ë¦¬ ì›Œì»¤ ìˆ˜"
    )
    
    args = parser.parse_args()
    
    print("=" * 80)
    print("ğŸ¯ ì£¼ê°„/ìµœì¢…ë³´ê³ ì„œ ìµœì  ì¡°í•© ì¬í‰ê°€ ì‹œìŠ¤í…œ")
    print("=" * 80)
    print(f"\nğŸ“Š í‰ê°€ ì„¤ì •:")
    print(f"   - ë³´ê³ ì„œ íƒ€ì…: {args.report_type}")
    print(f"   - Dataset: {args.dataset}")
    print(f"   - Version: {args.version}")
    print(f"   - Max Workers: {args.max_workers}")
    
    if args.report_type in ["weekly", "both"]:
        print("\n" + "=" * 80)
        print("ğŸ“Š ì£¼ê°„ë³´ê³ ì„œ í‰ê°€")
        print("=" * 80)
        run_evaluation(
            report_type="weekly",
            dataset_path=args.dataset,
            version=args.version,
            max_workers=args.max_workers
        )
    
    if args.report_type in ["executive", "both"]:
        print("\n" + "=" * 80)
        print("ğŸ“Š ìµœì¢…ë³´ê³ ì„œ í‰ê°€")
        print("=" * 80)
        run_evaluation(
            report_type="executive",
            dataset_path=args.dataset,
            version=args.version,
            max_workers=args.max_workers
        )
    
    print("\n" + "=" * 80)
    print("âœ… ëª¨ë“  í‰ê°€ ì™„ë£Œ!")
    print("=" * 80)
    print("\nğŸ“Š Langfuse ëŒ€ì‹œë³´ë“œì—ì„œ ê²°ê³¼ í™•ì¸: https://cloud.langfuse.com")


if __name__ == "__main__":
    main()
