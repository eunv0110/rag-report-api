#!/usr/bin/env python3
"""LLM ì„±ëŠ¥ ë¹„êµ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸

ë‹¤ì–‘í•œ LLM ëª¨ë¸ì˜ ì„±ëŠ¥ì„ GPT ëŒ€ë¹„ ë¹„ìœ¨ë¡œ ë¹„êµí•˜ì—¬ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
"""

import sys
from pathlib import Path

# Python ê²½ë¡œ ì„¤ì •
sys.path.insert(0, str(Path(__file__).parent.parent))

import json
from typing import Dict, List, Any
from datetime import datetime
import pandas as pd


def load_evaluation_results(eval_dir: Path) -> Dict[str, Any]:
    """í‰ê°€ ê²°ê³¼ ë¡œë“œ

    Args:
        eval_dir: í‰ê°€ ê²°ê³¼ ë””ë ‰í† ë¦¬

    Returns:
        í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    overall_eval_file = eval_dir / "overall_evaluation.json"

    if not overall_eval_file.exists():
        return None

    with open(overall_eval_file, 'r', encoding='utf-8') as f:
        return json.load(f)


def extract_llm_scores(evaluation_data: Dict[str, Any]) -> Dict[str, float]:
    """í‰ê°€ ë°ì´í„°ì—ì„œ LLMë³„ í‰ê·  ì ìˆ˜ ì¶”ì¶œ

    Args:
        evaluation_data: í‰ê°€ ê²°ê³¼ ë°ì´í„°

    Returns:
        LLMë³„ í‰ê·  ì ìˆ˜ ë”•ì…”ë„ˆë¦¬
    """
    llm_scores = {}

    for eval_item in evaluation_data.get('evaluations', []):
        for llm_name, llm_result in eval_item.get('results', {}).items():
            if llm_name not in llm_scores:
                llm_scores[llm_name] = []

            final_score = llm_result.get('final_score', 0)
            llm_scores[llm_name].append(final_score)

    # í‰ê·  ê³„ì‚°
    avg_scores = {
        llm: sum(scores) / len(scores) if scores else 0
        for llm, scores in llm_scores.items()
    }

    return avg_scores


def compare_with_baseline(
    llm_scores: Dict[str, float],
    baseline_model: str = "gpt41"
) -> Dict[str, Dict[str, float]]:
    """ë² ì´ìŠ¤ë¼ì¸(GPT) ëŒ€ë¹„ ì„±ëŠ¥ ë¹„ìœ¨ ê³„ì‚°

    Args:
        llm_scores: LLMë³„ í‰ê·  ì ìˆ˜
        baseline_model: ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ëª… (ê¸°ë³¸: gpt41)

    Returns:
        LLMë³„ ë¹„êµ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    baseline_score = llm_scores.get(baseline_model, 0)

    if baseline_score == 0:
        print(f"âš ï¸ ê²½ê³ : {baseline_model}ì˜ ì ìˆ˜ê°€ 0ì…ë‹ˆë‹¤.")
        return {}

    comparison = {}

    for llm_name, score in llm_scores.items():
        comparison[llm_name] = {
            "score": score,
            "percentage": (score / baseline_score) * 100,
            "diff": score - baseline_score
        }

    return comparison


def analyze_multi_judge_results(test_dir: Path, report_type: str) -> pd.DataFrame:
    """ì—¬ëŸ¬ Judge ëª¨ë¸ì˜ í‰ê°€ ê²°ê³¼ ë¶„ì„

    Args:
        test_dir: í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë””ë ‰í† ë¦¬
        report_type: ë³´ê³ ì„œ íƒ€ì…

    Returns:
        ë¶„ì„ ê²°ê³¼ DataFrame
    """
    judge_eval_dir = test_dir / "multi_judge_evaluation"

    if not judge_eval_dir.exists():
        print(f"âš ï¸ í‰ê°€ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {judge_eval_dir}")
        return None

    all_judge_results = []

    # ê° Judge ëª¨ë¸ë³„ í‰ê°€ ê²°ê³¼ ìˆ˜ì§‘
    for judge_dir in judge_eval_dir.iterdir():
        if not judge_dir.is_dir():
            continue

        if not judge_dir.name.startswith("evaluation_"):
            continue

        judge_name = judge_dir.name.replace("evaluation_", "")

        eval_data = load_evaluation_results(judge_dir)
        if not eval_data:
            print(f"âš ï¸ {judge_name} í‰ê°€ ê²°ê³¼ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            continue

        llm_scores = extract_llm_scores(eval_data)

        # gpt41 ëŒ€ë¹„ ë¹„êµ
        comparison = compare_with_baseline(llm_scores, baseline_model="gpt41")

        for llm_name, comp_result in comparison.items():
            all_judge_results.append({
                "judge": judge_name,
                "report_type": report_type,
                "llm": llm_name,
                "score": comp_result["score"],
                "vs_gpt41_percentage": comp_result["percentage"],
                "vs_gpt41_diff": comp_result["diff"]
            })

    if not all_judge_results:
        return None

    df = pd.DataFrame(all_judge_results)
    return df


def generate_markdown_report(
    weekly_df: pd.DataFrame,
    executive_df: pd.DataFrame,
    output_file: Path
):
    """ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„±

    Args:
        weekly_df: ì£¼ê°„ ë³´ê³ ì„œ ë¶„ì„ ê²°ê³¼
        executive_df: ì„ì› ë³´ê³ ì„œ ë¶„ì„ ê²°ê³¼
        output_file: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
    """
    lines = []

    lines.append("# LLM ì„±ëŠ¥ ë¹„êµ ë¶„ì„ ë¦¬í¬íŠ¸")
    lines.append("")
    lines.append(f"**ìƒì„± ì¼ì‹œ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append("")
    lines.append("---")
    lines.append("")

    # ìš”ì•½
    lines.append("## Executive Summary")
    lines.append("")
    lines.append("ë‹¤ì–‘í•œ LLM ëª¨ë¸ì˜ ì„±ëŠ¥ì„ GPT-4.1 ëŒ€ë¹„ ìƒëŒ€ ì„±ëŠ¥ìœ¼ë¡œ ë¹„êµ ë¶„ì„í–ˆìŠµë‹ˆë‹¤.")
    lines.append("3ê°œì˜ Judge ëª¨ë¸(GPT-5.1, Claude Opus 4.5, DeepSeek-V3.1)ì„ ì‚¬ìš©í•˜ì—¬ ê°ê´€ì ì¸ í‰ê°€ë¥¼ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.")
    lines.append("")

    # ì£¼ê°„ ë³´ê³ ì„œ ë¶„ì„
    if weekly_df is not None and not weekly_df.empty:
        lines.append("## 1. ì£¼ê°„ ë³´ê³ ì„œ (Weekly Report)")
        lines.append("")
        lines.append("**ì„¤ì •**: BGE-M3 + Qwen3-Reranker-4B + RRF Ensemble (Top 6)")
        lines.append("")

        # Judgeë³„ í‰ê·  ê³„ì‚°
        weekly_avg = weekly_df.groupby("llm").agg({
            "score": "mean",
            "vs_gpt41_percentage": "mean",
            "vs_gpt41_diff": "mean"
        }).round(2)

        weekly_avg = weekly_avg.sort_values("score", ascending=False)

        lines.append("### í‰ê·  ì„±ëŠ¥ (3ê°œ Judge í‰ê· )")
        lines.append("")
        lines.append("| ìˆœìœ„ | LLM | í‰ê·  ì ìˆ˜ | GPT-4.1 ëŒ€ë¹„ | ì ìˆ˜ ì°¨ì´ |")
        lines.append("|------|-----|-----------|--------------|-----------|")

        for rank, (llm, row) in enumerate(weekly_avg.iterrows(), 1):
            percentage_str = f"{row['vs_gpt41_percentage']:.1f}%"
            diff_str = f"{row['vs_gpt41_diff']:+.2f}"

            # íŠ¹ë³„ í‘œì‹œ
            if llm == "gpt41":
                llm_display = f"**{llm}** (ê¸°ì¤€)"
            elif llm == "deepseek_v31":
                llm_display = f"**{llm}** â­"
            else:
                llm_display = llm

            lines.append(f"| {rank} | {llm_display} | {row['score']:.2f} | {percentage_str} | {diff_str} |")

        lines.append("")

        # Judgeë³„ ìƒì„¸ ê²°ê³¼
        lines.append("### Judgeë³„ ìƒì„¸ ì ìˆ˜")
        lines.append("")

        weekly_pivot = weekly_df.pivot_table(
            index="llm",
            columns="judge",
            values="score",
            aggfunc="mean"
        ).round(2)

        lines.append(weekly_pivot.to_markdown())
        lines.append("")

    # ì„ì› ë³´ê³ ì„œ ë¶„ì„
    if executive_df is not None and not executive_df.empty:
        lines.append("## 2. ì„ì› ë³´ê³ ì„œ (Executive Report)")
        lines.append("")
        lines.append("**ì„¤ì •**: OpenAI + RRF MultiQuery (Top 8)")
        lines.append("")

        # Judgeë³„ í‰ê·  ê³„ì‚°
        executive_avg = executive_df.groupby("llm").agg({
            "score": "mean",
            "vs_gpt41_percentage": "mean",
            "vs_gpt41_diff": "mean"
        }).round(2)

        executive_avg = executive_avg.sort_values("score", ascending=False)

        lines.append("### í‰ê·  ì„±ëŠ¥ (3ê°œ Judge í‰ê· )")
        lines.append("")
        lines.append("| ìˆœìœ„ | LLM | í‰ê·  ì ìˆ˜ | GPT-4.1 ëŒ€ë¹„ | ì ìˆ˜ ì°¨ì´ |")
        lines.append("|------|-----|-----------|--------------|-----------|")

        for rank, (llm, row) in enumerate(executive_avg.iterrows(), 1):
            percentage_str = f"{row['vs_gpt41_percentage']:.1f}%"
            diff_str = f"{row['vs_gpt41_diff']:+.2f}"

            # íŠ¹ë³„ í‘œì‹œ
            if llm == "gpt41":
                llm_display = f"**{llm}** (ê¸°ì¤€)"
            else:
                llm_display = llm

            lines.append(f"| {rank} | {llm_display} | {row['score']:.2f} | {percentage_str} | {diff_str} |")

        lines.append("")

        # Judgeë³„ ìƒì„¸ ê²°ê³¼
        lines.append("### Judgeë³„ ìƒì„¸ ì ìˆ˜")
        lines.append("")

        executive_pivot = executive_df.pivot_table(
            index="llm",
            columns="judge",
            values="score",
            aggfunc="mean"
        ).round(2)

        lines.append(executive_pivot.to_markdown())
        lines.append("")

    # ì£¼ìš” ë°œê²¬ì‚¬í•­
    lines.append("## 3. ì£¼ìš” ë°œê²¬ì‚¬í•­ (Key Findings)")
    lines.append("")

    if weekly_df is not None and not weekly_df.empty:
        # DeepSeek-V3.1 vs GPT-4.1 ë¹„êµ
        deepseek_weekly = weekly_avg.loc["deepseek_v31"]
        gpt41_weekly = weekly_avg.loc["gpt41"]

        lines.append(f"### ì£¼ê°„ ë³´ê³ ì„œ: DeepSeek-V3.1 vs GPT-4.1")
        lines.append("")
        lines.append(f"- **DeepSeek-V3.1**: {deepseek_weekly['score']:.2f}ì ")
        lines.append(f"- **GPT-4.1**: {gpt41_weekly['score']:.2f}ì ")
        lines.append(f"- **ìƒëŒ€ ì„±ëŠ¥**: GPT-4.1 ëŒ€ë¹„ **{deepseek_weekly['vs_gpt41_percentage']:.1f}%**")
        lines.append(f"- **ì ìˆ˜ ì°¨ì´**: {deepseek_weekly['vs_gpt41_diff']:+.2f}ì ")
        lines.append("")

        if deepseek_weekly['score'] > gpt41_weekly['score']:
            lines.append(f"âœ… DeepSeek-V3.1ì´ GPT-4.1ë³´ë‹¤ **{abs(deepseek_weekly['vs_gpt41_diff']):.2f}ì  ë†’ì€** ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.")
        elif deepseek_weekly['score'] < gpt41_weekly['score']:
            lines.append(f"âš ï¸ DeepSeek-V3.1ì´ GPT-4.1ë³´ë‹¤ {abs(deepseek_weekly['vs_gpt41_diff']):.2f}ì  ë‚®ì€ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.")
        else:
            lines.append("ğŸ“Š DeepSeek-V3.1ê³¼ GPT-4.1ì´ ë™ì¼í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.")

        lines.append("")

    # ëª¨ë¸ë³„ ê°•ì  ë¶„ì„
    lines.append("### ëª¨ë¸ë³„ íŠ¹ì§•")
    lines.append("")
    lines.append("| ëª¨ë¸ | íƒ€ì… | íŠ¹ì§• | ê¶Œì¥ ì‚¬ìš© |")
    lines.append("|------|------|------|-----------|")
    lines.append("| GPT-4.1 | í”„ë¡ í‹°ì–´ | ì•ˆì •ì  ì„±ëŠ¥, ë² ì´ìŠ¤ë¼ì¸ | ì¤‘ìš” ì˜ì‚¬ê²°ì • ë³´ê³ ì„œ |")
    lines.append("| GPT-5.1 | í”„ë¡ í‹°ì–´ | ìµœì‹  ëª¨ë¸, ê³ ì„±ëŠ¥ | ë³µì¡í•œ ë¶„ì„ í•„ìš” ì‹œ |")
    lines.append("| DeepSeek-V3.1 | ì˜¤í”ˆì†ŒìŠ¤ | ë¹„ìš© íš¨ìœ¨ì , GPT ìˆ˜ì¤€ ì„±ëŠ¥ | ëŒ€ëŸ‰ ë³´ê³ ì„œ ìƒì„± |")
    lines.append("| Claude Opus 4.5 | í”„ë¡ í‹°ì–´ | ì¥ë¬¸ ì²˜ë¦¬ ìš°ìˆ˜ | ìƒì„¸í•œ ë¶„ì„ ë³´ê³ ì„œ |")
    lines.append("| Claude Sonnet 4.5 | í”„ë¡ í‹°ì–´ | ë¹ ë¥¸ ì†ë„, ê· í˜•ì¡íŒ ì„±ëŠ¥ | ì¼ë°˜ ì—…ë¬´ ë³´ê³ ì„œ |")
    lines.append("| Llama-3.3-70B | ì˜¤í”ˆì†ŒìŠ¤ | ë¡œì»¬ ì‹¤í–‰ ê°€ëŠ¥ | ë¯¼ê° ë°ì´í„° ì²˜ë¦¬ |")
    lines.append("| Phi-4 | ê²½ëŸ‰ | ë‚®ì€ ë¦¬ì†ŒìŠ¤, ë¹ ë¥¸ ì‘ë‹µ | ê°„ë‹¨í•œ ìš”ì•½ |")
    lines.append("")

    # ê²°ë¡ 
    lines.append("## 4. ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­")
    lines.append("")
    lines.append("### ë¹„ìš© ëŒ€ë¹„ ì„±ëŠ¥ ê´€ì ")
    lines.append("")
    lines.append("ë³¸ í”„ë¡œì íŠ¸ëŠ” ë‹¤ì–‘í•œ LLM ëª¨ë¸ì„ ì‹¤í—˜í•˜ì—¬ ê° ìƒí™©ì— ë§ëŠ” ìµœì ì˜ ëª¨ë¸ì„ ì„ íƒí•  ìˆ˜ ìˆëŠ” ì—­ëŸ‰ì„ í™•ë³´í–ˆìŠµë‹ˆë‹¤.")
    lines.append("")
    lines.append("**í•µì‹¬ ê°•ì :**")
    lines.append("")
    lines.append("1. **ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ í™œìš©**: DeepSeek-V3.1ê³¼ ê°™ì€ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ì´ GPT-4.1ê³¼ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë³´ì—¬, ë¹„ìš© ì ˆê° ê°€ëŠ¥")
    lines.append("2. **ë‹¤ì–‘í•œ ëª¨ë¸ ì‹¤í—˜**: 7ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ LLM ëª¨ë¸ì„ ì²´ê³„ì ìœ¼ë¡œ í‰ê°€")
    lines.append("3. **ê°ê´€ì  í‰ê°€**: 3ê°œì˜ Judge ëª¨ë¸ì„ ì‚¬ìš©í•œ ë‹¤ê°ë„ ì„±ëŠ¥ í‰ê°€")
    lines.append("4. **ìµœì í™”ëœ RAG íŒŒì´í”„ë¼ì¸**: ë³´ê³ ì„œ íƒ€ì…ë³„ ìµœì ì˜ Retriever + LLM ì¡°í•© ë„ì¶œ")
    lines.append("")
    lines.append("### ê¶Œì¥ì‚¬í•­")
    lines.append("")
    lines.append("- **ì£¼ê°„ ë³´ê³ ì„œ**: BGE-M3 + Qwen3 Reranker + **DeepSeek-V3.1** (ë¹„ìš© íš¨ìœ¨ì )")
    lines.append("- **ì„ì› ë³´ê³ ì„œ**: OpenAI + RRF MultiQuery + **GPT-4.1** (ì•ˆì •ì  í’ˆì§ˆ)")
    lines.append("- **ëŒ€ëŸ‰ ì²˜ë¦¬**: DeepSeek-V3.1 ë˜ëŠ” Llama-3.3-70B (ë¡œì»¬ ì‹¤í–‰ ê°€ëŠ¥)")
    lines.append("- **ì‹¤í—˜/ì—°êµ¬**: ê³„ì†í•´ì„œ ìƒˆë¡œìš´ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ë° ë²¤ì¹˜ë§ˆí¬")
    lines.append("")

    # ë©”íƒ€ë°ì´í„°
    lines.append("---")
    lines.append("")
    lines.append("## ë©”íƒ€ë°ì´í„°")
    lines.append("")
    lines.append("- **í‰ê°€ ë°©ë²•**: LLM-as-Judge (Multi-Judge)")
    lines.append("- **Judge ëª¨ë¸**: GPT-5.1, Claude Opus 4.5, DeepSeek-V3.1")
    lines.append("- **í‰ê°€ ê¸°ì¤€**: Conciseness, Strategic Value, Accuracy, Clarity, Actionability")
    lines.append("- **í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ìˆ˜**: 5ê°œ (ê° ë³´ê³ ì„œ íƒ€ì…)")
    lines.append("- **í‰ê°€ LLM ìˆ˜**: 7ê°œ")
    lines.append("")

    # íŒŒì¼ ì €ì¥
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(lines))

    print(f"âœ… ë¦¬í¬íŠ¸ ì €ì¥: {output_file}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""

    # í…ŒìŠ¤íŠ¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ
    weekly_dir = Path("/home/work/rag/Project/rag-report-generator/data/results/multi_llm_test/weekly/20260102_052136")
    executive_dir = Path("/home/work/rag/Project/rag-report-generator/data/results/multi_llm_test/executive/20260102_053212")

    print("\n" + "="*100)
    print("ğŸš€ LLM ì„±ëŠ¥ ë¹„êµ ë¶„ì„ ì‹œì‘")
    print("="*100)

    # ì£¼ê°„ ë³´ê³ ì„œ ë¶„ì„
    print("\nğŸ“Š ì£¼ê°„ ë³´ê³ ì„œ ë¶„ì„ ì¤‘...")
    weekly_df = analyze_multi_judge_results(weekly_dir, "weekly")

    if weekly_df is not None:
        print(f"âœ… ì£¼ê°„ ë³´ê³ ì„œ: {len(weekly_df)} ê±´ì˜ í‰ê°€ ê²°ê³¼ ë¡œë“œ")
        print("\nì£¼ê°„ ë³´ê³ ì„œ í‰ê·  ì ìˆ˜:")
        weekly_avg = weekly_df.groupby("llm")["score"].mean().sort_values(ascending=False)
        for llm, score in weekly_avg.items():
            print(f"  {llm}: {score:.2f}")
    else:
        print("âš ï¸ ì£¼ê°„ ë³´ê³ ì„œ ë¶„ì„ ì‹¤íŒ¨")

    # ì„ì› ë³´ê³ ì„œ ë¶„ì„
    print("\nğŸ“Š ì„ì› ë³´ê³ ì„œ ë¶„ì„ ì¤‘...")
    executive_df = analyze_multi_judge_results(executive_dir, "executive")

    if executive_df is not None:
        print(f"âœ… ì„ì› ë³´ê³ ì„œ: {len(executive_df)} ê±´ì˜ í‰ê°€ ê²°ê³¼ ë¡œë“œ")
        print("\nì„ì› ë³´ê³ ì„œ í‰ê·  ì ìˆ˜:")
        executive_avg = executive_df.groupby("llm")["score"].mean().sort_values(ascending=False)
        for llm, score in executive_avg.items():
            print(f"  {llm}: {score:.2f}")
    else:
        print("âš ï¸ ì„ì› ë³´ê³ ì„œ ë¶„ì„ ì‹¤íŒ¨")

    # ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„±
    print("\nğŸ“ ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")

    output_dir = Path("/home/work/rag/Project/rag-report-generator/data/results")
    output_dir.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = output_dir / f"llm_performance_comparison_{timestamp}.md"

    generate_markdown_report(weekly_df, executive_df, output_file)

    # CSV ì €ì¥
    if weekly_df is not None:
        weekly_csv = output_dir / f"weekly_scores_{timestamp}.csv"
        weekly_df.to_csv(weekly_csv, index=False, encoding='utf-8-sig')
        print(f"âœ… ì£¼ê°„ ë³´ê³ ì„œ CSV ì €ì¥: {weekly_csv}")

    if executive_df is not None:
        executive_csv = output_dir / f"executive_scores_{timestamp}.csv"
        executive_df.to_csv(executive_csv, index=False, encoding='utf-8-sig')
        print(f"âœ… ì„ì› ë³´ê³ ì„œ CSV ì €ì¥: {executive_csv}")

    print("\n" + "="*100)
    print("âœ… ë¶„ì„ ì™„ë£Œ!")
    print("="*100)


if __name__ == "__main__":
    main()
