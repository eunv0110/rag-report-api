#!/usr/bin/env python3
"""ìœ¡ì•ˆ í‰ê°€ì™€ Judge í‰ê°€ ë¹„êµ ë¶„ì„

ì§ˆë¬¸ë³„ë¡œ í©ì–´ì§„ Judge í‰ê°€ ê²°ê³¼ë¥¼ í†µí•©í•˜ê³ , ìœ¡ì•ˆ í‰ê°€ì™€ ë¹„êµí•©ë‹ˆë‹¤.
"""

import json
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, List, Any
from scipy.stats import spearmanr, kendalltau
from datetime import datetime


def aggregate_judge_evaluations(judge_eval_dir: Path) -> Dict[str, Any]:
    """Judge í‰ê°€ ê²°ê³¼ í†µí•©

    ì§ˆë¬¸ë³„ë¡œ í©ì–´ì§„ í‰ê°€ ê²°ê³¼ë¥¼ LLMë³„ë¡œ í†µí•©í•˜ì—¬ í‰ê·  ì ìˆ˜ ê³„ì‚°
    """

    # ëª¨ë“  Judge ëª¨ë¸ ë””ë ‰í† ë¦¬
    judge_dirs = [d for d in judge_eval_dir.iterdir() if d.is_dir() and d.name.startswith('evaluation_')]

    if not judge_dirs:
        return None

    print(f"\n{'='*80}")
    print(f"Judge í‰ê°€ ê²°ê³¼ í†µí•©")
    print(f"{'='*80}")
    print(f"ë°œê²¬ëœ Judge ëª¨ë¸: {len(judge_dirs)}ê°œ")

    # ê° Judge ëª¨ë¸ë³„ë¡œ ê²°ê³¼ ìˆ˜ì§‘
    all_judge_results = {}

    for judge_dir in judge_dirs:
        judge_name = judge_dir.name.replace('evaluation_', '')
        print(f"\nğŸ“Š Judge: {judge_name}")

        # ì§ˆë¬¸ë³„ í‰ê°€ íŒŒì¼ë“¤
        question_files = sorted(judge_dir.glob('question_*.json'))

        if not question_files:
            continue

        # LLMë³„ ì ìˆ˜ ìˆ˜ì§‘ (ì§ˆë¬¸ë³„ë¡œ)
        llm_scores_by_question = {}

        for qfile in question_files:
            with open(qfile, 'r', encoding='utf-8') as f:
                data = json.load(f)

            for llm_name, result in data.get('results', {}).items():
                if llm_name not in llm_scores_by_question:
                    llm_scores_by_question[llm_name] = []

                llm_scores_by_question[llm_name].append({
                    'question': data.get('question', ''),
                    'final_score': result.get('final_score', 0),
                    'criterion_results': result.get('criterion_results', [])
                })

        # LLMë³„ í‰ê·  ì ìˆ˜ ê³„ì‚°
        llm_avg_scores = {}
        for llm_name, scores_list in llm_scores_by_question.items():
            avg_score = np.mean([s['final_score'] for s in scores_list])
            llm_avg_scores[llm_name] = {
                'final_score': avg_score,
                'num_questions': len(scores_list),
                'scores_by_question': scores_list
            }
            print(f"  {llm_name}: {avg_score:.2f} (ì§ˆë¬¸ {len(scores_list)}ê°œ)")

        all_judge_results[judge_name] = llm_avg_scores

    return all_judge_results


def map_llm_names(name: str) -> str:
    """LLM ì´ë¦„ í‘œì¤€í™”"""
    name_mapping = {
        'phi4': 'Phi-4',
        'deepseek_v31': 'DeepSeek-V3.1',
        'gpt41': 'OpenAI GPT-4.1',
        'gpt51': 'OpenAI GPT-5.1',
        'llama33_70b': 'Llama-3.3-70B-Instruct',
        'claude_opus_45': 'Claude 4.5 Opus',
        'claude_sonnet_45': 'Claude 4.5 Sonnet'
    }
    return name_mapping.get(name, name)


def compare_human_vs_judges(
    human_eval_path: Path,
    judge_results: Dict[str, Any],
    report_type: str
) -> Dict[str, Any]:
    """ìœ¡ì•ˆ í‰ê°€ì™€ ê° Judge í‰ê°€ ë¹„êµ"""

    # ìœ¡ì•ˆ í‰ê°€ ë¡œë“œ
    with open(human_eval_path, 'r', encoding='utf-8') as f:
        human_data = json.load(f)

    human_results = human_data['results']

    print(f"\n{'='*80}")
    print(f"{report_type.upper()} - ìœ¡ì•ˆ í‰ê°€ vs Judge í‰ê°€ ë¹„êµ")
    print(f"{'='*80}")

    comparisons = {}

    for judge_name, judge_scores in judge_results.items():
        print(f"\nğŸ¤– Judge: {judge_name}")

        # ê³µí†µ LLM ì°¾ê¸°
        comparison_data = []

        for llm_code, judge_data in judge_scores.items():
            llm_name = map_llm_names(llm_code)

            if llm_name not in human_results:
                print(f"  âš ï¸ {llm_name}: ìœ¡ì•ˆ í‰ê°€ ì—†ìŒ")
                continue

            human_score = human_results[llm_name]['final_score']
            judge_score = judge_data['final_score']

            comparison_data.append({
                'llm': llm_name,
                'human_score': human_score,
                'judge_score': judge_score,
                'score_diff': human_score - judge_score,
                'num_questions': judge_data['num_questions']
            })

        if not comparison_data:
            print("  âš ï¸ ë¹„êµ ê°€ëŠ¥í•œ LLM ì—†ìŒ")
            continue

        # DataFrame ìƒì„±
        df = pd.DataFrame(comparison_data)

        # ìˆœìœ„ ê³„ì‚°
        df['human_rank'] = df['human_score'].rank(ascending=False, method='min').astype(int)
        df['judge_rank'] = df['judge_score'].rank(ascending=False, method='min').astype(int)
        df['rank_diff'] = abs(df['human_rank'] - df['judge_rank'])

        df = df.sort_values('human_rank')

        # ìƒê´€ê´€ê³„
        if len(df) > 2:
            spearman_corr, spearman_p = spearmanr(df['human_score'], df['judge_score'])
            kendall_corr, kendall_p = kendalltau(df['human_score'], df['judge_score'])
            pearson_corr = np.corrcoef(df['human_score'], df['judge_score'])[0, 1]

            print(f"\n  ğŸ“Š ìƒê´€ê´€ê³„:")
            print(f"    Pearson: {pearson_corr:.4f}")
            print(f"    Spearman: {spearman_corr:.4f} (p={spearman_p:.4f})")
            print(f"    Kendall Tau: {kendall_corr:.4f} (p={kendall_p:.4f})")

            # í•´ì„
            if spearman_corr > 0.8:
                interpretation = "âœ… ë§¤ìš° ê°•í•œ ìƒê´€ê´€ê³„ - ìœ¡ì•ˆ í‰ê°€ì™€ ë§¤ìš° ìœ ì‚¬"
            elif spearman_corr > 0.6:
                interpretation = "âœ“ ê°•í•œ ìƒê´€ê´€ê³„ - ëŒ€ì²´ë¡œ ì¼ì¹˜"
            elif spearman_corr > 0.4:
                interpretation = "âš ï¸ ì¤‘ê°„ ì •ë„ - ì¼ë¶€ ì°¨ì´"
            else:
                interpretation = "âŒ ì•½í•œ ìƒê´€ê´€ê³„ - í° ì°¨ì´"

            print(f"    â†’ {interpretation}")

        # ìˆœìœ„ ì°¨ì´
        print(f"\n  ğŸ“ˆ ìˆœìœ„ ì°¨ì´:")
        print(f"    í‰ê· : {df['rank_diff'].mean():.2f}")
        print(f"    ìµœëŒ€: {df['rank_diff'].max()}")

        # ë¹„êµí‘œ
        print(f"\n  ğŸ“‹ ìƒì„¸ ë¹„êµ:")
        for _, row in df.iterrows():
            print(f"    {row['llm']:<25} | ìœ¡ì•ˆ: {row['human_rank']}ìœ„({row['human_score']:.2f}) | Judge: {row['judge_rank']}ìœ„({row['judge_score']:.2f}) | ì°¨ì´: {row['rank_diff']}")

        # í° ì°¨ì´
        big_diff = df[df['rank_diff'] >= 2]
        if len(big_diff) > 0:
            print(f"\n  âš ï¸ í° ìˆœìœ„ ì°¨ì´ (>= 2):")
            for _, row in big_diff.iterrows():
                print(f"    {row['llm']}: ìœ¡ì•ˆ {row['human_rank']}ìœ„ â†’ Judge {row['judge_rank']}ìœ„ (ì°¨ì´: {row['rank_diff']})")

        comparisons[judge_name] = {
            'df': df,
            'correlations': {
                'pearson': pearson_corr if len(df) > 2 else None,
                'spearman': spearman_corr if len(df) > 2 else None,
                'kendall': kendall_corr if len(df) > 2 else None
            } if len(df) > 2 else None,
            'rank_diff_stats': {
                'mean': df['rank_diff'].mean(),
                'max': df['rank_diff'].max(),
                'std': df['rank_diff'].std()
            }
        }

    return comparisons


def generate_comparison_report(
    comparisons: Dict[str, Any],
    report_type: str,
    output_dir: Path
):
    """ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±"""

    report_path = output_dir / f"human_vs_judge_comparison_{report_type}.md"

    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(f"# ìœ¡ì•ˆ í‰ê°€ vs LLM Judge í‰ê°€ ë¹„êµ\n\n")
        f.write(f"**ë³´ê³ ì„œ íƒ€ì…**: {report_type}\n\n")
        f.write(f"**ë¶„ì„ ì¼ì‹œ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write("---\n\n")

        for judge_name, comp_data in comparisons.items():
            f.write(f"## Judge: {judge_name}\n\n")

            df = comp_data['df']

            # ìƒê´€ê´€ê³„
            if comp_data['correlations']:
                corr = comp_data['correlations']
                f.write(f"### ìƒê´€ê´€ê³„\n\n")
                f.write(f"- **Pearson**: {corr['pearson']:.4f}\n")
                f.write(f"- **Spearman**: {corr['spearman']:.4f}\n")
                f.write(f"- **Kendall Tau**: {corr['kendall']:.4f}\n\n")

                # í•´ì„
                spearman = corr['spearman']
                if spearman > 0.8:
                    f.write("âœ… **ë§¤ìš° ê°•í•œ ìƒê´€ê´€ê³„** - ìœ¡ì•ˆ í‰ê°€ì™€ ë§¤ìš° ìœ ì‚¬í•©ë‹ˆë‹¤.\n\n")
                elif spearman > 0.6:
                    f.write("âœ“ **ê°•í•œ ìƒê´€ê´€ê³„** - ëŒ€ì²´ë¡œ ì¼ì¹˜í•©ë‹ˆë‹¤.\n\n")
                elif spearman > 0.4:
                    f.write("âš ï¸ **ì¤‘ê°„ ì •ë„ì˜ ìƒê´€ê´€ê³„** - ì¼ë¶€ ì°¨ì´ê°€ ìˆìŠµë‹ˆë‹¤.\n\n")
                else:
                    f.write("âŒ **ì•½í•œ ìƒê´€ê´€ê³„** - í° ì°¨ì´ê°€ ìˆìŠµë‹ˆë‹¤.\n\n")

            # ìˆœìœ„ ì°¨ì´
            rank_diff = comp_data['rank_diff_stats']
            f.write(f"### ìˆœìœ„ ì°¨ì´\n\n")
            f.write(f"- **í‰ê· **: {rank_diff['mean']:.2f}\n")
            f.write(f"- **ìµœëŒ€**: {rank_diff['max']}\n")
            f.write(f"- **í‘œì¤€í¸ì°¨**: {rank_diff['std']:.2f}\n\n")

            # ë¹„êµí‘œ
            f.write(f"### ìƒì„¸ ë¹„êµ\n\n")
            f.write("| LLM | ìœ¡ì•ˆ ì ìˆ˜ | ìœ¡ì•ˆ ìˆœìœ„ | Judge ì ìˆ˜ | Judge ìˆœìœ„ | ìˆœìœ„ ì°¨ì´ |\n")
            f.write("|-----|-----------|-----------|------------|-----------|----------|\n")
            for _, row in df.iterrows():
                f.write(f"| {row['llm']} | {row['human_score']:.2f} | {row['human_rank']} | ")
                f.write(f"{row['judge_score']:.2f} | {row['judge_rank']} | {row['rank_diff']} |\n")
            f.write("\n")

            # í° ì°¨ì´
            big_diff = df[df['rank_diff'] >= 2]
            if len(big_diff) > 0:
                f.write(f"### âš ï¸ í° ìˆœìœ„ ì°¨ì´ (>= 2)\n\n")
                for _, row in big_diff.iterrows():
                    f.write(f"- **{row['llm']}**\n")
                    f.write(f"  - ìœ¡ì•ˆ í‰ê°€: {row['human_rank']}ìœ„ ({row['human_score']:.2f}ì )\n")
                    f.write(f"  - Judge í‰ê°€: {row['judge_rank']}ìœ„ ({row['judge_score']:.2f}ì )\n")
                    f.write(f"  - ìˆœìœ„ ì°¨ì´: {row['rank_diff']}\n\n")

            f.write("---\n\n")

    print(f"\nğŸ“ ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±: {report_path}")

    # CSVë„ ì €ì¥
    for judge_name, comp_data in comparisons.items():
        csv_path = output_dir / f"{report_type}_{judge_name.replace('/', '_')}_comparison.csv"
        comp_data['df'].to_csv(csv_path, index=False, encoding='utf-8-sig')
        print(f"ğŸ’¾ CSV ì €ì¥: {csv_path}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""

    base_dir = Path("data/results/multi_llm_test")

    # Weekly ë³´ê³ ì„œ
    print("\n" + "="*80)
    print("ì£¼ê°„ ë³´ê³ ì„œ (WEEKLY) ë¶„ì„")
    print("="*80)

    weekly_judge_dir = base_dir / "weekly/20260102_052136/multi_judge_evaluation"
    weekly_human = base_dir / "human_evaluation/weekly_human_evaluation_judge_format.json"

    if weekly_judge_dir.exists():
        weekly_judge_results = aggregate_judge_evaluations(weekly_judge_dir)

        if weekly_judge_results:
            weekly_comparisons = compare_human_vs_judges(
                weekly_human,
                weekly_judge_results,
                "weekly"
            )

            # ë¦¬í¬íŠ¸ ìƒì„±
            output_dir = base_dir / "evaluation_comparison"
            output_dir.mkdir(parents=True, exist_ok=True)

            generate_comparison_report(weekly_comparisons, "weekly", output_dir)

    # Executive ë³´ê³ ì„œ
    print("\n" + "="*80)
    print("ìµœì¢… ë³´ê³ ì„œ (EXECUTIVE) ë¶„ì„")
    print("="*80)

    executive_judge_dir = base_dir / "executive/20260102_053212/multi_judge_evaluation"
    executive_human = base_dir / "human_evaluation/executive_human_evaluation_judge_format.json"

    if executive_judge_dir.exists():
        executive_judge_results = aggregate_judge_evaluations(executive_judge_dir)

        if executive_judge_results:
            executive_comparisons = compare_human_vs_judges(
                executive_human,
                executive_judge_results,
                "executive"
            )

            # ë¦¬í¬íŠ¸ ìƒì„±
            output_dir = base_dir / "evaluation_comparison"
            output_dir.mkdir(parents=True, exist_ok=True)

            generate_comparison_report(executive_comparisons, "executive", output_dir)

    print("\nâœ… ëª¨ë“  ë¹„êµ ë¶„ì„ ì™„ë£Œ!")


if __name__ == "__main__":
    main()
