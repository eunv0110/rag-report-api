#!/usr/bin/env python3
"""ì—¬ëŸ¬ Judge ëª¨ë¸ë¡œ ë³‘ë ¬ í‰ê°€ ìˆ˜í–‰

ì—¬ëŸ¬ Judge LLMì„ ì‚¬ìš©í•˜ì—¬ ë™ì‹œì— í‰ê°€ë¥¼ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.
"""

import sys
from pathlib import Path

# Python ê²½ë¡œ ì„¤ì •ì„ ê°€ìž¥ ë¨¼ì € ìˆ˜í–‰
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(Path(__file__).parent.parent))

import json
import argparse
from typing import Dict, Any, List
from datetime import datetime
from concurrent.futures import ProcessPoolExecutor, as_completed
import pandas as pd

from evaluate_combinations import evaluate_all_questions


def evaluate_with_single_judge(
    combinations_file: str,
    judge_model: str,
    provider: str,
    report_type: str,
    output_base_dir: str
) -> Dict[str, Any]:
    """ë‹¨ì¼ Judge ëª¨ë¸ë¡œ í‰ê°€ ìˆ˜í–‰ (ë³‘ë ¬ ì‹¤í–‰ìš©)

    Args:
        combinations_file: all_combinations.json íŒŒì¼ ê²½ë¡œ
        judge_model: Judge ëª¨ë¸ ì´ë¦„
        provider: LLM ì œê³µìž
        report_type: ë³´ê³ ì„œ íƒ€ìž…
        output_base_dir: ê²°ê³¼ ì €ìž¥ ê¸°ë³¸ ë””ë ‰í† ë¦¬

    Returns:
        í‰ê°€ ê²°ê³¼
    """
    # ê° í”„ë¡œì„¸ìŠ¤ì—ì„œ Python ê²½ë¡œ ìž¬ì„¤ì •
    import sys
    from pathlib import Path
    project_root = Path(__file__).parent.parent.parent
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
    if str(Path(__file__).parent.parent) not in sys.path:
        sys.path.insert(0, str(Path(__file__).parent.parent))

    # Judge ëª¨ë¸ ì´ë¦„ì„ íŒŒì¼ëª…ì— ì•ˆì „í•˜ê²Œ ì‚¬ìš©
    safe_model_name = judge_model.replace('/', '_').replace(':', '_')
    output_dir = Path(output_base_dir) / f"evaluation_{safe_model_name}"

    print(f"\n{'='*100}")
    print(f"ðŸš€ Judge ëª¨ë¸ '{judge_model}' í‰ê°€ ì‹œìž‘")
    print(f"{'='*100}")

    try:
        result = evaluate_all_questions(
            combinations_file=combinations_file,
            judge_model=judge_model,
            provider=provider,
            report_type=report_type,
            output_dir=str(output_dir)
        )

        print(f"\nâœ… Judge ëª¨ë¸ '{judge_model}' í‰ê°€ ì™„ë£Œ!")
        return {
            "judge_model": judge_model,
            "provider": provider,
            "success": True,
            "result": result,
            "output_dir": str(output_dir)
        }

    except Exception as e:
        print(f"\nâŒ Judge ëª¨ë¸ '{judge_model}' í‰ê°€ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return {
            "judge_model": judge_model,
            "provider": provider,
            "success": False,
            "error": str(e),
            "output_dir": str(output_dir)
        }


def evaluate_with_multiple_judges(
    combinations_file: str,
    judge_configs: List[Dict[str, str]],
    report_type: str,
    output_base_dir: str,
    max_workers: int = 3
) -> Dict[str, Any]:
    """ì—¬ëŸ¬ Judge ëª¨ë¸ë¡œ ë³‘ë ¬ í‰ê°€ ìˆ˜í–‰

    Args:
        combinations_file: all_combinations.json íŒŒì¼ ê²½ë¡œ
        judge_configs: Judge ì„¤ì • ë¦¬ìŠ¤íŠ¸ [{"model": "gpt-5.1", "provider": "azure_ai"}, ...]
        report_type: ë³´ê³ ì„œ íƒ€ìž…
        output_base_dir: ê²°ê³¼ ì €ìž¥ ê¸°ë³¸ ë””ë ‰í† ë¦¬
        max_workers: ìµœëŒ€ ë™ì‹œ ì‹¤í–‰ í”„ë¡œì„¸ìŠ¤ ìˆ˜ (ê¸°ë³¸: 3)

    Returns:
        ì „ì²´ í‰ê°€ ê²°ê³¼
    """
    print(f"\n{'='*100}")
    print(f"ðŸ”¥ ë‹¤ì¤‘ Judge ëª¨ë¸ ë³‘ë ¬ í‰ê°€ ì‹œìž‘")
    print(f"{'='*100}")
    print(f"ðŸ“Š í‰ê°€í•  Judge ëª¨ë¸ ìˆ˜: {len(judge_configs)}")
    for config in judge_configs:
        print(f"  - {config['model']} ({config['provider']})")
    print(f"âš¡ ìµœëŒ€ ë™ì‹œ ì‹¤í–‰ ìˆ˜: {max_workers}")

    output_base_path = Path(output_base_dir)
    output_base_path.mkdir(parents=True, exist_ok=True)

    all_results = []

    # ë³‘ë ¬ ì‹¤í–‰
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        # ëª¨ë“  Judge ëª¨ë¸ì— ëŒ€í•´ í‰ê°€ ìž‘ì—… ì œì¶œ
        future_to_config = {}
        for config in judge_configs:
            future = executor.submit(
                evaluate_with_single_judge,
                combinations_file=combinations_file,
                judge_model=config['model'],
                provider=config['provider'],
                report_type=report_type,
                output_base_dir=output_base_dir
            )
            future_to_config[future] = config

        # ì™„ë£Œëœ ìž‘ì—… ê²°ê³¼ ìˆ˜ì§‘
        for future in as_completed(future_to_config):
            config = future_to_config[future]
            try:
                result = future.result()
                all_results.append(result)
            except Exception as e:
                print(f"\nâŒ {config['model']} í‰ê°€ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
                all_results.append({
                    "judge_model": config['model'],
                    "provider": config['provider'],
                    "success": False,
                    "error": str(e)
                })

    # ê²°ê³¼ ìš”ì•½
    print(f"\n{'='*100}")
    print(f"ðŸ“Š ì „ì²´ í‰ê°€ ê²°ê³¼ ìš”ì•½")
    print(f"{'='*100}")

    successful_results = [r for r in all_results if r['success']]
    failed_results = [r for r in all_results if not r['success']]

    print(f"âœ… ì„±ê³µ: {len(successful_results)}/{len(all_results)}")
    print(f"âŒ ì‹¤íŒ¨: {len(failed_results)}/{len(all_results)}")

    if failed_results:
        print("\nì‹¤íŒ¨í•œ Judge ëª¨ë¸:")
        for r in failed_results:
            print(f"  - {r['judge_model']}: {r.get('error', 'Unknown error')}")

    # ì „ì²´ ê²°ê³¼ ì €ìž¥
    summary_result = {
        "combinations_file": combinations_file,
        "report_type": report_type,
        "num_judges": len(judge_configs),
        "num_successful": len(successful_results),
        "num_failed": len(failed_results),
        "judge_configs": judge_configs,
        "results": all_results,
        "timestamp": datetime.now().isoformat()
    }

    summary_file = output_base_path / "multi_judge_evaluation_summary.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary_result, f, indent=2, ensure_ascii=False)

    print(f"\nðŸ’¾ ì „ì²´ ê²°ê³¼ ì €ìž¥: {summary_file}")

    # Judge ê°„ ë¹„êµ ë¶„ì„
    if len(successful_results) > 1:
        compare_judges(successful_results, output_base_path, report_type)

    return summary_result


def compare_judges(
    judge_results: List[Dict[str, Any]],
    output_dir: Path,
    report_type: str
):
    """ì—¬ëŸ¬ Judge ëª¨ë¸ì˜ í‰ê°€ ê²°ê³¼ ë¹„êµ

    Args:
        judge_results: ì„±ê³µí•œ Judge í‰ê°€ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        output_dir: ê²°ê³¼ ì €ìž¥ ë””ë ‰í† ë¦¬
        report_type: ë³´ê³ ì„œ íƒ€ìž…
    """
    print(f"\n{'='*100}")
    print(f"ðŸ” Judge ëª¨ë¸ ê°„ ë¹„êµ ë¶„ì„")
    print(f"{'='*100}")

    # LLMë³„ë¡œ ê° Judgeê°€ ì¤€ ì ìˆ˜ ìˆ˜ì§‘
    llm_scores_by_judge = {}

    for judge_result in judge_results:
        judge_model = judge_result['judge_model']
        evaluations = judge_result['result']['evaluations']

        for evaluation in evaluations:
            question_id = evaluation['question_id']

            for llm_name, llm_result in evaluation['results'].items():
                key = (llm_name, question_id)

                if key not in llm_scores_by_judge:
                    llm_scores_by_judge[key] = {}

                llm_scores_by_judge[key][judge_model] = llm_result['final_score']

    # ë°ì´í„°í”„ë ˆìž„ ìƒì„±
    rows = []
    for (llm_name, question_id), judge_scores in llm_scores_by_judge.items():
        row = {
            'llm_name': llm_name,
            'question_id': question_id,
        }

        # ê° Judgeì˜ ì ìˆ˜ ì¶”ê°€
        for judge_model, score in judge_scores.items():
            safe_judge_name = judge_model.replace('/', '_').replace(':', '_')
            row[safe_judge_name] = score

        # í‰ê· , í‘œì¤€íŽ¸ì°¨, ë²”ìœ„ ê³„ì‚°
        scores = list(judge_scores.values())
        row['avg_score'] = sum(scores) / len(scores)
        row['std_score'] = pd.Series(scores).std()
        row['min_score'] = min(scores)
        row['max_score'] = max(scores)
        row['score_range'] = max(scores) - min(scores)

        rows.append(row)

    df = pd.DataFrame(rows)

    # LLMë³„ í‰ê·  ê³„ì‚°
    llm_avg = df.groupby('llm_name').agg({
        'avg_score': 'mean',
        'std_score': 'mean',
        'score_range': 'mean'
    }).round(3)

    llm_avg = llm_avg.sort_values('avg_score', ascending=False)

    # CSV ì €ìž¥
    detail_csv = output_dir / "judge_comparison_detailed.csv"
    df.to_csv(detail_csv, index=False, encoding='utf-8-sig')
    print(f"ðŸ“Š ìƒì„¸ ë¹„êµ ì €ìž¥: {detail_csv}")

    summary_csv = output_dir / "judge_comparison_summary.csv"
    llm_avg.to_csv(summary_csv, encoding='utf-8-sig')
    print(f"ðŸ“Š ìš”ì•½ ë¹„êµ ì €ìž¥: {summary_csv}")

    # ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸
    md_file = output_dir / "judge_comparison.md"
    with open(md_file, 'w', encoding='utf-8') as f:
        f.write("# Judge ëª¨ë¸ ê°„ ë¹„êµ ë¶„ì„\n\n")
        f.write(f"**ë³´ê³ ì„œ íƒ€ìž…**: {report_type}\n\n")
        f.write(f"**ë¹„êµí•œ Judge ëª¨ë¸ ìˆ˜**: {len(judge_results)}\n\n")
        f.write(f"**ë¶„ì„ ì¼ì‹œ**: {datetime.now().isoformat()}\n\n")
        f.write("---\n\n")

        f.write("## Judge ëª¨ë¸ ëª©ë¡\n\n")
        for jr in judge_results:
            f.write(f"- **{jr['judge_model']}** ({jr['provider']})\n")
        f.write("\n")

        f.write("## LLMë³„ í‰ê·  ì ìˆ˜ (ëª¨ë“  Judge í‰ê· )\n\n")
        f.write(llm_avg.to_markdown())
        f.write("\n\n")

        # Judge ê°„ ì¼ì¹˜ë„ ë¶„ì„
        f.write("## Judge ê°„ ì¼ì¹˜ë„ ë¶„ì„\n\n")
        f.write(f"- **í‰ê·  ì ìˆ˜ ì°¨ì´ ë²”ìœ„**: {df['score_range'].mean():.3f}\n")
        f.write(f"- **ìµœëŒ€ ì ìˆ˜ ì°¨ì´**: {df['score_range'].max():.3f}\n")
        f.write(f"- **ìµœì†Œ ì ìˆ˜ ì°¨ì´**: {df['score_range'].min():.3f}\n\n")

        if df['score_range'].mean() > 1.0:
            f.write("âš ï¸ **ê²½ê³ **: Judge ëª¨ë¸ ê°„ í‰ê°€ ì°¨ì´ê°€ í½ë‹ˆë‹¤ (í‰ê·  ë²”ìœ„ > 1.0)\n\n")
        else:
            f.write("âœ… **ì–‘í˜¸**: Judge ëª¨ë¸ ê°„ í‰ê°€ê°€ ë¹„êµì  ì¼ì¹˜í•©ë‹ˆë‹¤.\n\n")

    print(f"ðŸ“ ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ì €ìž¥: {md_file}")

    # ì½˜ì†” ì¶œë ¥
    print("\n" + "="*100)
    print("ðŸ† LLMë³„ í‰ê·  ì ìˆ˜ (ëª¨ë“  Judge í‰ê· )")
    print("="*100)
    print(llm_avg.to_string())
    print("="*100)


def main():
    parser = argparse.ArgumentParser(
        description="ì—¬ëŸ¬ Judge ëª¨ë¸ë¡œ ë³‘ë ¬ í‰ê°€ ìˆ˜í–‰"
    )

    parser.add_argument(
        "--combinations-file",
        type=str,
        required=True,
        help="all_combinations.json íŒŒì¼ ê²½ë¡œ"
    )

    parser.add_argument(
        "--report-type",
        type=str,
        default="weekly_report",
        choices=["weekly_report", "executive_report"],
        help="ë³´ê³ ì„œ íƒ€ìž… (ê¸°ë³¸: weekly_report)"
    )

    parser.add_argument(
        "--output-dir",
        type=str,
        required=True,
        help="ê²°ê³¼ ì €ìž¥ ê¸°ë³¸ ë””ë ‰í† ë¦¬"
    )

    parser.add_argument(
        "--max-workers",
        type=int,
        default=3,
        help="ìµœëŒ€ ë™ì‹œ ì‹¤í–‰ í”„ë¡œì„¸ìŠ¤ ìˆ˜ (ê¸°ë³¸: 3)"
    )

    parser.add_argument(
        "--judges",
        type=str,
        nargs='+',
        default=["gpt-5.1:azure_ai", "anthropic/claude-opus-4.5:openrouter", "DeepSeek-V3.1:azure_ai"],
        help="Judge ëª¨ë¸ ì„¤ì • (í˜•ì‹: model:provider, ì˜ˆ: gpt-5.1:azure_ai)"
    )

    args = parser.parse_args()

    # Judge ì„¤ì • íŒŒì‹±
    judge_configs = []
    for judge_spec in args.judges:
        parts = judge_spec.split(':')
        if len(parts) != 2:
            print(f"âš ï¸  ìž˜ëª»ëœ Judge ì„¤ì • í˜•ì‹: {judge_spec} (í˜•ì‹: model:provider)")
            continue

        model, provider = parts
        judge_configs.append({
            "model": model,
            "provider": provider
        })

    if not judge_configs:
        print("âŒ ìœ íš¨í•œ Judge ëª¨ë¸ ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(1)

    # ë³‘ë ¬ í‰ê°€ ì‹¤í–‰
    result = evaluate_with_multiple_judges(
        combinations_file=args.combinations_file,
        judge_configs=judge_configs,
        report_type=args.report_type,
        output_base_dir=args.output_dir,
        max_workers=args.max_workers
    )

    print("\nâœ… ëª¨ë“  Judge ëª¨ë¸ í‰ê°€ ì™„ë£Œ!")

    if result['num_failed'] > 0:
        sys.exit(1)


if __name__ == "__main__":
    main()
