#!/usr/bin/env python3
"""ìœ¡ì•ˆ í‰ê°€(Human Evaluation)ì™€ LLM as Judge í‰ê°€ ë¹„êµ ë¶„ì„ ë„êµ¬

ì‚¬ëŒì˜ ìœ¡ì•ˆ í‰ê°€ ê²°ê³¼ì™€ LLM Judgeì˜ ìë™ í‰ê°€ ê²°ê³¼ë¥¼ ë¹„êµí•˜ì—¬
í‰ê°€ ë°©ì‹ì˜ ì°¨ì´, ìƒê´€ê´€ê³„, í¸í–¥ ë“±ì„ ë¶„ì„í•©ë‹ˆë‹¤.
"""

import json
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, List, Any, Tuple
from scipy.stats import spearmanr, kendalltau
import matplotlib.pyplot as plt
import seaborn as sns


class HumanVsJudgeComparator:
    """ìœ¡ì•ˆ í‰ê°€ì™€ Judge í‰ê°€ ë¹„êµ ë¶„ì„ í´ë˜ìŠ¤"""

    def __init__(self):
        self.human_scores = {}
        self.judge_scores = {}

    def load_human_evaluation(
        self,
        report_type: str,
        scores: Dict[str, float]
    ):
        """ìœ¡ì•ˆ í‰ê°€ ê²°ê³¼ ë¡œë“œ

        Args:
            report_type: 'weekly' or 'executive'
            scores: {llm_name: score} ë”•ì…”ë„ˆë¦¬
        """
        self.human_scores[report_type] = scores
        print(f"âœ… {report_type} ìœ¡ì•ˆ í‰ê°€ ê²°ê³¼ ë¡œë“œ: {len(scores)}ê°œ LLM")

    def load_judge_evaluation(
        self,
        report_type: str,
        judge_results_path: str
    ):
        """Judge í‰ê°€ ê²°ê³¼ ë¡œë“œ

        Args:
            report_type: 'weekly' or 'executive'
            judge_results_path: Judge í‰ê°€ ê²°ê³¼ JSON íŒŒì¼ ê²½ë¡œ
        """
        with open(judge_results_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        scores = {}
        for llm_name, result in data.get('results', {}).items():
            scores[llm_name] = result.get('final_score', 0)

        self.judge_scores[report_type] = scores
        print(f"âœ… {report_type} Judge í‰ê°€ ê²°ê³¼ ë¡œë“œ: {len(scores)}ê°œ LLM")

    def normalize_llm_names(self, report_type: str):
        """LLM ì´ë¦„ ì •ê·œí™” (ë§¤ì¹­ì„ ìœ„í•´)

        ìœ¡ì•ˆ í‰ê°€ì™€ Judge í‰ê°€ì˜ LLM ì´ë¦„ì„ í†µì¼í•©ë‹ˆë‹¤.
        """
        # LLM ì´ë¦„ ë§¤í•‘ ê·œì¹™
        name_mapping = {
            # ìœ¡ì•ˆ í‰ê°€ ì´ë¦„ -> í‘œì¤€ ì´ë¦„
            "GPT-4.1": "OpenAI GPT-4.1",
            "GPT-5.1": "OpenAI GPT-5.1",
            "DeepSeek-V3.1": "DeepSeek-V3.1",
            "Claude Sonnet 4.5": "Claude 4.5 Sonnet",
            "Claude Opus 4.5": "Claude 4.5 Opus",
            "Phi-4": "Phi-4",
            "Llama-3.3-70B": "Llama-3.3-70B-Instruct"
        }

        # ìœ¡ì•ˆ í‰ê°€ ì´ë¦„ ë³€í™˜
        if report_type in self.human_scores:
            normalized = {}
            for name, score in self.human_scores[report_type].items():
                std_name = name_mapping.get(name, name)
                normalized[std_name] = score
            self.human_scores[report_type] = normalized

    def compare_rankings(
        self,
        report_type: str
    ) -> Dict[str, Any]:
        """ìœ¡ì•ˆ í‰ê°€ì™€ Judge í‰ê°€ì˜ ìˆœìœ„ ë¹„êµ

        Args:
            report_type: 'weekly' or 'executive'

        Returns:
            ë¹„êµ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if report_type not in self.human_scores or report_type not in self.judge_scores:
            raise ValueError(f"{report_type}ì— ëŒ€í•œ í‰ê°€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        human = self.human_scores[report_type]
        judge = self.judge_scores[report_type]

        # ê³µí†µ LLMë§Œ ë¹„êµ
        common_llms = set(human.keys()) & set(judge.keys())

        if not common_llms:
            raise ValueError("ìœ¡ì•ˆ í‰ê°€ì™€ Judge í‰ê°€ì— ê³µí†µ LLMì´ ì—†ìŠµë‹ˆë‹¤.")

        print(f"\n{'='*80}")
        print(f"{report_type.upper()} - ìœ¡ì•ˆ í‰ê°€ vs Judge í‰ê°€ ë¹„êµ")
        print(f"{'='*80}")
        print(f"ê³µí†µ LLM ìˆ˜: {len(common_llms)}")

        # ìˆœìœ„ ê³„ì‚°
        human_ranks = self._get_rankings(human)
        judge_ranks = self._get_rankings(judge)

        # ë¹„êµ ë°ì´í„° ìƒì„±
        comparison = []
        for llm in common_llms:
            comparison.append({
                'llm': llm,
                'human_score': human[llm],
                'human_rank': human_ranks[llm],
                'judge_score': judge[llm],
                'judge_rank': judge_ranks[llm],
                'rank_diff': abs(human_ranks[llm] - judge_ranks[llm]),
                'score_diff': human[llm] - judge[llm]
            })

        # DataFrame ìƒì„±
        df = pd.DataFrame(comparison)
        df = df.sort_values('human_rank')

        # í†µê³„ ê³„ì‚°
        human_scores_list = [human[llm] for llm in common_llms]
        judge_scores_list = [judge[llm] for llm in common_llms]

        spearman_corr, spearman_p = spearmanr(human_scores_list, judge_scores_list)
        kendall_corr, kendall_p = kendalltau(human_scores_list, judge_scores_list)

        # Pearson ìƒê´€ê³„ìˆ˜ (ì ìˆ˜ ê¸°ë°˜)
        pearson_corr = np.corrcoef(human_scores_list, judge_scores_list)[0, 1]

        print(f"\nğŸ“Š ìƒê´€ê´€ê³„ ë¶„ì„:")
        print(f"  Pearson ìƒê´€ê³„ìˆ˜: {pearson_corr:.4f} (ì ìˆ˜ ê¸°ë°˜)")
        print(f"  Spearman ìƒê´€ê³„ìˆ˜: {spearman_corr:.4f} (p={spearman_p:.4f}) (ìˆœìœ„ ê¸°ë°˜)")
        print(f"  Kendall Tau: {kendall_corr:.4f} (p={kendall_p:.4f}) (ìˆœìœ„ ì¼ì¹˜ë„)")

        # ìˆœìœ„ ì°¨ì´ ë¶„ì„
        avg_rank_diff = df['rank_diff'].mean()
        max_rank_diff = df['rank_diff'].max()

        print(f"\nğŸ“ˆ ìˆœìœ„ ì°¨ì´ ë¶„ì„:")
        print(f"  í‰ê·  ìˆœìœ„ ì°¨ì´: {avg_rank_diff:.2f}")
        print(f"  ìµœëŒ€ ìˆœìœ„ ì°¨ì´: {max_rank_diff}")

        # ê°€ì¥ í° ì°¨ì´ë¥¼ ë³´ì¸ LLM
        biggest_diff_llm = df.loc[df['rank_diff'].idxmax()]
        print(f"  ê°€ì¥ í° ì°¨ì´: {biggest_diff_llm['llm']}")
        print(f"    ìœ¡ì•ˆ: {biggest_diff_llm['human_rank']}ìœ„ ({biggest_diff_llm['human_score']}ì )")
        print(f"    Judge: {biggest_diff_llm['judge_rank']}ìœ„ ({biggest_diff_llm['judge_score']:.2f}ì )")

        # ìƒì„¸ ë¹„êµí‘œ ì¶œë ¥
        print(f"\nğŸ“‹ ìƒì„¸ ë¹„êµí‘œ:")
        print(df.to_string(index=False))

        return {
            'report_type': report_type,
            'num_llms': len(common_llms),
            'comparison_df': df,
            'correlations': {
                'pearson': pearson_corr,
                'spearman': spearman_corr,
                'spearman_pvalue': spearman_p,
                'kendall': kendall_corr,
                'kendall_pvalue': kendall_p
            },
            'rank_differences': {
                'mean': avg_rank_diff,
                'max': max_rank_diff,
                'std': df['rank_diff'].std()
            }
        }

    def _get_rankings(self, scores: Dict[str, float]) -> Dict[str, int]:
        """ì ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìˆœìœ„ ê³„ì‚°"""
        sorted_items = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        return {llm: rank + 1 for rank, (llm, _) in enumerate(sorted_items)}

    def analyze_evaluation_criteria_differences(
        self,
        report_type: str,
        human_criteria_scores: Dict[str, Dict[str, float]],
        judge_results_path: str
    ):
        """í‰ê°€ ê¸°ì¤€ë³„ ì°¨ì´ ë¶„ì„

        Args:
            report_type: 'weekly' or 'executive'
            human_criteria_scores: {llm_name: {criterion: score}} í˜•ì‹
            judge_results_path: Judge í‰ê°€ ê²°ê³¼ JSON íŒŒì¼ ê²½ë¡œ
        """
        with open(judge_results_path, 'r', encoding='utf-8') as f:
            judge_data = json.load(f)

        print(f"\n{'='*80}")
        print(f"{report_type.upper()} - í‰ê°€ ê¸°ì¤€ë³„ ì°¨ì´ ë¶„ì„")
        print(f"{'='*80}")

        # Judgeì˜ í‰ê°€ ê¸°ì¤€ë³„ ì ìˆ˜ ì¶”ì¶œ
        judge_criteria_scores = {}
        for llm_name, result in judge_data.get('results', {}).items():
            judge_criteria_scores[llm_name] = {}
            for criterion_result in result.get('criterion_results', []):
                criterion_key = criterion_result['criterion']
                # 10ì  ë§Œì ìœ¼ë¡œ ì •ê·œí™”
                judge_criteria_scores[llm_name][criterion_key] = criterion_result['score']

        # ë¹„êµ ë¶„ì„
        common_llms = set(human_criteria_scores.keys()) & set(judge_criteria_scores.keys())

        for llm in common_llms:
            print(f"\nğŸ¤– {llm}:")
            print(f"  ìœ¡ì•ˆ í‰ê°€:")
            for criterion, score in human_criteria_scores[llm].items():
                print(f"    {criterion}: {score}")
            print(f"  Judge í‰ê°€:")
            for criterion, score in judge_criteria_scores[llm].items():
                print(f"    {criterion}: {score}")

    def identify_disagreements(
        self,
        report_type: str,
        threshold: int = 2
    ) -> List[Dict[str, Any]]:
        """í° ì˜ê²¬ ì°¨ì´ë¥¼ ë³´ì´ëŠ” LLM ì‹ë³„

        Args:
            report_type: 'weekly' or 'executive'
            threshold: ìˆœìœ„ ì°¨ì´ ì„ê³„ê°’ (ê¸°ë³¸: 2)

        Returns:
            ì˜ê²¬ ì°¨ì´ê°€ í° LLM ë¦¬ìŠ¤íŠ¸
        """
        result = self.compare_rankings(report_type)
        df = result['comparison_df']

        disagreements = df[df['rank_diff'] >= threshold].to_dict('records')

        print(f"\nâš ï¸  í° ì˜ê²¬ ì°¨ì´ (ìˆœìœ„ ì°¨ì´ >= {threshold}):")
        for item in disagreements:
            print(f"\n  {item['llm']}:")
            print(f"    ìœ¡ì•ˆ: {item['human_rank']}ìœ„ ({item['human_score']}ì )")
            print(f"    Judge: {item['judge_rank']}ìœ„ ({item['judge_score']:.2f}ì )")
            print(f"    ìˆœìœ„ ì°¨ì´: {item['rank_diff']}")

        return disagreements

    def generate_comparison_report(
        self,
        output_dir: str
    ):
        """ì¢…í•© ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±

        Args:
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        """
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        # ê° ë³´ê³ ì„œ íƒ€ì…ë³„ë¡œ ë¹„êµ
        for report_type in self.human_scores.keys():
            if report_type not in self.judge_scores:
                continue

            # ë¹„êµ ë¶„ì„
            result = self.compare_rankings(report_type)

            # CSV ì €ì¥
            csv_path = output_path / f"comparison_{report_type}.csv"
            result['comparison_df'].to_csv(csv_path, index=False, encoding='utf-8-sig')
            print(f"\nğŸ’¾ ë¹„êµí‘œ ì €ì¥: {csv_path}")

            # Markdown ë¦¬í¬íŠ¸ ìƒì„±
            md_path = output_path / f"comparison_report_{report_type}.md"
            self._generate_markdown_report(result, md_path)
            print(f"ğŸ“ ë¦¬í¬íŠ¸ ì €ì¥: {md_path}")

            # JSON ì €ì¥
            json_path = output_path / f"comparison_{report_type}.json"
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump({
                    'report_type': result['report_type'],
                    'num_llms': result['num_llms'],
                    'correlations': result['correlations'],
                    'rank_differences': result['rank_differences'],
                    'comparison': result['comparison_df'].to_dict('records')
                }, f, indent=2, ensure_ascii=False)
            print(f"ğŸ’¾ JSON ì €ì¥: {json_path}")

    def _generate_markdown_report(
        self,
        result: Dict[str, Any],
        output_path: Path
    ):
        """Markdown ë¦¬í¬íŠ¸ ìƒì„±"""
        df = result['comparison_df']
        report_type = result['report_type']

        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(f"# ìœ¡ì•ˆ í‰ê°€ vs LLM Judge ë¹„êµ ë¦¬í¬íŠ¸\n\n")
            f.write(f"**ë³´ê³ ì„œ íƒ€ì…**: {report_type}\n\n")
            f.write(f"**ë¶„ì„ LLM ìˆ˜**: {result['num_llms']}\n\n")
            f.write("---\n\n")

            # ìƒê´€ê´€ê³„
            f.write("## ğŸ“Š ìƒê´€ê´€ê³„ ë¶„ì„\n\n")
            f.write(f"- **Pearson ìƒê´€ê³„ìˆ˜**: {result['correlations']['pearson']:.4f}\n")
            f.write(f"- **Spearman ìˆœìœ„ ìƒê´€ê³„ìˆ˜**: {result['correlations']['spearman']:.4f} (p={result['correlations']['spearman_pvalue']:.4f})\n")
            f.write(f"- **Kendall Tau**: {result['correlations']['kendall']:.4f} (p={result['correlations']['kendall_pvalue']:.4f})\n\n")

            # í•´ì„
            spearman = result['correlations']['spearman']
            if spearman > 0.8:
                interpretation = "âœ… **ë§¤ìš° ê°•í•œ ì–‘ì˜ ìƒê´€ê´€ê³„** - ìœ¡ì•ˆ í‰ê°€ì™€ Judge í‰ê°€ê°€ ë§¤ìš° ìœ ì‚¬í•©ë‹ˆë‹¤."
            elif spearman > 0.6:
                interpretation = "âœ“ **ê°•í•œ ì–‘ì˜ ìƒê´€ê´€ê³„** - ìœ¡ì•ˆ í‰ê°€ì™€ Judge í‰ê°€ê°€ ëŒ€ì²´ë¡œ ì¼ì¹˜í•©ë‹ˆë‹¤."
            elif spearman > 0.4:
                interpretation = "âš ï¸ **ì¤‘ê°„ ì •ë„ì˜ ìƒê´€ê´€ê³„** - ì¼ë¶€ ì°¨ì´ê°€ ìˆìŠµë‹ˆë‹¤."
            else:
                interpretation = "âŒ **ì•½í•œ ìƒê´€ê´€ê³„** - ìœ¡ì•ˆ í‰ê°€ì™€ Judge í‰ê°€ì˜ ì°¨ì´ê°€ í½ë‹ˆë‹¤."

            f.write(f"{interpretation}\n\n")

            # ìˆœìœ„ ì°¨ì´
            f.write("## ğŸ“ˆ ìˆœìœ„ ì°¨ì´ ë¶„ì„\n\n")
            f.write(f"- **í‰ê·  ìˆœìœ„ ì°¨ì´**: {result['rank_differences']['mean']:.2f}\n")
            f.write(f"- **ìµœëŒ€ ìˆœìœ„ ì°¨ì´**: {result['rank_differences']['max']}\n")
            f.write(f"- **í‘œì¤€í¸ì°¨**: {result['rank_differences']['std']:.2f}\n\n")

            # ìƒì„¸ ë¹„êµí‘œ
            f.write("## ğŸ“‹ ìƒì„¸ ë¹„êµí‘œ\n\n")
            f.write("| LLM | ìœ¡ì•ˆ ì ìˆ˜ | ìœ¡ì•ˆ ìˆœìœ„ | Judge ì ìˆ˜ | Judge ìˆœìœ„ | ìˆœìœ„ ì°¨ì´ |\n")
            f.write("|-----|-----------|-----------|------------|-----------|----------|\n")
            for _, row in df.iterrows():
                f.write(f"| {row['llm']} | {row['human_score']:.1f} | {row['human_rank']} | ")
                f.write(f"{row['judge_score']:.2f} | {row['judge_rank']} | {row['rank_diff']} |\n")

            f.write("\n")

            # í° ì°¨ì´ë¥¼ ë³´ì¸ LLM
            big_diff = df[df['rank_diff'] >= 2]
            if len(big_diff) > 0:
                f.write("## âš ï¸ í° ì˜ê²¬ ì°¨ì´ë¥¼ ë³´ì¸ LLM\n\n")
                for _, row in big_diff.iterrows():
                    f.write(f"### {row['llm']}\n\n")
                    f.write(f"- ìœ¡ì•ˆ í‰ê°€: {row['human_rank']}ìœ„ ({row['human_score']}ì )\n")
                    f.write(f"- Judge í‰ê°€: {row['judge_rank']}ìœ„ ({row['judge_score']:.2f}ì )\n")
                    f.write(f"- ìˆœìœ„ ì°¨ì´: {row['rank_diff']}\n\n")


def main():
    """ë©”ì¸ í•¨ìˆ˜ - ì˜ˆì œ ì‚¬ìš©ë²•"""
    comparator = HumanVsJudgeComparator()

    # ìœ¡ì•ˆ í‰ê°€ ê²°ê³¼ ì…ë ¥ (ê·€í•˜ì˜ í‰ê°€ ê²°ê³¼)
    weekly_human_scores = {
        "OpenAI GPT-4.1": 91,
        "DeepSeek-V3.1": 90,
        "Claude 4.5 Sonnet": 88,
        "Claude 4.5 Opus": 86,
        "Phi-4": 82,
        "OpenAI GPT-5.1": 81,
        "Llama-3.3-70B-Instruct": 72
    }

    executive_human_scores = {
        "OpenAI GPT-4.1": 87.5,
        "OpenAI GPT-5.1": 85.5,
        "DeepSeek-V3.1": 79.0,
        "Claude 4.5 Opus": 64.0,
        "Claude 4.5 Sonnet": 63.5,
        "Phi-4": 54.5,
        "Llama-3.3-70B-Instruct": 28.0
    }

    # ìœ¡ì•ˆ í‰ê°€ ë¡œë“œ
    comparator.load_human_evaluation('weekly', weekly_human_scores)
    comparator.load_human_evaluation('executive', executive_human_scores)

    # Judge í‰ê°€ ê²°ê³¼ ë¡œë“œ (ì‹¤ì œ íŒŒì¼ì´ ìˆë‹¤ë©´)
    # comparator.load_judge_evaluation(
    #     'weekly',
    #     'data/results/multi_llm_test/weekly/20260102_052136/judge_evaluation.json'
    # )

    # ë¹„êµ ë¶„ì„
    # comparator.compare_rankings('weekly')

    # ë¦¬í¬íŠ¸ ìƒì„±
    # comparator.generate_comparison_report('data/results/evaluation_comparison')

    print("\nâœ… ë¹„êµ ë¶„ì„ ë„êµ¬ ë¡œë“œ ì™„ë£Œ!")
    print("ğŸ“– ì‚¬ìš© ì˜ˆì‹œ:")
    print("  1. comparator.load_judge_evaluation('weekly', 'judge_results.json')")
    print("  2. comparator.compare_rankings('weekly')")
    print("  3. comparator.generate_comparison_report('output_dir')")


if __name__ == "__main__":
    main()
