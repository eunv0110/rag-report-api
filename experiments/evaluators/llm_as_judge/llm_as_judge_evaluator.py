#!/usr/bin/env python3
"""LLM as Judge í‰ê°€ê¸° - ìƒì„±ëœ ë³´ê³ ì„œì˜ í’ˆì§ˆì„ LLMìœ¼ë¡œ í‰ê°€

ì—¬ëŸ¬ LLMì´ ìƒì„±í•œ ì£¼ê°„/ì›”ê°„ ë³´ê³ ì„œë¥¼ í‰ê°€í•˜ì—¬ ì–´ë–¤ ë³´ê³ ì„œê°€ ë” ì í•©í•œì§€ íŒë‹¨í•©ë‹ˆë‹¤.
Azure AI ë° OpenRouterë¥¼ í†µí•œ ë‹¤ì–‘í•œ Judge ëª¨ë¸ ì§€ì›.
"""

import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

import json
import re
from typing import Dict, Any, List, Optional
from datetime import datetime
from langchain.chat_models import init_chat_model
from openai import OpenAI
import pandas as pd


class LLMAsJudgeEvaluator:
    """LLM as Judge í‰ê°€ í´ë˜ìŠ¤

    Azure AI ë˜ëŠ” OpenRouterë¥¼ í†µí•´ ë‹¤ì–‘í•œ Judge ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬
    ìƒì„±ëœ ë³´ê³ ì„œì˜ í’ˆì§ˆì„ í‰ê°€í•©ë‹ˆë‹¤.
    """

    # í‰ê°€ ê¸°ì¤€ ì •ì˜
    EVALUATION_CRITERIA = {
        "weekly_report": {
            "accuracy": {
                "name": "ì •í™•ì„± (Accuracy)",
                "description": "ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ë‚´ìš©ì„ ì •í™•í•˜ê²Œ ë°˜ì˜í•˜ê³  ìˆëŠ”ê°€? í™˜ê°ì€ ì—†ëŠ”ê°€?",
                "weight": 0.30
            },
            "completeness": {
                "name": "ì™„ê²°ì„± (Completeness)",
                "description": "ì£¼ìš” ì§€í‘œ, í™œë™, ì´ìŠˆ, ë‹¤ìŒ ì£¼ ê³„íš ë“± í•„ìš”í•œ ì •ë³´ê°€ ë¹ ì§ì—†ì´ í¬í•¨ë˜ì—ˆëŠ”ê°€?",
                "weight": 0.25
            },
            "structure": {
                "name": "êµ¬ì¡°í™” (Structure)",
                "description": "ì£¼ê°„ ë³´ê³ ì„œ í˜•ì‹ì— ì í•©í•˜ê²Œ ë…¼ë¦¬ì ìœ¼ë¡œ êµ¬ì¡°í™”ë˜ì—ˆëŠ”ê°€?",
                "weight": 0.25
            },
            "detail": {
                "name": "ìƒì„¸ë„ (Detail)",
                "description": "ì ì ˆí•œ ìˆ˜ì¤€ì˜ ìƒì„¸ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ”ê°€? ë„ˆë¬´ ê°„ëµí•˜ê±°ë‚˜ ì§€ë‚˜ì¹˜ê²Œ ì¥í™©í•˜ì§€ ì•Šì€ê°€?",
                "weight": 0.20
            }
        },
        "executive_report": {
            "structural_completeness": {
                "name": "êµ¬ì¡° ì™„ì„±ë„ (Structural Completeness)",
                "description": "ë³´ê³ ì„œ êµ¬ì¡°ê°€ ë…¼ë¦¬ì ì´ê³  ì™„ì„±ë„ê°€ ë†’ì€ê°€? ê²½ì˜ì§„ ë³´ê³ ì„œë¡œì„œ ì ì ˆí•œ êµ¬ì¡°ì¸ê°€?",
                "weight": 0.30
            },
            "document_reference_accuracy": {
                "name": "ë¬¸ì„œ ì°¸ì¡° ì •í™•ì„± (Document Reference Accuracy)",
                "description": "ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ë‚´ìš©ì„ ì •í™•í•˜ê²Œ ì°¸ì¡°í•˜ê³  ë°˜ì˜í•˜ê³  ìˆëŠ”ê°€? í™˜ê°ì€ ì—†ëŠ”ê°€?",
                "weight": 0.25
            },
            "practical_value": {
                "name": "ë‚´ìš© ì‹¤ìš©ì„± (Practical Value)",
                "description": "ê²½ì˜ì§„ì´ ì˜ì‚¬ê²°ì •ì— ì‹¤ì œë¡œ í™œìš©í•  ìˆ˜ ìˆëŠ” ì‹¤ìš©ì ì¸ ì •ë³´ì™€ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•˜ëŠ”ê°€?",
                "weight": 0.25
            },
            "conciseness": {
                "name": "ê°„ê²°ì„± (Conciseness)",
                "description": "í•µì‹¬ë§Œ ê°„ê²°í•˜ê²Œ ìš”ì•½ë˜ì—ˆëŠ”ê°€? ë¶ˆí•„ìš”í•œ ì„¸ë¶€ì‚¬í•­ ì—†ì´ ëª…ë£Œí•œê°€?",
                "weight": 0.20
            }
        }
    }

    def __init__(
        self,
        judge_model: str = "gpt-4o",
        provider: str = "azure_ai",
        temperature: float = 0
    ):
        """
        Args:
            judge_model: í‰ê°€ì— ì‚¬ìš©í•  LLM ëª¨ë¸
                - Azure AI: gpt-4o, gpt-4.5, o1 ë“±
                - OpenRouter: anthropic/claude-opus-4.5 ë“±
            provider: ì‚¬ìš©í•  ì œê³µì ("azure_ai" ë˜ëŠ” "openrouter")
            temperature: ìƒì„± ì˜¨ë„ (ê¸°ë³¸: 0 - ì¼ê´€ëœ í‰ê°€ë¥¼ ìœ„í•¨)
        """
        self.judge_model = judge_model
        self.provider = provider
        self.temperature = temperature

        # Judge LLM ì´ˆê¸°í™”
        if provider == "azure_ai":
            from config.settings import AZURE_AI_CREDENTIAL, AZURE_AI_ENDPOINT

            self.llm = init_chat_model(
                model=f"azure_ai:{judge_model}",
                api_key=AZURE_AI_CREDENTIAL,
                azure_endpoint=AZURE_AI_ENDPOINT,
                temperature=temperature
            )
            self.client = None
        elif provider == "openrouter":
            from config.settings import OPENROUTER_API_KEY, OPENROUTER_BASE_URL

            self.llm = None
            self.client = OpenAI(
                api_key=OPENROUTER_API_KEY,
                base_url=OPENROUTER_BASE_URL
            )
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” provider: {provider}. 'azure_ai' ë˜ëŠ” 'openrouter'ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")

    def _load_prompt_template(self, template_name: str) -> str:
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¡œë“œ

        Args:
            template_name: í…œí”Œë¦¿ íŒŒì¼ ì´ë¦„

        Returns:
            í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¬¸ìì—´
        """
        template_path = Path(__file__).parent.parent.parent / "prompts" / "templates" / "evaluation" / "llm_judge" / template_name
        with open(template_path, 'r', encoding='utf-8') as f:
            return f.read()

    def _create_evaluation_prompt(
        self,
        question: str,
        answer: str,
        report_type: str,
        criteria_name: str,
        criteria_desc: str
    ) -> str:
        """í‰ê°€ í”„ë¡¬í”„íŠ¸ ìƒì„±

        Args:
            question: ì›ë³¸ ì§ˆë¬¸
            answer: í‰ê°€í•  ë‹µë³€
            report_type: ë³´ê³ ì„œ íƒ€ì… (weekly_report, executive_report)
            criteria_name: í‰ê°€ ê¸°ì¤€ ì´ë¦„
            criteria_desc: í‰ê°€ ê¸°ì¤€ ì„¤ëª…

        Returns:
            í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´
        """
        report_type_ko = "ì£¼ê°„ ë³´ê³ ì„œ" if report_type == "weekly_report" else "ê²½ì˜ì§„ ë³´ê³ ì„œ"

        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¡œë“œ
        template = self._load_prompt_template("criterion_evaluation_prompt.txt")

        # í…œí”Œë¦¿ì— ê°’ ì±„ìš°ê¸°
        prompt = template.replace("{report_type_ko}", report_type_ko)\
                        .replace("{criteria_name}", criteria_name)\
                        .replace("{criteria_desc}", criteria_desc)\
                        .replace("{question}", question)\
                        .replace("{answer}", answer)

        return prompt

    def _evaluate_single_criterion(
        self,
        question: str,
        answer: str,
        report_type: str,
        criterion_key: str,
        criterion_info: Dict[str, Any],
        max_retries: int = 3
    ) -> Dict[str, Any]:
        """ë‹¨ì¼ í‰ê°€ ê¸°ì¤€ìœ¼ë¡œ ë‹µë³€ í‰ê°€

        Args:
            question: ì§ˆë¬¸
            answer: ë‹µë³€
            report_type: ë³´ê³ ì„œ íƒ€ì…
            criterion_key: í‰ê°€ ê¸°ì¤€ í‚¤
            criterion_info: í‰ê°€ ê¸°ì¤€ ì •ë³´
            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜

        Returns:
            í‰ê°€ ê²°ê³¼
        """
        prompt = self._create_evaluation_prompt(
            question=question,
            answer=answer,
            report_type=report_type,
            criteria_name=criterion_info["name"],
            criteria_desc=criterion_info["description"]
        )

        messages = [
            {"role": "system", "content": "ë‹¹ì‹ ì€ ë³´ê³ ì„œ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê³µì •í•˜ê³  ê°ê´€ì ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”. ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì•¼ í•©ë‹ˆë‹¤."},
            {"role": "user", "content": prompt}
        ]

        last_error = None
        for attempt in range(max_retries):
            try:
                if attempt > 0:
                    print(f"   ğŸ”„ ì¬ì‹œë„ {attempt}/{max_retries-1}...")
                    import time
                    time.sleep(2)  # ì¬ì‹œë„ ì „ 2ì´ˆ ëŒ€ê¸°

                # Providerì— ë”°ë¼ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ í˜¸ì¶œ
                if self.provider == "azure_ai":
                    # Azure AIì˜ ê²½ìš° - langchainì„ í†µí•œ í˜¸ì¶œ
                    # max_completion_tokens ë˜ëŠ” max_tokensë¥¼ ì‹œë„
                    try:
                        # GPT-5.1 ë“± ìƒˆ ëª¨ë¸ì€ max_completion_tokens ì‚¬ìš©
                        response = self.llm.invoke(messages, max_completion_tokens=8000)
                        content = response.content
                    except Exception as e:
                        error_msg = str(e)
                        # max_tokensë¡œ ì¬ì‹œë„
                        if "max_completion_tokens" in error_msg or "unsupported" in error_msg.lower():
                            try:
                                response = self.llm.invoke(messages, max_tokens=8000)
                                content = response.content
                            except:
                                # ë‘˜ ë‹¤ ì•ˆë˜ë©´ íŒŒë¼ë¯¸í„° ì—†ì´ í˜¸ì¶œ
                                response = self.llm.invoke(messages)
                                content = response.content
                        else:
                            # ë‹¤ë¥¸ ì—ëŸ¬ë©´ íŒŒë¼ë¯¸í„° ì—†ì´ ì¬ì‹œë„
                            response = self.llm.invoke(messages)
                            content = response.content
                else:  # openrouter
                    # JSON ëª¨ë“œë¥¼ ì‹œë„í•˜ë˜, ì‹¤íŒ¨í•˜ë©´ ì¼ë°˜ ëª¨ë“œë¡œ í´ë°±
                    try:
                        response = self.client.chat.completions.create(
                            model=self.judge_model,
                            messages=messages,
                            max_tokens=8000,  # í† í° ì œí•œ ì¦ê°€ (4000 -> 8000)
                            temperature=self.temperature,
                            response_format={"type": "json_object"}  # JSON ëª¨ë“œ í™œì„±í™”
                        )
                        content = response.choices[0].message.content
                    except Exception as json_mode_error:
                        # JSON ëª¨ë“œ ì‹¤íŒ¨ ì‹œ ì¼ë°˜ ëª¨ë“œë¡œ ì¬ì‹œë„
                        if attempt == 0:
                            print(f"âš ï¸  JSON ëª¨ë“œ ì‹¤íŒ¨, ì¼ë°˜ ëª¨ë“œë¡œ ì¬ì‹œë„: {json_mode_error}")
                        response = self.client.chat.completions.create(
                            model=self.judge_model,
                            messages=messages,
                            max_tokens=8000,
                            temperature=self.temperature
                        )
                        content = response.choices[0].message.content

                # ì‘ë‹µ ë‚´ìš© í™•ì¸
                if not content or content.strip() == "":
                    raise ValueError("LLMì´ ë¹ˆ ì‘ë‹µì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤")

                # JSON ì¶”ì¶œ - ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›
                result = None

                # 1. ```json ... ``` í˜•ì‹
                json_match = re.search(r'```json\s*(\{.*?\})\s*```', content, re.DOTALL)
                if json_match:
                    try:
                        result = json.loads(json_match.group(1))
                    except json.JSONDecodeError:
                        pass

                # 2. ``` ... ``` í˜•ì‹ (json í‚¤ì›Œë“œ ì—†ì´)
                if result is None:
                    json_match = re.search(r'```\s*(\{.*?\})\s*```', content, re.DOTALL)
                    if json_match:
                        try:
                            result = json.loads(json_match.group(1))
                        except json.JSONDecodeError:
                            pass

                # 3. ìˆœìˆ˜ JSON (ì½”ë“œ ë¸”ë¡ ì—†ì´)
                if result is None:
                    try:
                        result = json.loads(content)
                    except json.JSONDecodeError:
                        # 4. JSON ê°ì²´ ë¶€ë¶„ë§Œ ì¶”ì¶œ ì‹œë„ (non-greedy)
                        json_match = re.search(r'\{.*?\}', content, re.DOTALL)
                        if json_match:
                            try:
                                result = json.loads(json_match.group(0))
                            except json.JSONDecodeError:
                                pass

                        # 5. ê°€ì¥ ê¸´ JSON ê°ì²´ ì¶”ì¶œ ì‹œë„ (greedy, ë¶ˆì™„ì „í•œ JSON ì²˜ë¦¬)
                        if result is None:
                            json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', content, re.DOTALL)
                            if json_match:
                                try:
                                    result = json.loads(json_match.group(0))
                                except json.JSONDecodeError:
                                    pass

                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ - ë¶€ë¶„ ë°ì´í„° ìˆ˜ë™ ì¶”ì¶œ ì‹œë„
                if result is None:
                    if attempt == 0:
                        print(f"âš ï¸  JSON íŒŒì‹± ì‹¤íŒ¨. ìˆ˜ë™ ì¶”ì¶œ ì‹œë„ ì¤‘...")
                        print(f"ì›ë³¸ ì‘ë‹µ:\n{content[:500]}")

                    # ìˆ˜ë™ìœ¼ë¡œ í•„ë“œ ì¶”ì¶œ
                    score_match = re.search(r'"score"\s*:\s*(\d+)', content)
                    reasoning_match = re.search(r'"reasoning"\s*:\s*"([^"]*(?:\\"[^"]*)*)"', content, re.DOTALL)

                    if score_match and reasoning_match:
                        result = {
                            "score": int(score_match.group(1)),
                            "reasoning": reasoning_match.group(1),
                            "strengths": [],
                            "weaknesses": []
                        }

                        # strengths ì¶”ì¶œ ì‹œë„
                        strengths_match = re.search(r'"strengths"\s*:\s*\[(.*?)\]', content, re.DOTALL)
                        if strengths_match:
                            strengths_str = strengths_match.group(1)
                            strengths = re.findall(r'"([^"]*)"', strengths_str)
                            result["strengths"] = strengths

                        # weaknesses ì¶”ì¶œ ì‹œë„
                        weaknesses_match = re.search(r'"weaknesses"\s*:\s*\[(.*?)\]', content, re.DOTALL)
                        if weaknesses_match:
                            weaknesses_str = weaknesses_match.group(1)
                            weaknesses = re.findall(r'"([^"]*)"', weaknesses_str)
                            result["weaknesses"] = weaknesses

                        print(f"âœ… ìˆ˜ë™ ì¶”ì¶œ ì„±ê³µ: score={result['score']}")
                    else:
                        raise ValueError("JSON íŒŒì‹± ì‹¤íŒ¨ - score ë˜ëŠ” reasoningì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")

                # ì„±ê³µ ì‹œ ê²°ê³¼ ë°˜í™˜
                return {
                    "criterion": criterion_key,
                    "criterion_name": criterion_info["name"],
                    "weight": criterion_info["weight"],
                    "score": result.get("score", 0),
                    "weighted_score": result.get("score", 0) * criterion_info["weight"],
                    "reasoning": result.get("reasoning", ""),
                    "strengths": result.get("strengths", []),
                    "weaknesses": result.get("weaknesses", [])
                }

            except Exception as e:
                last_error = e
                if attempt < max_retries - 1:
                    print(f"âš ï¸  ì‹œë„ {attempt + 1} ì‹¤íŒ¨: {e}")
                continue

        # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨ ì‹œ
        print(f"âŒ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({criterion_key}): {last_error}")
        print(f"   ëª¨ë¸: {self.judge_model}, Provider: {self.provider}")
        print(f"   {max_retries}ë²ˆ ì¬ì‹œë„ ëª¨ë‘ ì‹¤íŒ¨")
        import traceback
        print(f"   ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
        return {
            "criterion": criterion_key,
            "criterion_name": criterion_info["name"],
            "weight": criterion_info["weight"],
            "score": 0,
            "weighted_score": 0,
            "reasoning": f"í‰ê°€ ì‹¤íŒ¨ ({max_retries}ë²ˆ ì¬ì‹œë„): {str(last_error)}",
            "strengths": [],
            "weaknesses": [],
            "error": str(last_error)
        }

    def evaluate_answer(
        self,
        question: str,
        answer: str,
        report_type: str = "weekly_report"
    ) -> Dict[str, Any]:
        """ë‹µë³€ì„ ëª¨ë“  ê¸°ì¤€ìœ¼ë¡œ í‰ê°€

        Args:
            question: ì§ˆë¬¸
            answer: ë‹µë³€
            report_type: ë³´ê³ ì„œ íƒ€ì… (weekly_report, executive_report)

        Returns:
            ì¢…í•© í‰ê°€ ê²°ê³¼
        """
        if report_type not in self.EVALUATION_CRITERIA:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë³´ê³ ì„œ íƒ€ì…: {report_type}")

        criteria = self.EVALUATION_CRITERIA[report_type]
        criterion_results = []

        print(f"\n{'='*80}")
        print(f"ğŸ“Š {report_type} í‰ê°€ ì‹œì‘")
        print(f"{'='*80}")

        # ê° í‰ê°€ ê¸°ì¤€ë³„ë¡œ í‰ê°€
        for criterion_key, criterion_info in criteria.items():
            print(f"\ní‰ê°€ ê¸°ì¤€: {criterion_info['name']}")

            result = self._evaluate_single_criterion(
                question=question,
                answer=answer,
                report_type=report_type,
                criterion_key=criterion_key,
                criterion_info=criterion_info
            )

            criterion_results.append(result)
            print(f"  ì ìˆ˜: {result['score']}/10 (ê°€ì¤‘ì¹˜: {result['weight']}, ê°€ì¤‘ ì ìˆ˜: {result['weighted_score']:.2f})")

        # ì¢…í•© ì ìˆ˜ ê³„ì‚°
        total_weighted_score = sum(r["weighted_score"] for r in criterion_results)
        total_max_score = sum(10 * r["weight"] for r in criterion_results)
        final_score = (total_weighted_score / total_max_score) * 10

        print(f"\n{'='*80}")
        print(f"âœ… ìµœì¢… ì ìˆ˜: {final_score:.2f}/10")
        print(f"{'='*80}")

        return {
            "report_type": report_type,
            "final_score": final_score,
            "total_weighted_score": total_weighted_score,
            "criterion_results": criterion_results,
            "timestamp": datetime.now().isoformat()
        }

    def compare_multiple_answers(
        self,
        question: str,
        answers: Dict[str, str],
        report_type: str = "weekly_report"
    ) -> Dict[str, Any]:
        """ì—¬ëŸ¬ ë‹µë³€ì„ ë¹„êµ í‰ê°€

        Args:
            question: ì§ˆë¬¸
            answers: {llm_name: answer} í˜•ì‹ì˜ ë”•ì…”ë„ˆë¦¬
            report_type: ë³´ê³ ì„œ íƒ€ì…

        Returns:
            ë¹„êµ í‰ê°€ ê²°ê³¼
        """
        results = {}

        print(f"\n{'='*80}")
        print(f"ğŸ” {len(answers)}ê°œ ë‹µë³€ ë¹„êµ í‰ê°€")
        print(f"{'='*80}")

        # ê° ë‹µë³€ í‰ê°€
        for llm_name, answer in answers.items():
            print(f"\n[{llm_name}] í‰ê°€ ì¤‘...")
            results[llm_name] = self.evaluate_answer(question, answer, report_type)

        # ë­í‚¹ ìƒì„±
        ranking = sorted(
            results.items(),
            key=lambda x: x[1]["final_score"],
            reverse=True
        )

        print(f"\n{'='*80}")
        print("ğŸ† ìµœì¢… ìˆœìœ„")
        print(f"{'='*80}")
        for rank, (llm_name, result) in enumerate(ranking, 1):
            print(f"{rank}ìœ„: {llm_name:<20} - {result['final_score']:.2f}/10")

        return {
            "question": question,
            "report_type": report_type,
            "num_answers": len(answers),
            "results": results,
            "ranking": [(name, result["final_score"]) for name, result in ranking],
            "timestamp": datetime.now().isoformat()
        }

    def evaluate_from_results_dir(
        self,
        results_dir: str,
        report_type: str = "weekly_report",
        output_path: str = None
    ) -> Dict[str, Any]:
        """ê²°ê³¼ ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  LLM ë‹µë³€ì„ ë¡œë“œí•˜ê³  í‰ê°€

        Args:
            results_dir: ê²°ê³¼ ë””ë ‰í† ë¦¬ ê²½ë¡œ (ì˜ˆ: .../llm_comparison/bge_m3_rrf_ensemble_20251228_114425)
            report_type: ë³´ê³ ì„œ íƒ€ì…
            output_path: ê²°ê³¼ ì €ì¥ ê²½ë¡œ (Noneì´ë©´ ìë™ ìƒì„±)

        Returns:
            í‰ê°€ ê²°ê³¼
        """
        results_path = Path(results_dir)

        if not results_path.exists():
            raise FileNotFoundError(f"ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {results_dir}")

        # ê° LLM ë””ë ‰í† ë¦¬ì—ì„œ results.json ë¡œë“œ
        answers = {}
        question = None

        for llm_dir in results_path.iterdir():
            if not llm_dir.is_dir():
                continue

            result_file = llm_dir / "results.json"
            if not result_file.exists():
                print(f"âš ï¸  {llm_dir.name}ì˜ results.jsonì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                continue

            try:
                with open(result_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                if data.get("results") and len(data["results"]) > 0:
                    result = data["results"][0]

                    if question is None:
                        question = result.get("question", "")

                    if result.get("result", {}).get("success"):
                        answers[data["llm_name"]] = result["result"]["answer"]
                        print(f"âœ… {data['llm_name']} ë‹µë³€ ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸  {llm_dir.name} ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")

        if not answers:
            raise ValueError("í‰ê°€í•  ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        if not question:
            raise ValueError("ì§ˆë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # ë¹„êµ í‰ê°€ ì‹¤í–‰
        comparison_result = self.compare_multiple_answers(question, answers, report_type)

        # ê²°ê³¼ ì €ì¥
        if output_path is None:
            output_path = results_path / f"llm_judge_evaluation_{report_type}.json"

        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(comparison_result, f, indent=2, ensure_ascii=False)

        print(f"\nğŸ’¾ í‰ê°€ ê²°ê³¼ ì €ì¥: {output_path}")

        # ìƒì„¸ ë³´ê³ ì„œ ìƒì„± (CSV)
        self._save_detailed_report(comparison_result, output_path.parent / f"llm_judge_report_{report_type}.csv")

        return comparison_result

    def batch_evaluate_from_dir(
        self,
        base_dir: str,
        report_type: str = "weekly_report"
    ) -> Dict[str, Any]:
        """ì—¬ëŸ¬ retriever ê²°ê³¼ë¥¼ ì¼ê´„ í‰ê°€

        Args:
            base_dir: llm_comparison ë””ë ‰í† ë¦¬ ê²½ë¡œ
            report_type: ë³´ê³ ì„œ íƒ€ì…

        Returns:
            ì „ì²´ í‰ê°€ ê²°ê³¼
        """
        base_path = Path(base_dir)

        if not base_path.exists():
            raise FileNotFoundError(f"ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {base_dir}")

        all_results = {}

        # ê° retriever ë””ë ‰í† ë¦¬ ìˆœíšŒ
        for retriever_dir in sorted(base_path.iterdir()):
            if not retriever_dir.is_dir():
                continue

            retriever_name = retriever_dir.name
            print(f"\n{'='*100}")
            print(f"ğŸ“‚ {retriever_name} í‰ê°€ ì¤‘...")
            print(f"{'='*100}")

            try:
                result = self.evaluate_from_results_dir(
                    results_dir=str(retriever_dir),
                    report_type=report_type
                )
                all_results[retriever_name] = result
                print(f"âœ… {retriever_name} í‰ê°€ ì™„ë£Œ")
            except Exception as e:
                print(f"âŒ {retriever_name} í‰ê°€ ì‹¤íŒ¨: {e}")
                all_results[retriever_name] = {"error": str(e)}

        # ì „ì²´ ê²°ê³¼ ì €ì¥
        output_path = base_path / f"all_evaluations_{report_type}.json"
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, indent=2, ensure_ascii=False)

        print(f"\nğŸ’¾ ì „ì²´ í‰ê°€ ê²°ê³¼ ì €ì¥: {output_path}")

        # ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
        self._create_summary_report(all_results, base_path / f"summary_{report_type}.csv")

        return all_results

    def _create_summary_report(self, all_results: Dict[str, Any], output_path: Path):
        """ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±

        Args:
            all_results: ì „ì²´ í‰ê°€ ê²°ê³¼
            output_path: CSV ì¶œë ¥ ê²½ë¡œ
        """
        rows = []

        for retriever_name, result in all_results.items():
            if "error" in result:
                continue

            for llm_name, llm_result in result.get("results", {}).items():
                row = {
                    "retriever": retriever_name,
                    "llm_name": llm_name,
                    "final_score": llm_result["final_score"]
                }

                # ê° í‰ê°€ ê¸°ì¤€ë³„ ì ìˆ˜ ì¶”ê°€
                for criterion_result in llm_result["criterion_results"]:
                    criterion_key = criterion_result["criterion"]
                    row[f"{criterion_key}_score"] = criterion_result["score"]
                    row[f"{criterion_key}_weighted"] = criterion_result["weighted_score"]

                rows.append(row)

        df = pd.DataFrame(rows)

        # ì •ë ¬: retrieverë³„, ìµœì¢… ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ
        df = df.sort_values(["retriever", "final_score"], ascending=[True, False])

        df.to_csv(output_path, index=False, encoding='utf-8-sig')
        print(f"ğŸ“Š ìš”ì•½ ë¦¬í¬íŠ¸ ì €ì¥: {output_path}")

        # ì½˜ì†” ì¶œë ¥
        print(f"\n{'='*100}")
        print("ğŸ† Retrieverë³„ ìµœê³  ì ìˆ˜ LLM")
        print(f"{'='*100}")

        for retriever in df["retriever"].unique():
            retriever_df = df[df["retriever"] == retriever]
            top_llm = retriever_df.iloc[0]
            print(f"\n{retriever}:")
            print(f"  1ìœ„: {top_llm['llm_name']:<20} - {top_llm['final_score']:.2f}/10")

            if len(retriever_df) > 1:
                second_llm = retriever_df.iloc[1]
                print(f"  2ìœ„: {second_llm['llm_name']:<20} - {second_llm['final_score']:.2f}/10")

        # ì „ì²´ LLMë³„ í‰ê·  ì ìˆ˜
        print(f"\n{'='*100}")
        print("ğŸ¯ LLMë³„ í‰ê·  ì ìˆ˜ (ëª¨ë“  retriever)")
        print(f"{'='*100}")

        llm_avg = df.groupby("llm_name")["final_score"].mean().sort_values(ascending=False)
        for llm_name, avg_score in llm_avg.items():
            print(f"  {llm_name:<20} - {avg_score:.2f}/10")

    def _save_detailed_report(self, comparison_result: Dict[str, Any], output_path: Path):
        """ìƒì„¸ í‰ê°€ ë³´ê³ ì„œë¥¼ CSVë¡œ ì €ì¥

        Args:
            comparison_result: ë¹„êµ í‰ê°€ ê²°ê³¼
            output_path: CSV ì¶œë ¥ ê²½ë¡œ
        """
        rows = []

        for llm_name, result in comparison_result["results"].items():
            for criterion_result in result["criterion_results"]:
                rows.append({
                    "llm_name": llm_name,
                    "final_score": result["final_score"],
                    "criterion": criterion_result["criterion"],
                    "criterion_name": criterion_result["criterion_name"],
                    "weight": criterion_result["weight"],
                    "score": criterion_result["score"],
                    "weighted_score": criterion_result["weighted_score"],
                    "reasoning": criterion_result["reasoning"],
                    "strengths": " | ".join(criterion_result["strengths"]),
                    "weaknesses": " | ".join(criterion_result["weaknesses"])
                })

        df = pd.DataFrame(rows)
        df.to_csv(output_path, index=False, encoding='utf-8-sig')
        print(f"ğŸ“Š ìƒì„¸ ë³´ê³ ì„œ ì €ì¥: {output_path}")

    def analyze_judge_bias(
        self,
        all_evaluations: List[Dict[str, Any]],
        judge_model: str,
        output_dir: Path
    ) -> Dict[str, Any]:
        """Judge ëª¨ë¸ì˜ í‰ê°€ í¸í–¥ ë¶„ì„

        ìì‚¬ ëª¨ë¸ ì„ í˜¸ í¸í–¥, ì¼ê´€ì„± ë“±ì„ ë¶„ì„í•©ë‹ˆë‹¤.

        Args:
            all_evaluations: ëª¨ë“  í‰ê°€ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            judge_model: í‰ê°€ì— ì‚¬ìš©í•œ ëª¨ë¸
            output_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬

        Returns:
            í¸í–¥ ë¶„ì„ ê²°ê³¼
        """
        print(f"\n{'='*100}")
        print(f"ğŸ” Judge ëª¨ë¸ í¸í–¥ ë¶„ì„: {judge_model}")
        print(f"{'='*100}")

        # ì œì¡°ì‚¬ë³„ LLM ë¶„ë¥˜
        vendor_map = {
            "openai": ["gpt", "o1"],
            "anthropic": ["claude"],
            "meta": ["llama"],
            "microsoft": ["phi"],
            "deepseek": ["deepseek"],
            "google": ["gemini"],
            "alibaba": ["qwen"]
        }

        # Judge ëª¨ë¸ì˜ ì œì¡°ì‚¬ ì‹ë³„
        judge_vendor = None
        for vendor, keywords in vendor_map.items():
            if any(keyword in judge_model.lower() for keyword in keywords):
                judge_vendor = vendor
                break

        print(f"ğŸ“Œ Judge ëª¨ë¸ ì œì¡°ì‚¬: {judge_vendor or 'ì•Œ ìˆ˜ ì—†ìŒ'}")

        # LLMë³„ ì ìˆ˜ ë° ìˆœìœ„ ìˆ˜ì§‘
        llm_rankings = {}
        llm_scores = {}
        llm_vendors = {}

        for evaluation in all_evaluations:
            for rank, (llm_name, score) in enumerate(evaluation['ranking'], 1):
                if llm_name not in llm_rankings:
                    llm_rankings[llm_name] = []
                    llm_scores[llm_name] = []

                    # LLM ì œì¡°ì‚¬ ì‹ë³„
                    llm_vendor = None
                    for vendor, keywords in vendor_map.items():
                        if any(keyword in llm_name.lower() for keyword in keywords):
                            llm_vendor = vendor
                            break
                    llm_vendors[llm_name] = llm_vendor

                llm_rankings[llm_name].append(rank)
                llm_scores[llm_name].append(score)

        # í†µê³„ ê³„ì‚°
        stats = []
        for llm_name in llm_scores.keys():
            scores = llm_scores[llm_name]
            rankings = llm_rankings[llm_name]
            llm_vendor = llm_vendors[llm_name]
            is_same_vendor = (llm_vendor == judge_vendor) if (llm_vendor and judge_vendor) else False

            stats.append({
                "llm_name": llm_name,
                "llm_vendor": llm_vendor or "unknown",
                "is_same_vendor": is_same_vendor,
                "avg_score": sum(scores) / len(scores),
                "std_score": pd.Series(scores).std(),
                "avg_rank": sum(rankings) / len(rankings),
                "first_place_count": rankings.count(1),
                "last_place_count": rankings.count(max(rankings)),
                "num_evaluations": len(scores)
            })

        # DataFrame ìƒì„± ë° ì •ë ¬
        df_stats = pd.DataFrame(stats)
        df_stats = df_stats.sort_values("avg_score", ascending=False)

        # ìì‚¬ vs íƒ€ì‚¬ ë¹„êµ
        if judge_vendor:
            same_vendor_df = df_stats[df_stats["is_same_vendor"] == True]
            other_vendor_df = df_stats[df_stats["is_same_vendor"] == False]

            bias_analysis = {
                "judge_model": judge_model,
                "judge_vendor": judge_vendor,
                "same_vendor_avg_score": same_vendor_df["avg_score"].mean() if len(same_vendor_df) > 0 else 0,
                "other_vendor_avg_score": other_vendor_df["avg_score"].mean() if len(other_vendor_df) > 0 else 0,
                "same_vendor_avg_rank": same_vendor_df["avg_rank"].mean() if len(same_vendor_df) > 0 else 0,
                "other_vendor_avg_rank": other_vendor_df["avg_rank"].mean() if len(other_vendor_df) > 0 else 0,
                "score_difference": (same_vendor_df["avg_score"].mean() - other_vendor_df["avg_score"].mean()) if (len(same_vendor_df) > 0 and len(other_vendor_df) > 0) else 0,
                "num_same_vendor_llms": len(same_vendor_df),
                "num_other_vendor_llms": len(other_vendor_df)
            }

            print(f"\nğŸ“Š ìì‚¬ ëª¨ë¸ í¸í–¥ ë¶„ì„:")
            print(f"  ìì‚¬ ëª¨ë¸ í‰ê·  ì ìˆ˜: {bias_analysis['same_vendor_avg_score']:.3f}")
            print(f"  íƒ€ì‚¬ ëª¨ë¸ í‰ê·  ì ìˆ˜: {bias_analysis['other_vendor_avg_score']:.3f}")
            print(f"  ì ìˆ˜ ì°¨ì´: {bias_analysis['score_difference']:.3f}")
            print(f"  ìì‚¬ ëª¨ë¸ í‰ê·  ìˆœìœ„: {bias_analysis['same_vendor_avg_rank']:.2f}")
            print(f"  íƒ€ì‚¬ ëª¨ë¸ í‰ê·  ìˆœìœ„: {bias_analysis['other_vendor_avg_rank']:.2f}")

            if bias_analysis['score_difference'] > 0.5:
                print(f"\nâš ï¸  ê²½ê³ : ìì‚¬ ëª¨ë¸ ì„ í˜¸ í¸í–¥ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤ (ì°¨ì´: {bias_analysis['score_difference']:.3f})")
            elif bias_analysis['score_difference'] < -0.5:
                print(f"\nâš ï¸  ê²½ê³ : ìì‚¬ ëª¨ë¸ì— ëŒ€í•œ ë¶€ì •ì  í¸í–¥ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤ (ì°¨ì´: {bias_analysis['score_difference']:.3f})")
            else:
                print(f"\nâœ… ìì‚¬ ëª¨ë¸ í¸í–¥ì´ ì ì ˆí•œ ìˆ˜ì¤€ì…ë‹ˆë‹¤ (ì°¨ì´: {bias_analysis['score_difference']:.3f})")
        else:
            bias_analysis = {
                "judge_model": judge_model,
                "judge_vendor": "unknown",
                "message": "Judge ëª¨ë¸ì˜ ì œì¡°ì‚¬ë¥¼ ì‹ë³„í•  ìˆ˜ ì—†ì–´ í¸í–¥ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            }

        # ìƒì„¸ í†µê³„ ì €ì¥
        stats_csv = output_dir / f"bias_analysis_stats_{judge_model.replace('/', '_')}.csv"
        df_stats.to_csv(stats_csv, index=False, encoding='utf-8-sig')
        print(f"\nğŸ’¾ ìƒì„¸ í†µê³„ ì €ì¥: {stats_csv}")

        # í¸í–¥ ë¶„ì„ ê²°ê³¼ ì €ì¥
        bias_json = output_dir / f"bias_analysis_{judge_model.replace('/', '_')}.json"
        bias_result = {
            **bias_analysis,
            "detailed_stats": stats,
            "timestamp": datetime.now().isoformat()
        }
        with open(bias_json, 'w', encoding='utf-8') as f:
            json.dump(bias_result, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ í¸í–¥ ë¶„ì„ ê²°ê³¼ ì €ì¥: {bias_json}")

        # ì‹œê°í™”ë¥¼ ìœ„í•œ ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸
        md_file = output_dir / f"bias_analysis_{judge_model.replace('/', '_')}.md"
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write(f"# Judge ëª¨ë¸ í¸í–¥ ë¶„ì„ ë¦¬í¬íŠ¸\n\n")
            f.write(f"**Judge ëª¨ë¸**: {judge_model}\n\n")
            f.write(f"**Judge ì œì¡°ì‚¬**: {judge_vendor or 'ì•Œ ìˆ˜ ì—†ìŒ'}\n\n")
            f.write(f"**ë¶„ì„ ì¼ì‹œ**: {datetime.now().isoformat()}\n\n")
            f.write("---\n\n")

            if judge_vendor:
                f.write("## ìì‚¬ ëª¨ë¸ í¸í–¥ ë¶„ì„\n\n")
                f.write(f"- **ìì‚¬ ëª¨ë¸ í‰ê·  ì ìˆ˜**: {bias_analysis['same_vendor_avg_score']:.3f}\n")
                f.write(f"- **íƒ€ì‚¬ ëª¨ë¸ í‰ê·  ì ìˆ˜**: {bias_analysis['other_vendor_avg_score']:.3f}\n")
                f.write(f"- **ì ìˆ˜ ì°¨ì´**: {bias_analysis['score_difference']:.3f}\n")
                f.write(f"- **ìì‚¬ ëª¨ë¸ í‰ê·  ìˆœìœ„**: {bias_analysis['same_vendor_avg_rank']:.2f}\n")
                f.write(f"- **íƒ€ì‚¬ ëª¨ë¸ í‰ê·  ìˆœìœ„**: {bias_analysis['other_vendor_avg_rank']:.2f}\n\n")

                if abs(bias_analysis['score_difference']) > 0.5:
                    f.write(f"âš ï¸ **í¸í–¥ ê²½ê³ **: ì ìˆ˜ ì°¨ì´ê°€ 0.5ë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤.\n\n")
                else:
                    f.write(f"âœ… **í¸í–¥ ì ì •**: ì ìˆ˜ ì°¨ì´ê°€ í—ˆìš© ë²”ìœ„ ë‚´ì…ë‹ˆë‹¤.\n\n")

            f.write("## LLMë³„ ìƒì„¸ í†µê³„\n\n")
            f.write(df_stats.to_markdown(index=False))
            f.write("\n")

        print(f"ğŸ“ ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ì €ì¥: {md_file}")

        return bias_result


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse

    parser = argparse.ArgumentParser(
        description="LLM as Judge í‰ê°€ê¸° - ìƒì„±ëœ ë³´ê³ ì„œì˜ í’ˆì§ˆì„ LLMìœ¼ë¡œ í‰ê°€"
    )

    # ëª¨ë“œ ì„ íƒ
    subparsers = parser.add_subparsers(dest="mode", help="ì‹¤í–‰ ëª¨ë“œ")

    # ë‹¨ì¼ ë””ë ‰í† ë¦¬ í‰ê°€
    single_parser = subparsers.add_parser("single", help="ë‹¨ì¼ ê²°ê³¼ ë””ë ‰í† ë¦¬ í‰ê°€")
    single_parser.add_argument("--results-dir", required=True, help="ê²°ê³¼ ë””ë ‰í† ë¦¬ ê²½ë¡œ")
    single_parser.add_argument("--report-type", default="weekly_report",
                              choices=["weekly_report", "executive_report"],
                              help="ë³´ê³ ì„œ íƒ€ì…")
    single_parser.add_argument("--output", help="ê²°ê³¼ ì €ì¥ ê²½ë¡œ (ì„ íƒ)")

    # ë°°ì¹˜ í‰ê°€
    batch_parser = subparsers.add_parser("batch", help="ì—¬ëŸ¬ retriever ê²°ê³¼ ì¼ê´„ í‰ê°€")
    batch_parser.add_argument("--base-dir", required=True,
                             help="llm_comparison ë””ë ‰í† ë¦¬ ê²½ë¡œ")
    batch_parser.add_argument("--report-type", default="weekly_report",
                             choices=["weekly_report", "executive_report"],
                             help="ë³´ê³ ì„œ íƒ€ì…")

    # ê³µí†µ ì˜µì…˜
    for p in [single_parser, batch_parser]:
        p.add_argument("--judge-model", default="gpt-4o",
                      help="í‰ê°€ì— ì‚¬ìš©í•  LLM ëª¨ë¸ (ì˜ˆ: gpt-4o, anthropic/claude-opus-4.5)")
        p.add_argument("--provider", default="azure_ai",
                      choices=["azure_ai", "openrouter"],
                      help="LLM ì œê³µì")
        p.add_argument("--temperature", type=float, default=0,
                      help="ìƒì„± ì˜¨ë„ (ê¸°ë³¸: 0)")

    args = parser.parse_args()

    if not args.mode:
        parser.print_help()
        return

    # í‰ê°€ê¸° ìƒì„±
    evaluator = LLMAsJudgeEvaluator(
        judge_model=args.judge_model,
        provider=args.provider,
        temperature=args.temperature
    )

    print(f"\n{'='*100}")
    print(f"ğŸ¤– Judge ëª¨ë¸: {args.judge_model} (provider: {args.provider})")
    print(f"{'='*100}\n")

    # ëª¨ë“œë³„ ì‹¤í–‰
    if args.mode == "single":
        evaluator.evaluate_from_results_dir(
            results_dir=args.results_dir,
            report_type=args.report_type,
            output_path=args.output
        )
    elif args.mode == "batch":
        evaluator.batch_evaluate_from_dir(
            base_dir=args.base_dir,
            report_type=args.report_type
        )

    print("\nâœ… ëª¨ë“  í‰ê°€ ì™„ë£Œ!")


if __name__ == "__main__":
    main()
