#!/usr/bin/env python3
"""í‰ê°€ ê¸°ì¤€ë³„ ìœ¡ì•ˆ í‰ê°€ vs Judge í‰ê°€ ì°¨ì´ ë¶„ì„

ê° í‰ê°€ ê¸°ì¤€(ê°„ê²°ì„±, êµ¬ì¡°, ì •í™•ì„± ë“±)ë³„ë¡œ ìœ¡ì•ˆ í‰ê°€ì™€ Judge í‰ê°€ì˜ ì°¨ì´ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
"""

import json
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, List, Any
from datetime import datetime


def load_human_criteria_scores(human_eval_path: Path) -> Dict[str, Dict[str, float]]:
    """ìœ¡ì•ˆ í‰ê°€ì˜ ê¸°ì¤€ë³„ ì ìˆ˜ ì¶”ì¶œ

    Returns:
        {llm_name: {criterion: score}}
    """
    with open(human_eval_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    llm_criteria_scores = {}

    for llm_name, result in data['results'].items():
        llm_criteria_scores[llm_name] = {}
        for criterion_result in result['criterion_results']:
            criterion = criterion_result['criterion']
            score = criterion_result['score']
            llm_criteria_scores[llm_name][criterion] = score

    return llm_criteria_scores


def load_judge_criteria_scores(judge_eval_dir: Path, judge_name: str) -> Dict[str, Dict[str, float]]:
    """Judge í‰ê°€ì˜ ê¸°ì¤€ë³„ ì ìˆ˜ ì¶”ì¶œ (ì§ˆë¬¸ë³„ í‰ê· )

    Returns:
        {llm_name: {criterion: average_score}}
    """
    judge_dir = judge_eval_dir / f"evaluation_{judge_name}"

    if not judge_dir.exists():
        return {}

    # ì§ˆë¬¸ë³„ íŒŒì¼ë“¤
    question_files = sorted(judge_dir.glob('question_*.json'))

    # LLMë³„, ê¸°ì¤€ë³„ ì ìˆ˜ ìˆ˜ì§‘
    llm_criterion_scores = {}

    for qfile in question_files:
        with open(qfile, 'r', encoding='utf-8') as f:
            data = json.load(f)

        for llm_code, result in data.get('results', {}).items():
            llm_name = map_llm_names(llm_code)

            if llm_name not in llm_criterion_scores:
                llm_criterion_scores[llm_name] = {}

            for criterion_result in result.get('criterion_results', []):
                criterion = criterion_result['criterion']
                score = criterion_result['score']

                if criterion not in llm_criterion_scores[llm_name]:
                    llm_criterion_scores[llm_name][criterion] = []

                llm_criterion_scores[llm_name][criterion].append(score)

    # í‰ê·  ê³„ì‚°
    llm_avg_scores = {}
    for llm_name, criteria in llm_criterion_scores.items():
        llm_avg_scores[llm_name] = {
            criterion: np.mean(scores)
            for criterion, scores in criteria.items()
        }

    return llm_avg_scores


def map_llm_names(name: str) -> str:
    """LLM ì´ë¦„ í‘œì¤€í™”"""
    name_mapping = {
        'phi4': 'Phi-4',
        'deepseek_v31': 'DeepSeek-V3.1',
        'gpt41': 'OpenAI GPT-4.1',
        'gpt51': 'OpenAI GPT-5.1',
        'llama33_70b': 'Llama-3.3-70B-Instruct',
        'claude_opus_45': 'Claude 4.5 Opus',
        'claude_sonnet_45': 'Claude 4.5 Sonnet'
    }
    return name_mapping.get(name, name)


def analyze_criterion_differences(
    human_scores: Dict[str, Dict[str, float]],
    judge_scores: Dict[str, Dict[str, float]],
    criterion: str,
    criterion_name: str,
    report_type: str
) -> Dict[str, Any]:
    """íŠ¹ì • í‰ê°€ ê¸°ì¤€ì— ëŒ€í•œ ì°¨ì´ ë¶„ì„"""

    comparison = []

    for llm_name in human_scores.keys():
        if llm_name not in judge_scores:
            continue

        if criterion not in human_scores[llm_name]:
            continue

        if criterion not in judge_scores[llm_name]:
            continue

        human_score = human_scores[llm_name][criterion]
        judge_score = judge_scores[llm_name][criterion]

        comparison.append({
            'llm': llm_name,
            'human_score': human_score,
            'judge_score': judge_score,
            'diff': human_score - judge_score,
            'abs_diff': abs(human_score - judge_score)
        })

    if not comparison:
        return None

    df = pd.DataFrame(comparison)
    df = df.sort_values('diff', ascending=False)

    # í†µê³„
    stats = {
        'criterion': criterion,
        'criterion_name': criterion_name,
        'mean_diff': df['diff'].mean(),
        'mean_abs_diff': df['abs_diff'].mean(),
        'std_diff': df['diff'].std(),
        'max_positive_diff': df['diff'].max(),
        'max_negative_diff': df['diff'].min(),
        'correlation': np.corrcoef(df['human_score'], df['judge_score'])[0, 1] if len(df) > 1 else None
    }

    return {
        'df': df,
        'stats': stats
    }


def generate_criteria_comparison_report(
    report_type: str,
    human_eval_path: Path,
    judge_eval_dir: Path,
    judges: List[str],
    output_dir: Path
):
    """í‰ê°€ ê¸°ì¤€ë³„ ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±"""

    # í‰ê°€ ê¸°ì¤€ ì •ì˜
    if report_type == 'weekly':
        criteria = {
            'accuracy': 'ì •í™•ì„± (Accuracy)',
            'completeness': 'ì™„ê²°ì„± (Completeness)',
            'structure': 'êµ¬ì¡°í™” (Structure)',
            'detail': 'ìƒì„¸ë„ (Detail)'
        }
    else:  # executive
        criteria = {
            'structural_completeness': 'êµ¬ì¡° ì™„ì„±ë„ (Structural Completeness)',
            'document_reference_accuracy': 'ë¬¸ì„œ ì°¸ì¡° ì •í™•ì„± (Document Reference Accuracy)',
            'practical_value': 'ë‚´ìš© ì‹¤ìš©ì„± (Practical Value)',
            'conciseness': 'ê°„ê²°ì„± (Conciseness)'
        }

    # ìœ¡ì•ˆ í‰ê°€ ê¸°ì¤€ ë§¤í•‘ (ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
    human_criteria_map = {
        'weekly': {
            'accuracy': 'accuracy',
            'completeness': 'completeness',
            'structure': 'structure',
            'detail': 'practicality'  # ìœ¡ì•ˆ í‰ê°€ëŠ” practicality ì‚¬ìš©
        },
        'executive': {
            'structural_completeness': 'completeness',
            'document_reference_accuracy': 'accuracy',
            'practical_value': 'practicality',
            'conciseness': 'conciseness'
        }
    }

    # ìœ¡ì•ˆ í‰ê°€ ë¡œë“œ
    human_scores = load_human_criteria_scores(human_eval_path)

    print(f"\n{'='*80}")
    print(f"{report_type.upper()} - í‰ê°€ ê¸°ì¤€ë³„ ì°¨ì´ ë¶„ì„")
    print(f"{'='*80}")

    # ê° Judgeë³„ë¡œ ë¶„ì„
    for judge_name in judges:
        print(f"\nğŸ¤– Judge: {judge_name}")

        # Judge í‰ê°€ ë¡œë“œ
        judge_scores = load_judge_criteria_scores(judge_eval_dir, judge_name)

        if not judge_scores:
            print(f"  âš ï¸ Judge í‰ê°€ ê²°ê³¼ ì—†ìŒ")
            continue

        # ê¸°ì¤€ë³„ ë¶„ì„
        judge_results = {}

        for criterion, criterion_name in criteria.items():
            # ìœ¡ì•ˆ í‰ê°€ ê¸°ì¤€ ë§¤í•‘
            human_criterion = human_criteria_map[report_type].get(criterion, criterion)

            # ìœ¡ì•ˆ í‰ê°€ ì ìˆ˜ë¥¼ Judge ê¸°ì¤€ìœ¼ë¡œ ë³€í™˜
            human_criterion_scores = {}
            for llm_name, llm_criteria in human_scores.items():
                if human_criterion in llm_criteria:
                    human_criterion_scores[llm_name] = {
                        criterion: llm_criteria[human_criterion]
                    }

            result = analyze_criterion_differences(
                human_criterion_scores,
                judge_scores,
                criterion,
                criterion_name,
                report_type
            )

            if result:
                judge_results[criterion] = result

                stats = result['stats']
                print(f"\n  ğŸ“Š {criterion_name}:")
                print(f"    í‰ê·  ì°¨ì´: {stats['mean_diff']:+.2f} (ìœ¡ì•ˆ - Judge)")
                print(f"    í‰ê·  ì ˆëŒ€ ì°¨ì´: {stats['mean_abs_diff']:.2f}")
                print(f"    ìƒê´€ê´€ê³„: {stats['correlation']:.4f}" if stats['correlation'] else "    ìƒê´€ê´€ê³„: N/A")

                # ê°€ì¥ í° ì°¨ì´
                df = result['df']
                max_pos = df.loc[df['diff'].idxmax()]
                max_neg = df.loc[df['diff'].idxmin()]

                if max_pos['diff'] > 1.0:
                    print(f"    ìœ¡ì•ˆì´ ë†’ê²Œ í‰ê°€: {max_pos['llm']} ({max_pos['diff']:+.2f})")
                if max_neg['diff'] < -1.0:
                    print(f"    Judgeê°€ ë†’ê²Œ í‰ê°€: {max_neg['llm']} ({max_neg['diff']:+.2f})")

        # Markdown ë¦¬í¬íŠ¸ ìƒì„±
        report_path = output_dir / f"criteria_comparison_{report_type}_{judge_name.replace('/', '_')}.md"
        generate_criteria_markdown(judge_results, judge_name, report_type, report_path)

        # CSV ì €ì¥
        for criterion, result in judge_results.items():
            csv_path = output_dir / f"criteria_{report_type}_{judge_name.replace('/', '_')}_{criterion}.csv"
            result['df'].to_csv(csv_path, index=False, encoding='utf-8-sig')


def generate_criteria_markdown(
    results: Dict[str, Any],
    judge_name: str,
    report_type: str,
    output_path: Path
):
    """í‰ê°€ ê¸°ì¤€ë³„ ë¹„êµ Markdown ë¦¬í¬íŠ¸ ìƒì„±"""

    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(f"# í‰ê°€ ê¸°ì¤€ë³„ ìœ¡ì•ˆ vs Judge ë¹„êµ\n\n")
        f.write(f"**Judge**: {judge_name}\n\n")
        f.write(f"**ë³´ê³ ì„œ íƒ€ì…**: {report_type}\n\n")
        f.write(f"**ë¶„ì„ ì¼ì‹œ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write("---\n\n")

        # ìš”ì•½
        f.write("## ğŸ“Š í‰ê°€ ê¸°ì¤€ë³„ ì°¨ì´ ìš”ì•½\n\n")
        f.write("| í‰ê°€ ê¸°ì¤€ | í‰ê·  ì°¨ì´ | í‰ê·  ì ˆëŒ€ì°¨ | ìƒê´€ê´€ê³„ |\n")
        f.write("|-----------|-----------|-------------|----------|\n")

        for criterion, result in results.items():
            stats = result['stats']
            f.write(f"| {stats['criterion_name']} | {stats['mean_diff']:+.2f} | ")
            f.write(f"{stats['mean_abs_diff']:.2f} | ")
            corr = f"{stats['correlation']:.4f}" if stats['correlation'] else "N/A"
            f.write(f"{corr} |\n")

        f.write("\n")
        f.write("**ì°¸ê³ **: í‰ê·  ì°¨ì´ = ìœ¡ì•ˆ ì ìˆ˜ - Judge ì ìˆ˜\n")
        f.write("- ì–‘ìˆ˜(+): ìœ¡ì•ˆ í‰ê°€ê°€ ë” ë†’ìŒ\n")
        f.write("- ìŒìˆ˜(-): Judge í‰ê°€ê°€ ë” ë†’ìŒ\n\n")

        # ê¸°ì¤€ë³„ ìƒì„¸
        for criterion, result in results.items():
            stats = result['stats']
            df = result['df']

            f.write(f"## {stats['criterion_name']}\n\n")

            # í†µê³„
            f.write("### í†µê³„\n\n")
            f.write(f"- **í‰ê·  ì°¨ì´**: {stats['mean_diff']:+.2f}\n")
            f.write(f"- **í‰ê·  ì ˆëŒ€ ì°¨ì´**: {stats['mean_abs_diff']:.2f}\n")
            f.write(f"- **í‘œì¤€í¸ì°¨**: {stats['std_diff']:.2f}\n")
            if stats['correlation']:
                f.write(f"- **ìƒê´€ê´€ê³„**: {stats['correlation']:.4f}\n")
            f.write("\n")

            # í•´ì„
            mean_diff = stats['mean_diff']
            if abs(mean_diff) < 0.5:
                interpretation = "âœ… **ë§¤ìš° ìœ ì‚¬** - ìœ¡ì•ˆ í‰ê°€ì™€ Judge í‰ê°€ê°€ ê±°ì˜ ì¼ì¹˜"
            elif abs(mean_diff) < 1.0:
                interpretation = "âœ“ **ìœ ì‚¬** - í° ì°¨ì´ ì—†ìŒ"
            elif mean_diff > 0:
                interpretation = f"âš ï¸ **ìœ¡ì•ˆ í‰ê°€ê°€ í‰ê·  {mean_diff:.2f}ì  ë†’ìŒ** - ìœ¡ì•ˆì´ ë” ê´€ëŒ€"
            else:
                interpretation = f"âš ï¸ **Judge í‰ê°€ê°€ í‰ê·  {abs(mean_diff):.2f}ì  ë†’ìŒ** - Judgeê°€ ë” ê´€ëŒ€"

            f.write(f"{interpretation}\n\n")

            # ìƒì„¸ ë¹„êµí‘œ
            f.write("### ìƒì„¸ ë¹„êµ\n\n")
            f.write("| LLM | ìœ¡ì•ˆ ì ìˆ˜ | Judge ì ìˆ˜ | ì°¨ì´ |\n")
            f.write("|-----|-----------|------------|------|\n")

            for _, row in df.iterrows():
                f.write(f"| {row['llm']} | {row['human_score']:.2f} | ")
                f.write(f"{row['judge_score']:.2f} | {row['diff']:+.2f} |\n")

            f.write("\n")

            # í° ì°¨ì´ í•˜ì´ë¼ì´íŠ¸
            big_diff = df[df['abs_diff'] >= 1.5]
            if len(big_diff) > 0:
                f.write("### âš ï¸ í° ì°¨ì´ (ì ˆëŒ€ê°’ >= 1.5)\n\n")
                for _, row in big_diff.iterrows():
                    if row['diff'] > 0:
                        f.write(f"- **{row['llm']}**: ìœ¡ì•ˆì´ {row['diff']:.2f}ì  ë” ë†’ìŒ\n")
                    else:
                        f.write(f"- **{row['llm']}**: Judgeê°€ {abs(row['diff']):.2f}ì  ë” ë†’ìŒ\n")
                f.write("\n")

            f.write("---\n\n")

    print(f"\nğŸ“ í‰ê°€ ê¸°ì¤€ë³„ ë¦¬í¬íŠ¸ ìƒì„±: {output_path}")


def generate_insights_summary(
    report_type: str,
    output_dir: Path
):
    """ì¸ì‚¬ì´íŠ¸ ìš”ì•½ ìƒì„±"""

    summary_path = output_dir / f"criteria_insights_summary_{report_type}.md"

    with open(summary_path, 'w', encoding='utf-8') as f:
        f.write(f"# í‰ê°€ ê¸°ì¤€ë³„ ì°¨ì´ ì¸ì‚¬ì´íŠ¸ ìš”ì•½\n\n")
        f.write(f"**ë³´ê³ ì„œ íƒ€ì…**: {report_type}\n\n")
        f.write(f"**ë¶„ì„ ì¼ì‹œ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write("---\n\n")

        if report_type == 'weekly':
            f.write("## ì£¼ê°„ ë³´ê³ ì„œ í‰ê°€ ê¸°ì¤€ë³„ ì¸ì‚¬ì´íŠ¸\n\n")

            f.write("### 1. ê°„ê²°ì„±/ìƒì„¸ë„ (Conciseness/Detail)\n\n")
            f.write("**ê°€ì¥ í° ì°¨ì´ë¥¼ ë³´ì´ëŠ” ê¸°ì¤€**\n\n")
            f.write("- **ìœ¡ì•ˆ í‰ê°€**:\n")
            f.write("  - GPT-5.1ì„ 'ë„ˆë¬´ ì¥í™©í•˜ë‹¤'ê³  í‰ê°€ (ë‚®ì€ ì ìˆ˜)\n")
            f.write("  - DeepSeek-V3.1ì„ 'ê°€ì¥ ê°„ê²°í•˜ë‹¤'ê³  í‰ê°€ (ë†’ì€ ì ìˆ˜)\n")
            f.write("  - ì ì ˆí•œ ê¸¸ì´(800-1200ì)ë¥¼ ì¤‘ì‹œ\n\n")

            f.write("- **Judge í‰ê°€**:\n")
            f.write("  - GPT-5.1ì„ 'ìƒì„¸í•˜ê³  ì™„ë²½í•˜ë‹¤'ê³  í‰ê°€ (ë†’ì€ ì ìˆ˜)\n")
            f.write("  - ê¸¸ì´ë³´ë‹¤ ë‚´ìš©ì˜ ì™„ì„±ë„ë¥¼ ì¤‘ì‹œ\n")
            f.write("  - 'ë¹ ì§ì—†ëŠ” ì •ë³´ ì œê³µ'ì„ ê°„ê²°ì„±ë³´ë‹¤ ìš°ì„ ì‹œ\n\n")

            f.write("**â†’ í•µì‹¬ ì°¨ì´**: ìœ¡ì•ˆì€ 'ì½ê¸° ë¶€ë‹´', JudgeëŠ” 'ì •ë³´ ì™„ì„±ë„' ì¤‘ì‹œ\n\n")

            f.write("### 2. ì •í™•ì„± (Accuracy)\n\n")
            f.write("**ë¹„êµì  ì¼ì¹˜ë„ê°€ ë†’ì€ ê¸°ì¤€**\n\n")
            f.write("- í™˜ê°(Hallucination) ì—¬ë¶€ëŠ” ìœ¡ì•ˆ/Judge ëª¨ë‘ ëª…í™•íˆ íŒë‹¨\n")
            f.write("- ë¬¸ì„œ ì°¸ì¡°ì˜ ì •í™•ì„±ì€ ê°ê´€ì  í‰ê°€ ê°€ëŠ¥\n")
            f.write("- Llama-3.3-70Bë¥¼ ëª¨ë‘ ë‚®ê²Œ í‰ê°€ (ì •í™•ì„± ë¶€ì¡±)\n\n")

            f.write("### 3. êµ¬ì¡°í™” (Structure)\n\n")
            f.write("**ì¤‘ê°„ ì •ë„ì˜ ì¼ì¹˜ë„**\n\n")
            f.write("- **ìœ¡ì•ˆ í‰ê°€**:\n")
            f.write("  - 'ì‹¤ë¬´ì—ì„œ ë°”ë¡œ í™œìš© ê°€ëŠ¥í•œ êµ¬ì¡°'ë¥¼ ì¤‘ì‹œ\n")
            f.write("  - GPT-4.1ì˜ ìì—°ìŠ¤ëŸ¬ìš´ íë¦„ì„ ë†’ì´ í‰ê°€\n\n")

            f.write("- **Judge í‰ê°€**:\n")
            f.write("  - 'ë…¼ë¦¬ì  ì„¹ì…˜ êµ¬ì„±'ì„ ì¤‘ì‹œ\n")
            f.write("  - í˜•ì‹ì  ì™„ì„±ë„ë¥¼ ë” ì¤‘ìš”í•˜ê²Œ ë´„\n\n")

            f.write("### 4. ì‹¤ë¬´ì„±/ì™„ê²°ì„± (Practicality/Completeness)\n\n")
            f.write("**ìœ¡ì•ˆì´ ë” ì—„ê²©**\n\n")
            f.write("- ìœ¡ì•ˆ: 'ì¦‰ì‹œ í™œìš© ê°€ëŠ¥ì„±'ì„ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€\n")
            f.write("- Judge: 'ì •ë³´ì˜ í¬ê´„ì„±'ì„ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€\n")
            f.write("- Claude ëª¨ë¸ë“¤: JudgeëŠ” ë†’ê²Œ, ìœ¡ì•ˆì€ ë‚®ê²Œ í‰ê°€\n")
            f.write("  - ì´ìœ : êµ¬ì¡°ëŠ” ì¢‹ì§€ë§Œ 'ë¬¸ì„œì— ëª…ì‹œë˜ì§€ ì•ŠìŒ' ê³¼ë‹¤ë¡œ ì‹¤ìš©ì„± ì €í•˜\n\n")

        else:  # executive
            f.write("## ìµœì¢… ë³´ê³ ì„œ í‰ê°€ ê¸°ì¤€ë³„ ì¸ì‚¬ì´íŠ¸\n\n")

            f.write("### 1. ê°„ê²°ì„± (Conciseness)\n\n")
            f.write("**ì£¼ê°„ ë³´ê³ ì„œë³´ë‹¤ ì°¨ì´ ì ìŒ**\n\n")
            f.write("- ìµœì¢… ë³´ê³ ì„œëŠ” ìƒì„¸í•¨ì´ ì¥ì ìœ¼ë¡œ ì‘ìš©\n")
            f.write("- GPT-5.1ì˜ ì¥í™©í•¨ì´ ëœ ë¬¸ì œë¨\n")
            f.write("- ìœ¡ì•ˆ/Judge ëª¨ë‘ 'í•µì‹¬ ìœ„ì£¼ ìš”ì•½' ì¤‘ì‹œ\n\n")

            f.write("### 2. ì™„ì„±ë„ (Structural Completeness)\n\n")
            f.write("**ë†’ì€ ì¼ì¹˜ë„**\n\n")
            f.write("- ê²½ì˜ì§„ ë³´ê³ ì„œëŠ” ì™„ì„±ë„ê°€ ë§¤ìš° ì¤‘ìš”\n")
            f.write("- GPT-5.1/GPT-4.1ì„ ëª¨ë‘ ë†’ê²Œ í‰ê°€\n")
            f.write("- Llama-3.3-70Bë¥¼ ëª¨ë‘ ë§¤ìš° ë‚®ê²Œ í‰ê°€ (2.8 vs 3.3)\n\n")

            f.write("### 3. ì •í™•ì„± (Document Reference Accuracy)\n\n")
            f.write("**ë§¤ìš° ë†’ì€ ì¼ì¹˜ë„**\n\n")
            f.write("- ê°ê´€ì ìœ¼ë¡œ ê²€ì¦ ê°€ëŠ¥í•œ ê¸°ì¤€\n")
            f.write("- ìœ¡ì•ˆ/Judge ëª¨ë‘ ìœ ì‚¬í•œ íŒë‹¨\n\n")

            f.write("### 4. ì‹¤ìš©ì„± (Practical Value)\n\n")
            f.write("**ì•½ê°„ì˜ ì°¨ì´**\n\n")
            f.write("- ìœ¡ì•ˆ: 'ì˜ì‚¬ê²°ì • ì§€ì›' ê´€ì \n")
            f.write("- Judge: 'ì¸ì‚¬ì´íŠ¸ ì œê³µ' ê´€ì \n")
            f.write("- GPT-5.1ì„ Judgeê°€ ë” ë†’ê²Œ í‰ê°€ (ìƒì„¸í•œ ë¶„ì„ ì œê³µ)\n\n")

        f.write("## ğŸ”‘ í•µì‹¬ ë°œê²¬ì‚¬í•­\n\n")

        f.write("### 1. Judgeê°€ ê³¼ëŒ€í‰ê°€í•˜ëŠ” íŠ¹ì„±\n\n")
        f.write("- âœ… ì •ë³´ì˜ ì™„ì„±ë„\n")
        f.write("- âœ… ìƒì„¸í•œ ì„¤ëª…\n")
        f.write("- âœ… ë…¼ë¦¬ì  êµ¬ì¡°\n")
        f.write("- âŒ ì ì ˆí•œ ê¸¸ì´ (ì£¼ê°„ ë³´ê³ ì„œ)\n")
        f.write("- âŒ ì‹¤ë¬´ í™œìš© í¸ì˜ì„±\n\n")

        f.write("### 2. ìœ¡ì•ˆ í‰ê°€ê°€ ë” ì¤‘ì‹œí•˜ëŠ” íŠ¹ì„±\n\n")
        f.write("- âœ… ì½ê¸° ë¶€ë‹´ (ê°„ê²°ì„±)\n")
        f.write("- âœ… ì¦‰ì‹œ í™œìš© ê°€ëŠ¥ì„±\n")
        f.write("- âœ… ìì—°ìŠ¤ëŸ¬ìš´ íë¦„\n")
        f.write("- âœ… ë§¥ë½ì— ë§ëŠ” í‘œí˜„\n\n")

        f.write("### 3. ë³´ê³ ì„œ íƒ€ì…ë³„ ê¶Œì¥ í‰ê°€ ë°©ì‹\n\n")

        f.write("**ì£¼ê°„ ë³´ê³ ì„œ:**\n")
        f.write("```\n")
        f.write("ê°„ê²°ì„±/ì‹¤ë¬´ì„±: ìœ¡ì•ˆ í‰ê°€ í•„ìˆ˜ (Judge ì‹ ë¢°ë„ ë‚®ìŒ)\n")
        f.write("ì •í™•ì„±/êµ¬ì¡°: Judge í‰ê°€ í™œìš© ê°€ëŠ¥ (ì‹ ë¢°ë„ ì¤‘ê°„)\n")
        f.write("ì¢…í•©: Judge 30% + ìœ¡ì•ˆ 70%\n")
        f.write("```\n\n")

        f.write("**ìµœì¢… ë³´ê³ ì„œ:**\n")
        f.write("```\n")
        f.write("ì™„ì„±ë„/ì •í™•ì„±: Judge í‰ê°€ ì‹ ë¢° ê°€ëŠ¥ (ë†’ì€ ì¼ì¹˜ë„)\n")
        f.write("ì‹¤ìš©ì„±: ìœ¡ì•ˆ í‰ê°€ë¡œ ë³´ì™„\n")
        f.write("ì¢…í•©: Judge 60% + ìœ¡ì•ˆ 40%\n")
        f.write("```\n\n")

    print(f"\nğŸ“ ì¸ì‚¬ì´íŠ¸ ìš”ì•½ ìƒì„±: {summary_path}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""

    base_dir = Path("data/results/multi_llm_test")
    output_dir = base_dir / "evaluation_comparison"
    output_dir.mkdir(parents=True, exist_ok=True)

    judges = ['anthropic_claude-opus-4.5', 'gpt-5.1', 'DeepSeek-V3.1']

    # Weekly ë³´ê³ ì„œ
    generate_criteria_comparison_report(
        report_type='weekly',
        human_eval_path=base_dir / "human_evaluation/weekly_human_evaluation_judge_format.json",
        judge_eval_dir=base_dir / "weekly/20260102_052136/multi_judge_evaluation",
        judges=judges,
        output_dir=output_dir
    )

    # Executive ë³´ê³ ì„œ
    generate_criteria_comparison_report(
        report_type='executive',
        human_eval_path=base_dir / "human_evaluation/executive_human_evaluation_judge_format.json",
        judge_eval_dir=base_dir / "executive/20260102_053212/multi_judge_evaluation",
        judges=judges,
        output_dir=output_dir
    )

    # ì¸ì‚¬ì´íŠ¸ ìš”ì•½
    generate_insights_summary('weekly', output_dir)
    generate_insights_summary('executive', output_dir)

    print("\nâœ… í‰ê°€ ê¸°ì¤€ë³„ ë¶„ì„ ì™„ë£Œ!")


if __name__ == "__main__":
    main()
